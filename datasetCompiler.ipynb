{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile datasets and pre-process the images\n",
    "\n",
    "## 1. Combining Dataset\n",
    "- Copy apex frames and combine micro-expression datasets (CASMEII, SAMM, SMIC) into 1 folder (for pre-proc)\n",
    "- Copy apex frames and combine macro-expression datasets (CK+ and Oulu-Casia) into 1 folder (for pre-proc)\n",
    "- Generate a csv file with filename and class label to be used in data_pre_processing.ipynb\n",
    "\n",
    "## 2. Pre-process datasets\n",
    "- Perform face alignment and cropping for all apex frames\n",
    "\n",
    "### Note\n",
    "- We'll take samples for these 7 classes only and ignore the rest (Happiness, Disgust, Surprise, Sadness, Fear, Anger, Contempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Combining Micro-Expression datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, csv\n",
    "import pandas as pd\n",
    "\n",
    "ignore = ['other', 'others']\n",
    "dest = 'C:/Users/User/Documents/UKM/Project/Dataset/Consolidated Dataset/original/'\n",
    "dest_proc = 'C:/Users/User/Documents/UKM/Project/Dataset/Consolidated Dataset/pre-proc/'\n",
    "csv_path = 'C:/Users/User/Documents/UKM/Project/Models/EfficientNet/dataset-csv/'\n",
    "dataset_hdr = ['filename','class']\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 028_4_1\n",
      "Error: 032_3_1\n"
     ]
    }
   ],
   "source": [
    "# copied filenames format:\n",
    "# <dataset>-<subj>_<sample>-<label>.jpg\n",
    "\n",
    "def copyApexSamm():\n",
    "    with open(csv_path + 'samm.csv') as file:\n",
    "        rdr = csv.reader(file, delimiter=',')\n",
    "        first = True\n",
    "        for row in rdr:\n",
    "            label = row[1].lower()\n",
    "            if(first): #skip header row\n",
    "                first = False\n",
    "            elif(label not in ignore):\n",
    "                try:\n",
    "                    subj = row[0].split('_')[0]\n",
    "                    inpPath = row[3] + '/' + row[0] + '/' + subj + '_' + row[2] + '.jpg'\n",
    "                    filename = 'samm-' + row[0] + '-' + label + '.jpg'\n",
    "                    if(os.path.exists(inpPath)):\n",
    "                        shutil.copy(inpPath, dest + filename)\n",
    "                    else:\n",
    "                        inpPath = row[3] + '/' + row[0] + '/' + subj + '_0' + row[2] + '.jpg'\n",
    "                        if(os.path.exists(inpPath)):\n",
    "                            shutil.copy(inpPath, dest + filename)\n",
    "                        else:\n",
    "                            inpPath = row[3] + '/' + row[0] + '/' + subj + '_00' + row[2] + '.jpg'\n",
    "                            shutil.copy(inpPath, dest + filename)\n",
    "                    dataset.append([filename, label])\n",
    "                except:\n",
    "                    print('Error: ' + row[0])\n",
    " \n",
    "copyApexSamm()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyApexCasme():\n",
    "    with open(csv_path + 'casme.csv') as file:\n",
    "            rdr = csv.reader(file, delimiter=',')\n",
    "            first = True\n",
    "            for row in rdr:\n",
    "                label = row[0].lower()\n",
    "                if(first): #skip header row\n",
    "                    first = False\n",
    "                elif(label not in ignore):   \n",
    "                    try:\n",
    "                        str = row[1].split('\\\\')\n",
    "                        filename =  'casme2-' + str[8] + '_' + str[9]  + '-' + label + '.jpg'\n",
    "                        inpPath = row[1]\n",
    "                        if(not os.path.exists(dest+filename)):\n",
    "                            shutil.copy(inpPath, dest + filename)\n",
    "                            dataset.append([filename, label])\n",
    "                    except:\n",
    "                        print('Error: ' + row[1])\n",
    "                        \n",
    "copyApexCasme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core import base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# work of Zhou et al. [55], the mid-position frame between the onset and offset is\n",
    "# approximated apex frame in the SMIC–HS dataset\n",
    "# L. Zhou, Q. Mao, L. Xue, Dual-inception network for cross-database microexpression recognition, in: 2019 14th IEEE International Conference on\n",
    "# Automatic Face & Gesture Recognition (FG 2019), IEEE, 2019, pp. 1–5.\n",
    "# http://staff.ustc.edu.cn/~tongxu/Papers/Sirui_NeuroC21.pdf\n",
    "def get_apex(p):\n",
    "    frames = os.listdir(p)\n",
    "    frames.sort()\n",
    "    ind = len(frames)//2\n",
    "    return frames[ind]\n",
    "\n",
    "#get from original folder in order to do data pre-proc\n",
    "def preproc_smic_sub():\n",
    "    classifyPath = 'C:/Users/User/Documents/UKM/Project/Dataset/SMICdatabase/SMICdatabase/classify/5class'\n",
    "    basePath = 'C:/Users/User/Documents/UKM/Project/Dataset/SMICdatabase/SMICdatabase/original/micro'\n",
    "    data = []\n",
    "    #the foldername for happy and sad was renamed manually to standardize\n",
    "    for label in os.listdir(classifyPath):\n",
    "        labelPath = classifyPath + '/' + label\n",
    "        samples = os.listdir(labelPath)\n",
    "        for sample in samples:\n",
    "            subject = sample[:2]\n",
    "            sample_s = sample[3:]\n",
    "            apex = get_apex(labelPath + '/' + sample)\n",
    "            p = (basePath + '/' + subject + '/' + sample_s + '/image' + apex)\n",
    "            data.append([label, p])\n",
    "    smic_df = pd.DataFrame.from_records(data, columns=['class','filepath'])\n",
    "    smic_df.to_csv('dataset-csv/smic.csv', index=None)\n",
    "    \n",
    "preproc_smic_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyApexSmic():\n",
    "    with open(csv_path + 'smic.csv') as file:\n",
    "            rdr = csv.reader(file, delimiter=',')\n",
    "            first = True\n",
    "            for row in rdr:\n",
    "                label = row[0].lower()\n",
    "                if(first): #skip header row\n",
    "                    first = False\n",
    "                elif(label not in ignore):   \n",
    "                    try:\n",
    "                        str = row[1].split('/')\n",
    "                        filename =  'smic-' + str[11] + '_' + str[12] + '-' + label + '.bmp'\n",
    "                        inpPath = row[1]\n",
    "                        shutil.copy(inpPath, dest + filename)\n",
    "                        dataset.append([filename, label])\n",
    "                    except:\n",
    "                        print('Error: ' + row[1])\n",
    "                        \n",
    "copyApexSmic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the copied files filenames and label to a csv file\n",
    "\n",
    "import csv\n",
    "with open(csv_path + 'cde.csv', 'w', newline='') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(dataset_hdr)\n",
    "    write.writerows(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACE ALIGNMENT - returns cropped and rotated colored images\n",
    "\n",
    "from mtcnn import MTCNN\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "face_detector = MTCNN()\n",
    "\n",
    "def eyesAlignment(img, left_eye, right_eye):\n",
    "\tleft_eye_x, left_eye_y = left_eye\n",
    "\tright_eye_x, right_eye_y = right_eye\n",
    "\t  \n",
    "\t#get angle of rotation\n",
    "\tdelta_x = right_eye_x - left_eye_x\n",
    "\tdelta_y = right_eye_y - left_eye_y\n",
    "\tangle=np.arctan(delta_y/delta_x)\n",
    "\tangle = (angle * 180) / np.pi\n",
    "\t\n",
    "\t# Width and height of the image\n",
    "\th, w = img.shape[:2]\n",
    "\t# Calculating a center point of the image\n",
    "\t# Integer division \"//\"\" ensures that we receive whole numbers\n",
    "\tcenter = (w // 2, h // 2)\n",
    "\t# Defining a matrix M and calling cv2.getRotationMatrix2D method\n",
    "\tM = cv2.getRotationMatrix2D(center, (angle), 1.0)\n",
    "\t# Applying the rotation to our image using the cv2.warpAffine method\n",
    "\trotated_img_c = cv2.warpAffine(img, M, (w, h))\n",
    "\t\n",
    "\treturn rotated_img_c\n",
    "\n",
    "def align_mtcnn(imgPath):\n",
    "\timg = cv2.imread(imgPath)\n",
    "\n",
    "\timg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #mtcnn expects RGB but OpenCV read BGR\n",
    "\tdetections = face_detector.detect_faces(img_rgb)\n",
    "\tif len(detections) > 0:\n",
    "\t\tdetection = detections[0]\n",
    "\t\tx, y, w, h = detection[\"box\"]\n",
    "\n",
    "\t\tkeypoints = detection[\"keypoints\"]\n",
    "\t\tleft_eye = keypoints[\"left_eye\"]\n",
    "\t\tright_eye = keypoints[\"right_eye\"]\n",
    "\t\trotated_face = eyesAlignment(img_rgb, left_eye, right_eye)\n",
    "\t\trotated_face = cv2.cvtColor(rotated_face, cv2.COLOR_RGB2BGR)\n",
    "  \n",
    "\t\tsqr_x = x - ((h-w)//2)\n",
    "\t\t#cv2.rectangle(img, [sqr_x,y], [sqr_x + h , y+h], (0,255,0),2)\n",
    "  \n",
    "\t\tcropped = rotated_face[y:y + h, sqr_x: sqr_x + h]\n",
    "\t\tr = 224.0 / cropped.shape[1]\n",
    "\t\tdim =  (224, int(cropped.shape[0] * r))\n",
    "\t\tresized = cv2.resize(cropped, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\t#cv2.imshow('ori', img)\n",
    "\t#cv2.imshow('rotated', rotated_face)\n",
    "\t#cv2.imshow('cropped', cropped)\n",
    "\t#cv2.imshow('resized', resized)\n",
    "\t#cv2.waitKey()\n",
    "\treturn resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "cde_path = r\"C:\\Users\\User\\Documents\\UKM\\Project\\Dataset\\Consolidated Dataset\\original\"\n",
    "cde_proc_path = r\"C:\\Users\\User\\Documents\\UKM\\Project\\Dataset\\Consolidated Dataset\\pre-proc\"\n",
    "\n",
    "samples = os.listdir(cde_path)\n",
    "try:\n",
    "    for sample in samples:\n",
    "        res = align_mtcnn(cde_path + '\\\\' + sample)\n",
    "        filename = cde_proc_path + '\\\\' + sample.split('.')[0] + '.jpg'\n",
    "        cv2.imwrite(filename, res)\n",
    "except:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "cde_path = r\"C:\\Users\\User\\Documents\\UKM\\Project\\Dataset\\Consolidated Dataset\\original\"\n",
    "cde_proc_path = r\"C:\\Users\\User\\Documents\\UKM\\Project\\Dataset\\Consolidated Dataset\\pre-proc\"\n",
    "\n",
    "samples = [\"casme2-sub02_EP01_11f-repression.jpg\",\n",
    "\"casme2-sub02_EP02_04f-repression.jpg\",\n",
    "\"casme2-sub02_EP03_02f-repression.jpg\",\n",
    "\"casme2-sub02_EP06_01f-repression.jpg\",\n",
    "\"casme2-sub02_EP06_02f-repression.jpg\",\n",
    "\"casme2-sub08_EP13_01f-repression.jpg\",\n",
    "\"casme2-sub09_EP06_01f-repression.jpg\",\n",
    "\"casme2-sub09_EP09_04-repression.jpg\",\n",
    "\"casme2-sub09_EP09_05-repression.jpg\",\n",
    "\"casme2-sub09_EP13_01-repression.jpg\",\n",
    "\"casme2-sub09_EP17_08-repression.jpg\",\n",
    "\"casme2-sub16_EP01_08-repression.jpg\",\n",
    "\"casme2-sub17_EP02_18f-repression.jpg\",\n",
    "\"casme2-sub17_EP05_03f-repression.jpg\",\n",
    "\"casme2-sub17_EP05_04-repression.jpg\",\n",
    "\"casme2-sub17_EP05_09-repression.jpg\",\n",
    "\"casme2-sub17_EP05_10-repression.jpg\",\n",
    "\"casme2-sub17_EP06_08-repression.jpg\",\n",
    "\"casme2-sub17_EP10_06-repression.jpg\",\n",
    "\"casme2-sub17_EP12_03-repression.jpg\",\n",
    "\"casme2-sub17_EP15_04-repression.jpg\",\n",
    "\"casme2-sub21_EP05_02-repression.jpg\",\n",
    "\"casme2-sub22_EP01_12-repression.jpg\",\n",
    "\"casme2-sub22_EP13_08-repression.jpg\",\n",
    "\"casme2-sub23_EP04_03f-repression.jpg\",\n",
    "\"casme2-sub23_EP05_24f-repression.jpg\",\n",
    "\"casme2-sub23_EP13_04-repression.jpg\",\n",
    "]\n",
    "try:\n",
    "    for sample in samples:\n",
    "        res = align_mtcnn(cde_path + '\\\\' + sample)\n",
    "        filename = cde_proc_path + '\\\\' + sample.split('.')[0] + '.jpg'\n",
    "        cv2.imwrite(filename, res)\n",
    "except:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save filenames and labels of our dataset in csv file\n",
    "\n",
    "1. 5 labels\n",
    "2. 3 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 labels\n",
    "\n",
    "import csv\n",
    "\n",
    "dest_proc = 'C:/Users/User/Documents/UKM/Project/Dataset/Consolidated Dataset/pre-proc'\n",
    "samples = os.listdir(dest_proc)\n",
    "\n",
    "#Encode labels as integers\n",
    "label_array = [\"disgust\",\"anger\", \"fear\", \"sadness\", \"surprise\", \"happiness\", \"contempt\"]\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_array))\n",
    "\n",
    "with open('dataset-csv/combinedDataset.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for sample in samples:\n",
    "        label = sample.rsplit('-',1)[1].rstrip('.jpg')\n",
    "        label = label_to_index[label]\n",
    "        filepath = \"{}/{}\".format(dest_proc , sample)\n",
    "        writer.writerow([filepath, label])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 labels\n",
    "\n",
    "import csv\n",
    "\n",
    "dest_proc = 'C:/Users/User/Documents/UKM/Project/Dataset/Consolidated Dataset/pre-proc'\n",
    "samples = os.listdir(dest_proc)\n",
    "\n",
    "#Encode labels as integers\n",
    "label_dict = {\n",
    "    \"repression\": 0,\n",
    "    \"anger\":0 ,\n",
    "    \"contempt\":0, \n",
    "    \"disgust\":0, \n",
    "    \"fear\":0, \n",
    "    \"sadness\":0,\n",
    "    \"happiness\": 1,\n",
    "    \"surprise\" : 2\n",
    "}\n",
    "\n",
    "with open('dataset-csv/combinedDataset3class.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for sample in samples:\n",
    "        label = sample.rsplit('-',1)[1].rstrip('.jpg')\n",
    "        label = label_dict[label]\n",
    "        filepath = \"{}/{}\".format(dest_proc , sample)\n",
    "        writer.writerow([filepath, label])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combining Macro-expression datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, csv\n",
    "import pandas as pd\n",
    "\n",
    "macro_dest = 'C:/Users/User/Documents/UKM/Project/Dataset/Consolidated Macro Dataset/'\n",
    "csv_path = 'C:/Users/User/Documents/UKM/Project/Models/EfficientNet/dataset-csv/'\n",
    "dataset_hdr = ['filename','class']\n",
    "macro_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete dataset retrieved from : https://www.kaggle.com/shareef0612/ckdataset\n",
    "\n",
    "def copyCK():\n",
    "    ckPath = 'C:/Users/User/Documents/UKM/Project/Dataset/macro-expression ds/ck+/'\n",
    "    labels = os.listdir(ckPath) #happy label was manually changed to happiness to standardize \n",
    "    for label in labels:\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyOuluCasia():\n",
    "    ckPath = 'C:/Users/User/Documents/UKM/Project/Dataset/macro-expression ds/OuluCASIA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def fera2013():\n",
    "    path = r'C:\\Users\\User\\Documents\\UKM\\Project\\Dataset\\fer2013\\\\'\n",
    "    emotion_cat = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}\n",
    "    with open(path + 'fer2013.csv') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "        testRow = rows[1]\n",
    "        label = testRow[0]\n",
    "        imData = np.array(testRow[1].split(' ')).reshape(48, 48, 1).astype('float32')\n",
    "        img = np.stack(imData, axis=0)\n",
    "        cv2.imshow('test',img)\n",
    "        cv2.waitKey()\n",
    "\n",
    "fera2013()      \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9966458c6ad74b8bc08c1c30471a4ad47794339d9867a51d09ce220173850fd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('effNetVenv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
