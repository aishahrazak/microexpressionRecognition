{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 classes Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment without data augmentation\n",
    " \n",
    "- dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(noOfHeadIter=1, noOfFtIter=10, noOfHeadEpoch=10, noOfFtEpoch=20, outp='output')\n",
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling_3 (Rescaling)        (None, 224, 224, 3)  0           ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " normalization_3 (Normalization  (None, 224, 224, 3)  7          ['rescaling_3[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['normalization_3[0][0]']        \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',     \n",
      "                                                                  'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3113 - accuracy: 0.5212 \n",
      "Epoch 00001: val_loss improved from inf to 2.19812, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 193s 15s/step - loss: 2.3113 - accuracy: 0.5212 - val_loss: 2.1981 - val_accuracy: 0.5341\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1359 - accuracy: 0.5297 \n",
      "Epoch 00002: val_loss improved from 2.19812 to 2.00975, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 208s 17s/step - loss: 2.1359 - accuracy: 0.5297 - val_loss: 2.0097 - val_accuracy: 0.5341\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.8911 - accuracy: 0.5581 \n",
      "Epoch 00003: val_loss improved from 2.00975 to 1.84997, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 213s 18s/step - loss: 1.8911 - accuracy: 0.5581 - val_loss: 1.8500 - val_accuracy: 0.5455\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.7577 - accuracy: 0.5496 \n",
      "Epoch 00004: val_loss improved from 1.84997 to 1.71674, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 215s 18s/step - loss: 1.7577 - accuracy: 0.5496 - val_loss: 1.7167 - val_accuracy: 0.5455\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6091 - accuracy: 0.5496 \n",
      "Epoch 00005: val_loss improved from 1.71674 to 1.60999, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 206s 17s/step - loss: 1.6091 - accuracy: 0.5496 - val_loss: 1.6100 - val_accuracy: 0.5341\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.5248 - accuracy: 0.5637 \n",
      "Epoch 00006: val_loss improved from 1.60999 to 1.50710, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 208s 17s/step - loss: 1.5248 - accuracy: 0.5637 - val_loss: 1.5071 - val_accuracy: 0.5341\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.4174 - accuracy: 0.5921 \n",
      "Epoch 00007: val_loss improved from 1.50710 to 1.43458, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 209s 17s/step - loss: 1.4174 - accuracy: 0.5921 - val_loss: 1.4346 - val_accuracy: 0.5341\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3558 - accuracy: 0.5864 \n",
      "Epoch 00008: val_loss improved from 1.43458 to 1.36829, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 206s 17s/step - loss: 1.3558 - accuracy: 0.5864 - val_loss: 1.3683 - val_accuracy: 0.5341\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2920 - accuracy: 0.5666 \n",
      "Epoch 00009: val_loss improved from 1.36829 to 1.31832, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 206s 17s/step - loss: 1.2920 - accuracy: 0.5666 - val_loss: 1.3183 - val_accuracy: 0.5455\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2357 - accuracy: 0.5807 \n",
      "Epoch 00010: val_loss improved from 1.31832 to 1.28276, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-0401202212-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 205s 17s/step - loss: 1.2357 - accuracy: 0.5807 - val_loss: 1.2828 - val_accuracy: 0.5455\n",
      "-------Iteration : 0\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2145 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 1.28001, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 207s 17s/step - loss: 1.2145 - accuracy: 0.5609 - val_loss: 1.2800 - val_accuracy: 0.5455\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2000 - accuracy: 0.5921 \n",
      "Epoch 00002: val_loss improved from 1.28001 to 1.27450, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 221s 18s/step - loss: 1.2000 - accuracy: 0.5921 - val_loss: 1.2745 - val_accuracy: 0.5114\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1927 - accuracy: 0.5892 \n",
      "Epoch 00003: val_loss did not improve from 1.27450\n",
      "12/12 [==============================] - 190s 16s/step - loss: 1.1927 - accuracy: 0.5892 - val_loss: 1.2786 - val_accuracy: 0.5227\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1653 - accuracy: 0.6147 \n",
      "Epoch 00004: val_loss did not improve from 1.27450\n",
      "12/12 [==============================] - 181s 15s/step - loss: 1.1653 - accuracy: 0.6147 - val_loss: 1.2768 - val_accuracy: 0.5227\n",
      "-------Iteration : 1\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2204 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 1.30043, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 204s 17s/step - loss: 1.2204 - accuracy: 0.5807 - val_loss: 1.3004 - val_accuracy: 0.5227\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2058 - accuracy: 0.5892 \n",
      "Epoch 00002: val_loss improved from 1.30043 to 1.29667, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 211s 17s/step - loss: 1.2058 - accuracy: 0.5892 - val_loss: 1.2967 - val_accuracy: 0.5227\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1864 - accuracy: 0.6091 \n",
      "Epoch 00003: val_loss improved from 1.29667 to 1.29509, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 225s 19s/step - loss: 1.1864 - accuracy: 0.6091 - val_loss: 1.2951 - val_accuracy: 0.5227\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1714 - accuracy: 0.5949 \n",
      "Epoch 00004: val_loss improved from 1.29509 to 1.29480, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 205s 17s/step - loss: 1.1714 - accuracy: 0.5949 - val_loss: 1.2948 - val_accuracy: 0.5227\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1528 - accuracy: 0.6119 \n",
      "Epoch 00005: val_loss improved from 1.29480 to 1.29463, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 203s 17s/step - loss: 1.1528 - accuracy: 0.6119 - val_loss: 1.2946 - val_accuracy: 0.5227\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1450 - accuracy: 0.6176 \n",
      "Epoch 00006: val_loss improved from 1.29463 to 1.29020, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 149s 12s/step - loss: 1.1450 - accuracy: 0.6176 - val_loss: 1.2902 - val_accuracy: 0.5114\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1589 - accuracy: 0.5949 \n",
      "Epoch 00007: val_loss improved from 1.29020 to 1.29012, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 1.1589 - accuracy: 0.5949 - val_loss: 1.2901 - val_accuracy: 0.5114\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1485 - accuracy: 0.6119 \n",
      "Epoch 00008: val_loss improved from 1.29012 to 1.28968, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 1.1485 - accuracy: 0.6119 - val_loss: 1.2897 - val_accuracy: 0.5227\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1219 - accuracy: 0.6374 \n",
      "Epoch 00009: val_loss improved from 1.28968 to 1.28564, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 1.1219 - accuracy: 0.6374 - val_loss: 1.2856 - val_accuracy: 0.5227\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1482 - accuracy: 0.6516 \n",
      "Epoch 00010: val_loss improved from 1.28564 to 1.28022, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 1.1482 - accuracy: 0.6516 - val_loss: 1.2802 - val_accuracy: 0.5341\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1391 - accuracy: 0.5892\n",
      "Epoch 00011: val_loss improved from 1.28022 to 1.27655, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.1391 - accuracy: 0.5892 - val_loss: 1.2765 - val_accuracy: 0.5455\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1022 - accuracy: 0.6544\n",
      "Epoch 00012: val_loss improved from 1.27655 to 1.27550, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.1022 - accuracy: 0.6544 - val_loss: 1.2755 - val_accuracy: 0.5455\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1122 - accuracy: 0.6714\n",
      "Epoch 00013: val_loss improved from 1.27550 to 1.27473, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.1122 - accuracy: 0.6714 - val_loss: 1.2747 - val_accuracy: 0.5568\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0891 - accuracy: 0.6402\n",
      "Epoch 00014: val_loss improved from 1.27473 to 1.27225, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0891 - accuracy: 0.6402 - val_loss: 1.2722 - val_accuracy: 0.5455\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0704 - accuracy: 0.6516\n",
      "Epoch 00015: val_loss improved from 1.27225 to 1.26995, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0704 - accuracy: 0.6516 - val_loss: 1.2700 - val_accuracy: 0.5341\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0679 - accuracy: 0.6516\n",
      "Epoch 00016: val_loss improved from 1.26995 to 1.26496, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0679 - accuracy: 0.6516 - val_loss: 1.2650 - val_accuracy: 0.5341\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0816 - accuracy: 0.6544\n",
      "Epoch 00017: val_loss improved from 1.26496 to 1.25945, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0816 - accuracy: 0.6544 - val_loss: 1.2594 - val_accuracy: 0.5341\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0482 - accuracy: 0.6856\n",
      "Epoch 00018: val_loss did not improve from 1.25945\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0482 - accuracy: 0.6856 - val_loss: 1.2620 - val_accuracy: 0.5341\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0455 - accuracy: 0.6544\n",
      "Epoch 00019: val_loss did not improve from 1.25945\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0455 - accuracy: 0.6544 - val_loss: 1.2596 - val_accuracy: 0.5341\n",
      "-------Iteration : 2\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.6629\n",
      "Epoch 00001: val_loss improved from inf to 1.25840, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 1.0745 - accuracy: 0.6629 - val_loss: 1.2584 - val_accuracy: 0.5455\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0381 - accuracy: 0.6601 \n",
      "Epoch 00002: val_loss improved from 1.25840 to 1.25171, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.0381 - accuracy: 0.6601 - val_loss: 1.2517 - val_accuracy: 0.5455\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0397 - accuracy: 0.6686\n",
      "Epoch 00003: val_loss did not improve from 1.25171\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0397 - accuracy: 0.6686 - val_loss: 1.2517 - val_accuracy: 0.5455\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0226 - accuracy: 0.6742\n",
      "Epoch 00004: val_loss improved from 1.25171 to 1.24820, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0226 - accuracy: 0.6742 - val_loss: 1.2482 - val_accuracy: 0.5455\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9927 - accuracy: 0.7167\n",
      "Epoch 00005: val_loss improved from 1.24820 to 1.24476, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9927 - accuracy: 0.7167 - val_loss: 1.2448 - val_accuracy: 0.5455\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9911 - accuracy: 0.6856\n",
      "Epoch 00006: val_loss improved from 1.24476 to 1.24317, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9911 - accuracy: 0.6856 - val_loss: 1.2432 - val_accuracy: 0.5341\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0028 - accuracy: 0.6856 \n",
      "Epoch 00007: val_loss improved from 1.24317 to 1.23688, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 1.0028 - accuracy: 0.6856 - val_loss: 1.2369 - val_accuracy: 0.5568\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0071 - accuracy: 0.6997\n",
      "Epoch 00008: val_loss improved from 1.23688 to 1.23410, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0071 - accuracy: 0.6997 - val_loss: 1.2341 - val_accuracy: 0.5568\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9635 - accuracy: 0.7025\n",
      "Epoch 00009: val_loss improved from 1.23410 to 1.23348, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.9635 - accuracy: 0.7025 - val_loss: 1.2335 - val_accuracy: 0.5455\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.6799\n",
      "Epoch 00010: val_loss did not improve from 1.23348\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.9702 - accuracy: 0.6799 - val_loss: 1.2346 - val_accuracy: 0.5568\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9562 - accuracy: 0.6941\n",
      "Epoch 00011: val_loss improved from 1.23348 to 1.23085, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9562 - accuracy: 0.6941 - val_loss: 1.2309 - val_accuracy: 0.5568\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9302 - accuracy: 0.7450\n",
      "Epoch 00012: val_loss improved from 1.23085 to 1.22925, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9302 - accuracy: 0.7450 - val_loss: 1.2293 - val_accuracy: 0.5455\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9399 - accuracy: 0.7054\n",
      "Epoch 00013: val_loss did not improve from 1.22925\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.9399 - accuracy: 0.7054 - val_loss: 1.2333 - val_accuracy: 0.5455\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9089 - accuracy: 0.7167\n",
      "Epoch 00014: val_loss improved from 1.22925 to 1.22794, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9089 - accuracy: 0.7167 - val_loss: 1.2279 - val_accuracy: 0.5341\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8985 - accuracy: 0.7564\n",
      "Epoch 00015: val_loss improved from 1.22794 to 1.21828, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.8985 - accuracy: 0.7564 - val_loss: 1.2183 - val_accuracy: 0.5455\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8998 - accuracy: 0.7195\n",
      "Epoch 00016: val_loss improved from 1.21828 to 1.21061, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.8998 - accuracy: 0.7195 - val_loss: 1.2106 - val_accuracy: 0.5568\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8936 - accuracy: 0.7280\n",
      "Epoch 00017: val_loss did not improve from 1.21061\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.8936 - accuracy: 0.7280 - val_loss: 1.2132 - val_accuracy: 0.5682\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8752 - accuracy: 0.7564\n",
      "Epoch 00018: val_loss did not improve from 1.21061\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8752 - accuracy: 0.7564 - val_loss: 1.2160 - val_accuracy: 0.5568\n",
      "-------Iteration : 3\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8895 - accuracy: 0.7620\n",
      "Epoch 00001: val_loss improved from inf to 1.22191, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.8895 - accuracy: 0.7620 - val_loss: 1.2219 - val_accuracy: 0.5795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8998 - accuracy: 0.7365\n",
      "Epoch 00002: val_loss did not improve from 1.22191\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8998 - accuracy: 0.7365 - val_loss: 1.2219 - val_accuracy: 0.5568\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8540 - accuracy: 0.7535\n",
      "Epoch 00003: val_loss improved from 1.22191 to 1.21489, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8540 - accuracy: 0.7535 - val_loss: 1.2149 - val_accuracy: 0.5682\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8553 - accuracy: 0.7479\n",
      "Epoch 00004: val_loss improved from 1.21489 to 1.21139, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.8553 - accuracy: 0.7479 - val_loss: 1.2114 - val_accuracy: 0.5909\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8544 - accuracy: 0.7677\n",
      "Epoch 00005: val_loss did not improve from 1.21139\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8544 - accuracy: 0.7677 - val_loss: 1.2115 - val_accuracy: 0.5909\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8363 - accuracy: 0.7649\n",
      "Epoch 00006: val_loss improved from 1.21139 to 1.20813, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.8363 - accuracy: 0.7649 - val_loss: 1.2081 - val_accuracy: 0.5909\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8214 - accuracy: 0.7507\n",
      "Epoch 00007: val_loss improved from 1.20813 to 1.20743, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.8214 - accuracy: 0.7507 - val_loss: 1.2074 - val_accuracy: 0.6023\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8003 - accuracy: 0.7989\n",
      "Epoch 00008: val_loss did not improve from 1.20743\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.8003 - accuracy: 0.7989 - val_loss: 1.2083 - val_accuracy: 0.5909\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.7790\n",
      "Epoch 00009: val_loss improved from 1.20743 to 1.20573, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8117 - accuracy: 0.7790 - val_loss: 1.2057 - val_accuracy: 0.6023\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7920 - accuracy: 0.7762\n",
      "Epoch 00010: val_loss improved from 1.20573 to 1.20183, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.7920 - accuracy: 0.7762 - val_loss: 1.2018 - val_accuracy: 0.6023\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7623 - accuracy: 0.8159\n",
      "Epoch 00011: val_loss did not improve from 1.20183\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7623 - accuracy: 0.8159 - val_loss: 1.2070 - val_accuracy: 0.6136\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7835 - accuracy: 0.7989\n",
      "Epoch 00012: val_loss did not improve from 1.20183\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.7835 - accuracy: 0.7989 - val_loss: 1.2147 - val_accuracy: 0.6364\n",
      "-------Iteration : 4\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8066 - accuracy: 0.7960\n",
      "Epoch 00001: val_loss improved from inf to 1.20542, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8066 - accuracy: 0.7960 - val_loss: 1.2054 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7693 - accuracy: 0.8187\n",
      "Epoch 00002: val_loss did not improve from 1.20542\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7693 - accuracy: 0.8187 - val_loss: 1.2185 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7816 - accuracy: 0.7904\n",
      "Epoch 00003: val_loss did not improve from 1.20542\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.7816 - accuracy: 0.7904 - val_loss: 1.2099 - val_accuracy: 0.6136\n",
      "-------Iteration : 5\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8036 - accuracy: 0.7904\n",
      "Epoch 00001: val_loss improved from inf to 1.20832, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8036 - accuracy: 0.7904 - val_loss: 1.2083 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7811 - accuracy: 0.7932\n",
      "Epoch 00002: val_loss did not improve from 1.20832\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7811 - accuracy: 0.7932 - val_loss: 1.2312 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7304 - accuracy: 0.8244\n",
      "Epoch 00003: val_loss did not improve from 1.20832\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7304 - accuracy: 0.8244 - val_loss: 1.2136 - val_accuracy: 0.6364\n",
      "-------Iteration : 6\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7718 - accuracy: 0.8017\n",
      "Epoch 00001: val_loss improved from inf to 1.21455, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7718 - accuracy: 0.8017 - val_loss: 1.2146 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7568 - accuracy: 0.7989\n",
      "Epoch 00002: val_loss did not improve from 1.21455\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7568 - accuracy: 0.7989 - val_loss: 1.2372 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7467 - accuracy: 0.8187\n",
      "Epoch 00003: val_loss improved from 1.21455 to 1.21206, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.7467 - accuracy: 0.8187 - val_loss: 1.2121 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7452 - accuracy: 0.8159\n",
      "Epoch 00004: val_loss did not improve from 1.21206\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7452 - accuracy: 0.8159 - val_loss: 1.2143 - val_accuracy: 0.6250\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7225 - accuracy: 0.8300\n",
      "Epoch 00005: val_loss improved from 1.21206 to 1.20313, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7225 - accuracy: 0.8300 - val_loss: 1.2031 - val_accuracy: 0.6136\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7271 - accuracy: 0.8385\n",
      "Epoch 00006: val_loss improved from 1.20313 to 1.19224, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7271 - accuracy: 0.8385 - val_loss: 1.1922 - val_accuracy: 0.6250\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7034 - accuracy: 0.8385\n",
      "Epoch 00007: val_loss did not improve from 1.19224\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7034 - accuracy: 0.8385 - val_loss: 1.2012 - val_accuracy: 0.6250\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.8357\n",
      "Epoch 00008: val_loss did not improve from 1.19224\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7001 - accuracy: 0.8357 - val_loss: 1.2242 - val_accuracy: 0.6364\n",
      "-------Iteration : 7\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7224 - accuracy: 0.8272\n",
      "Epoch 00001: val_loss improved from inf to 1.20394, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7224 - accuracy: 0.8272 - val_loss: 1.2039 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7180 - accuracy: 0.8272\n",
      "Epoch 00002: val_loss did not improve from 1.20394\n",
      "12/12 [==============================] - 109s 9s/step - loss: 0.7180 - accuracy: 0.8272 - val_loss: 1.2236 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7196 - accuracy: 0.8187\n",
      "Epoch 00003: val_loss did not improve from 1.20394\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7196 - accuracy: 0.8187 - val_loss: 1.2098 - val_accuracy: 0.6250\n",
      "-------Iteration : 8\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.8102\n",
      "Epoch 00001: val_loss improved from inf to 1.19973, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7501 - accuracy: 0.8102 - val_loss: 1.1997 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.8584\n",
      "Epoch 00002: val_loss did not improve from 1.19973\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6674 - accuracy: 0.8584 - val_loss: 1.2145 - val_accuracy: 0.6364\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.8555\n",
      "Epoch 00003: val_loss did not improve from 1.19973\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6912 - accuracy: 0.8555 - val_loss: 1.2240 - val_accuracy: 0.6364\n",
      "-------Iteration : 9\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7202 - accuracy: 0.8442\n",
      "Epoch 00001: val_loss improved from inf to 1.20572, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-0401202212-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7202 - accuracy: 0.8442 - val_loss: 1.2057 - val_accuracy: 0.5909\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.8612\n",
      "Epoch 00002: val_loss did not improve from 1.20572\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6820 - accuracy: 0.8612 - val_loss: 1.2203 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.8612\n",
      "Epoch 00003: val_loss did not improve from 1.20572\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6594 - accuracy: 0.8612 - val_loss: 1.2207 - val_accuracy: 0.6250\n",
      "Evaluating samm-024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\train.py:191: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  f1_0 = 0 if (tp0 != 0 and fn0 != 0 and fp0 != 0) else (2*tp0)/((2*tp0) + fp0 + fn0)\n",
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\train.py:192: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  f1_1 = 0 if (tp1 != 0 and fn1 != 0 and fp1 != 0) else (2*tp1)/((2*tp1) + fp1 + fn1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Base experiment - 1H iter 10HEp 10 FT iter 20FtEp\n",
    "%run 3classBaseExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(noOfHeadIter=1, noOfFtIter=10, noOfHeadEpoch=50, noOfFtEpoch=100, outp='output')\n",
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling_4 (Rescaling)        (None, 224, 224, 3)  0           ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " normalization_4 (Normalization  (None, 224, 224, 3)  7          ['rescaling_4[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['normalization_4[0][0]']        \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',     \n",
      "                                                                  'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Learn the head\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3076 - accuracy: 0.5071\n",
      "Epoch 00001: val_loss improved from inf to 2.11548, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.3076 - accuracy: 0.5071 - val_loss: 2.1155 - val_accuracy: 0.5698\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0933 - accuracy: 0.5354\n",
      "Epoch 00002: val_loss improved from 2.11548 to 1.93923, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.0933 - accuracy: 0.5354 - val_loss: 1.9392 - val_accuracy: 0.5698\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.9039 - accuracy: 0.5439 \n",
      "Epoch 00003: val_loss improved from 1.93923 to 1.80436, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 1.9039 - accuracy: 0.5439 - val_loss: 1.8044 - val_accuracy: 0.5814\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.7629 - accuracy: 0.5496 \n",
      "Epoch 00004: val_loss improved from 1.80436 to 1.68826, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 1.7629 - accuracy: 0.5496 - val_loss: 1.6883 - val_accuracy: 0.5814\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6470 - accuracy: 0.5637 \n",
      "Epoch 00005: val_loss improved from 1.68826 to 1.58590, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 1.6470 - accuracy: 0.5637 - val_loss: 1.5859 - val_accuracy: 0.5814\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.5559 - accuracy: 0.5751 \n",
      "Epoch 00006: val_loss improved from 1.58590 to 1.50193, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 1.5559 - accuracy: 0.5751 - val_loss: 1.5019 - val_accuracy: 0.5581\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.4626 - accuracy: 0.5581 \n",
      "Epoch 00007: val_loss improved from 1.50193 to 1.42867, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 1.4626 - accuracy: 0.5581 - val_loss: 1.4287 - val_accuracy: 0.5465\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3843 - accuracy: 0.5977 \n",
      "Epoch 00008: val_loss improved from 1.42867 to 1.36783, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 1.3843 - accuracy: 0.5977 - val_loss: 1.3678 - val_accuracy: 0.5465\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3375 - accuracy: 0.5637 \n",
      "Epoch 00009: val_loss improved from 1.36783 to 1.31690, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.3375 - accuracy: 0.5637 - val_loss: 1.3169 - val_accuracy: 0.5581\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2809 - accuracy: 0.5694 \n",
      "Epoch 00010: val_loss improved from 1.31690 to 1.27721, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.2809 - accuracy: 0.5694 - val_loss: 1.2772 - val_accuracy: 0.5698\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2307 - accuracy: 0.5779 \n",
      "Epoch 00011: val_loss improved from 1.27721 to 1.23833, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.2307 - accuracy: 0.5779 - val_loss: 1.2383 - val_accuracy: 0.5581\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2049 - accuracy: 0.5722\n",
      "Epoch 00012: val_loss improved from 1.23833 to 1.20861, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 107s 9s/step - loss: 1.2049 - accuracy: 0.5722 - val_loss: 1.2086 - val_accuracy: 0.5581\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1786 - accuracy: 0.5779\n",
      "Epoch 00013: val_loss improved from 1.20861 to 1.18494, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 1.1786 - accuracy: 0.5779 - val_loss: 1.1849 - val_accuracy: 0.5698\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1401 - accuracy: 0.5892\n",
      "Epoch 00014: val_loss improved from 1.18494 to 1.16742, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.1401 - accuracy: 0.5892 - val_loss: 1.1674 - val_accuracy: 0.5581\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1445 - accuracy: 0.5949\n",
      "Epoch 00015: val_loss improved from 1.16742 to 1.15574, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.1445 - accuracy: 0.5949 - val_loss: 1.1557 - val_accuracy: 0.5698\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1227 - accuracy: 0.5609\n",
      "Epoch 00016: val_loss improved from 1.15574 to 1.14325, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.1227 - accuracy: 0.5609 - val_loss: 1.1433 - val_accuracy: 0.5581\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0951 - accuracy: 0.5836\n",
      "Epoch 00017: val_loss improved from 1.14325 to 1.13012, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0951 - accuracy: 0.5836 - val_loss: 1.1301 - val_accuracy: 0.5465\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1057 - accuracy: 0.5751\n",
      "Epoch 00018: val_loss improved from 1.13012 to 1.12508, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.1057 - accuracy: 0.5751 - val_loss: 1.1251 - val_accuracy: 0.5581\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0897 - accuracy: 0.5836\n",
      "Epoch 00019: val_loss improved from 1.12508 to 1.12051, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0897 - accuracy: 0.5836 - val_loss: 1.1205 - val_accuracy: 0.5581\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0977 - accuracy: 0.5921\n",
      "Epoch 00020: val_loss improved from 1.12051 to 1.11237, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0977 - accuracy: 0.5921 - val_loss: 1.1124 - val_accuracy: 0.5465\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0728 - accuracy: 0.5807\n",
      "Epoch 00021: val_loss improved from 1.11237 to 1.10124, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0728 - accuracy: 0.5807 - val_loss: 1.1012 - val_accuracy: 0.5465\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0618 - accuracy: 0.6006\n",
      "Epoch 00022: val_loss improved from 1.10124 to 1.09457, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0618 - accuracy: 0.6006 - val_loss: 1.0946 - val_accuracy: 0.5465\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0698 - accuracy: 0.5609\n",
      "Epoch 00023: val_loss improved from 1.09457 to 1.09363, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0698 - accuracy: 0.5609 - val_loss: 1.0936 - val_accuracy: 0.5465\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0595 - accuracy: 0.5864\n",
      "Epoch 00024: val_loss improved from 1.09363 to 1.09352, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0595 - accuracy: 0.5864 - val_loss: 1.0935 - val_accuracy: 0.5465\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0567 - accuracy: 0.5496\n",
      "Epoch 00025: val_loss improved from 1.09352 to 1.08884, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0567 - accuracy: 0.5496 - val_loss: 1.0888 - val_accuracy: 0.5465\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0493 - accuracy: 0.6006\n",
      "Epoch 00026: val_loss improved from 1.08884 to 1.08274, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0493 - accuracy: 0.6006 - val_loss: 1.0827 - val_accuracy: 0.5581\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0401 - accuracy: 0.5892\n",
      "Epoch 00027: val_loss improved from 1.08274 to 1.08077, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0401 - accuracy: 0.5892 - val_loss: 1.0808 - val_accuracy: 0.5581\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0505 - accuracy: 0.5977\n",
      "Epoch 00028: val_loss did not improve from 1.08077\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0505 - accuracy: 0.5977 - val_loss: 1.0827 - val_accuracy: 0.5465\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0360 - accuracy: 0.6006\n",
      "Epoch 00029: val_loss improved from 1.08077 to 1.08029, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 1.0360 - accuracy: 0.6006 - val_loss: 1.0803 - val_accuracy: 0.5465\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0350 - accuracy: 0.6034\n",
      "Epoch 00030: val_loss improved from 1.08029 to 1.07629, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0350 - accuracy: 0.6034 - val_loss: 1.0763 - val_accuracy: 0.5465\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0503 - accuracy: 0.5779\n",
      "Epoch 00031: val_loss improved from 1.07629 to 1.07559, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0503 - accuracy: 0.5779 - val_loss: 1.0756 - val_accuracy: 0.5465\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0618 - accuracy: 0.5751\n",
      "Epoch 00032: val_loss improved from 1.07559 to 1.07438, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-0401202215-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0618 - accuracy: 0.5751 - val_loss: 1.0744 - val_accuracy: 0.5465\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0415 - accuracy: 0.5751\n",
      "Epoch 00033: val_loss did not improve from 1.07438\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0415 - accuracy: 0.5751 - val_loss: 1.0778 - val_accuracy: 0.5698\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0330 - accuracy: 0.6147\n",
      "Epoch 00034: val_loss did not improve from 1.07438\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0330 - accuracy: 0.6147 - val_loss: 1.0781 - val_accuracy: 0.5465\n",
      "-------Iteration : 0\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0466 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 1.08848, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0466 - accuracy: 0.5864 - val_loss: 1.0885 - val_accuracy: 0.5698\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0410 - accuracy: 0.5864\n",
      "Epoch 00002: val_loss improved from 1.08848 to 1.08679, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 1.0410 - accuracy: 0.5864 - val_loss: 1.0868 - val_accuracy: 0.5814\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0258 - accuracy: 0.5836\n",
      "Epoch 00003: val_loss did not improve from 1.08679\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0258 - accuracy: 0.5836 - val_loss: 1.0870 - val_accuracy: 0.5465\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0154 - accuracy: 0.5977\n",
      "Epoch 00004: val_loss improved from 1.08679 to 1.08431, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0154 - accuracy: 0.5977 - val_loss: 1.0843 - val_accuracy: 0.5465\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0093 - accuracy: 0.5864\n",
      "Epoch 00005: val_loss improved from 1.08431 to 1.07906, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0093 - accuracy: 0.5864 - val_loss: 1.0791 - val_accuracy: 0.5465\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0039 - accuracy: 0.6006\n",
      "Epoch 00006: val_loss improved from 1.07906 to 1.07691, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0039 - accuracy: 0.6006 - val_loss: 1.0769 - val_accuracy: 0.5465\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0041 - accuracy: 0.5864\n",
      "Epoch 00007: val_loss improved from 1.07691 to 1.07539, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0041 - accuracy: 0.5864 - val_loss: 1.0754 - val_accuracy: 0.5349\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9941 - accuracy: 0.5949\n",
      "Epoch 00008: val_loss improved from 1.07539 to 1.07506, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9941 - accuracy: 0.5949 - val_loss: 1.0751 - val_accuracy: 0.5465\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9740 - accuracy: 0.6232\n",
      "Epoch 00009: val_loss improved from 1.07506 to 1.07373, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9740 - accuracy: 0.6232 - val_loss: 1.0737 - val_accuracy: 0.5465\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9773 - accuracy: 0.6119\n",
      "Epoch 00010: val_loss improved from 1.07373 to 1.06994, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9773 - accuracy: 0.6119 - val_loss: 1.0699 - val_accuracy: 0.5465\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9743 - accuracy: 0.6034\n",
      "Epoch 00011: val_loss improved from 1.06994 to 1.06971, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9743 - accuracy: 0.6034 - val_loss: 1.0697 - val_accuracy: 0.5581\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9633 - accuracy: 0.6176\n",
      "Epoch 00012: val_loss improved from 1.06971 to 1.06946, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9633 - accuracy: 0.6176 - val_loss: 1.0695 - val_accuracy: 0.5581\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9706 - accuracy: 0.5977\n",
      "Epoch 00013: val_loss improved from 1.06946 to 1.06635, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9706 - accuracy: 0.5977 - val_loss: 1.0664 - val_accuracy: 0.5465\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9347 - accuracy: 0.6317\n",
      "Epoch 00014: val_loss improved from 1.06635 to 1.06200, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9347 - accuracy: 0.6317 - val_loss: 1.0620 - val_accuracy: 0.5581\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9407 - accuracy: 0.6176\n",
      "Epoch 00015: val_loss improved from 1.06200 to 1.05894, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9407 - accuracy: 0.6176 - val_loss: 1.0589 - val_accuracy: 0.5465\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9286 - accuracy: 0.6289\n",
      "Epoch 00016: val_loss improved from 1.05894 to 1.05814, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9286 - accuracy: 0.6289 - val_loss: 1.0581 - val_accuracy: 0.5581\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9337 - accuracy: 0.6317\n",
      "Epoch 00017: val_loss improved from 1.05814 to 1.05385, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9337 - accuracy: 0.6317 - val_loss: 1.0539 - val_accuracy: 0.5698\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9156 - accuracy: 0.6544\n",
      "Epoch 00018: val_loss improved from 1.05385 to 1.04926, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9156 - accuracy: 0.6544 - val_loss: 1.0493 - val_accuracy: 0.5581\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9159 - accuracy: 0.6402\n",
      "Epoch 00019: val_loss improved from 1.04926 to 1.04521, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9159 - accuracy: 0.6402 - val_loss: 1.0452 - val_accuracy: 0.5581\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9020 - accuracy: 0.6402\n",
      "Epoch 00020: val_loss improved from 1.04521 to 1.04466, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9020 - accuracy: 0.6402 - val_loss: 1.0447 - val_accuracy: 0.5698\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8886 - accuracy: 0.6742\n",
      "Epoch 00021: val_loss improved from 1.04466 to 1.04150, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8886 - accuracy: 0.6742 - val_loss: 1.0415 - val_accuracy: 0.6047\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8726 - accuracy: 0.6799\n",
      "Epoch 00022: val_loss improved from 1.04150 to 1.03625, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8726 - accuracy: 0.6799 - val_loss: 1.0363 - val_accuracy: 0.6047\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8717 - accuracy: 0.6686\n",
      "Epoch 00023: val_loss improved from 1.03625 to 1.03255, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8717 - accuracy: 0.6686 - val_loss: 1.0325 - val_accuracy: 0.6047\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8632 - accuracy: 0.6799\n",
      "Epoch 00024: val_loss improved from 1.03255 to 1.03140, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8632 - accuracy: 0.6799 - val_loss: 1.0314 - val_accuracy: 0.6047\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8643 - accuracy: 0.6686\n",
      "Epoch 00025: val_loss improved from 1.03140 to 1.02976, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.8643 - accuracy: 0.6686 - val_loss: 1.0298 - val_accuracy: 0.6047\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8445 - accuracy: 0.6714\n",
      "Epoch 00026: val_loss improved from 1.02976 to 1.02783, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.8445 - accuracy: 0.6714 - val_loss: 1.0278 - val_accuracy: 0.6047\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8290 - accuracy: 0.6941\n",
      "Epoch 00027: val_loss improved from 1.02783 to 1.02662, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8290 - accuracy: 0.6941 - val_loss: 1.0266 - val_accuracy: 0.6163\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.6941\n",
      "Epoch 00028: val_loss improved from 1.02662 to 1.02405, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8085 - accuracy: 0.6941 - val_loss: 1.0240 - val_accuracy: 0.6163\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.6827\n",
      "Epoch 00029: val_loss improved from 1.02405 to 1.02053, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8196 - accuracy: 0.6827 - val_loss: 1.0205 - val_accuracy: 0.6395\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7873 - accuracy: 0.7054\n",
      "Epoch 00030: val_loss improved from 1.02053 to 1.01650, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7873 - accuracy: 0.7054 - val_loss: 1.0165 - val_accuracy: 0.6279\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7884 - accuracy: 0.7054\n",
      "Epoch 00031: val_loss improved from 1.01650 to 1.01548, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7884 - accuracy: 0.7054 - val_loss: 1.0155 - val_accuracy: 0.6512\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7844 - accuracy: 0.7195\n",
      "Epoch 00032: val_loss improved from 1.01548 to 1.01533, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7844 - accuracy: 0.7195 - val_loss: 1.0153 - val_accuracy: 0.6395\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7644 - accuracy: 0.7195\n",
      "Epoch 00033: val_loss improved from 1.01533 to 1.01516, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7644 - accuracy: 0.7195 - val_loss: 1.0152 - val_accuracy: 0.6395\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7550 - accuracy: 0.7110\n",
      "Epoch 00034: val_loss did not improve from 1.01516\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7550 - accuracy: 0.7110 - val_loss: 1.0168 - val_accuracy: 0.6628\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7464 - accuracy: 0.7309\n",
      "Epoch 00035: val_loss improved from 1.01516 to 1.01387, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7464 - accuracy: 0.7309 - val_loss: 1.0139 - val_accuracy: 0.6163\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7334 - accuracy: 0.7450\n",
      "Epoch 00036: val_loss did not improve from 1.01387\n",
      "12/12 [==============================] - 109s 9s/step - loss: 0.7334 - accuracy: 0.7450 - val_loss: 1.0162 - val_accuracy: 0.6395\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7170 - accuracy: 0.7252\n",
      "Epoch 00037: val_loss did not improve from 1.01387\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7170 - accuracy: 0.7252 - val_loss: 1.0183 - val_accuracy: 0.6512\n",
      "-------Iteration : 1\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7526 - accuracy: 0.7224\n",
      "Epoch 00001: val_loss improved from inf to 1.00756, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7526 - accuracy: 0.7224 - val_loss: 1.0076 - val_accuracy: 0.6395\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7313 - accuracy: 0.7450\n",
      "Epoch 00002: val_loss did not improve from 1.00756\n",
      "12/12 [==============================] - 109s 9s/step - loss: 0.7313 - accuracy: 0.7450 - val_loss: 1.0167 - val_accuracy: 0.6512\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7069 - accuracy: 0.7507\n",
      "Epoch 00003: val_loss did not improve from 1.00756\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7069 - accuracy: 0.7507 - val_loss: 1.0139 - val_accuracy: 0.6279\n",
      "-------Iteration : 2\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.7195\n",
      "Epoch 00001: val_loss improved from inf to 1.00830, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7419 - accuracy: 0.7195 - val_loss: 1.0083 - val_accuracy: 0.6395\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.7507\n",
      "Epoch 00002: val_loss did not improve from 1.00830\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7037 - accuracy: 0.7507 - val_loss: 1.0113 - val_accuracy: 0.6512\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.7195\n",
      "Epoch 00003: val_loss did not improve from 1.00830\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7090 - accuracy: 0.7195 - val_loss: 1.0109 - val_accuracy: 0.6512\n",
      "-------Iteration : 3\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.7309\n",
      "Epoch 00001: val_loss improved from inf to 1.01148, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7174 - accuracy: 0.7309 - val_loss: 1.0115 - val_accuracy: 0.6628\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.7564\n",
      "Epoch 00002: val_loss did not improve from 1.01148\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6853 - accuracy: 0.7564 - val_loss: 1.0250 - val_accuracy: 0.6395\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6856 - accuracy: 0.7535\n",
      "Epoch 00003: val_loss did not improve from 1.01148\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6856 - accuracy: 0.7535 - val_loss: 1.0213 - val_accuracy: 0.6512\n",
      "-------Iteration : 4\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7217 - accuracy: 0.7252\n",
      "Epoch 00001: val_loss improved from inf to 1.01457, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.7217 - accuracy: 0.7252 - val_loss: 1.0146 - val_accuracy: 0.6512\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7080 - accuracy: 0.7422\n",
      "Epoch 00002: val_loss did not improve from 1.01457\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.7080 - accuracy: 0.7422 - val_loss: 1.0165 - val_accuracy: 0.6512\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7052 - accuracy: 0.7280\n",
      "Epoch 00003: val_loss improved from 1.01457 to 1.01209, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7052 - accuracy: 0.7280 - val_loss: 1.0121 - val_accuracy: 0.6628\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6806 - accuracy: 0.7507\n",
      "Epoch 00004: val_loss did not improve from 1.01209\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6806 - accuracy: 0.7507 - val_loss: 1.0170 - val_accuracy: 0.6628\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.7734\n",
      "Epoch 00005: val_loss did not improve from 1.01209\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6584 - accuracy: 0.7734 - val_loss: 1.0194 - val_accuracy: 0.6628\n",
      "-------Iteration : 5\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.7535\n",
      "Epoch 00001: val_loss improved from inf to 1.01612, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.6821 - accuracy: 0.7535 - val_loss: 1.0161 - val_accuracy: 0.6512\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6641 - accuracy: 0.7649\n",
      "Epoch 00002: val_loss did not improve from 1.01612\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6641 - accuracy: 0.7649 - val_loss: 1.0288 - val_accuracy: 0.6512\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6582 - accuracy: 0.7819\n",
      "Epoch 00003: val_loss did not improve from 1.01612\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6582 - accuracy: 0.7819 - val_loss: 1.0249 - val_accuracy: 0.6512\n",
      "-------Iteration : 6\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6836 - accuracy: 0.7620\n",
      "Epoch 00001: val_loss improved from inf to 1.01744, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6836 - accuracy: 0.7620 - val_loss: 1.0174 - val_accuracy: 0.6512\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.7762\n",
      "Epoch 00002: val_loss did not improve from 1.01744\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6533 - accuracy: 0.7762 - val_loss: 1.0363 - val_accuracy: 0.6279\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.7819\n",
      "Epoch 00003: val_loss did not improve from 1.01744\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6370 - accuracy: 0.7819 - val_loss: 1.0255 - val_accuracy: 0.6628\n",
      "-------Iteration : 7\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.7479\n",
      "Epoch 00001: val_loss improved from inf to 1.02683, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6741 - accuracy: 0.7479 - val_loss: 1.0268 - val_accuracy: 0.6512\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6436 - accuracy: 0.7705\n",
      "Epoch 00002: val_loss did not improve from 1.02683\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6436 - accuracy: 0.7705 - val_loss: 1.0451 - val_accuracy: 0.6395\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6487 - accuracy: 0.7592\n",
      "Epoch 00003: val_loss did not improve from 1.02683\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6487 - accuracy: 0.7592 - val_loss: 1.0325 - val_accuracy: 0.6512\n",
      "-------Iteration : 8\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6614 - accuracy: 0.7649\n",
      "Epoch 00001: val_loss improved from inf to 1.02617, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6614 - accuracy: 0.7649 - val_loss: 1.0262 - val_accuracy: 0.6512\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6307 - accuracy: 0.7904\n",
      "Epoch 00002: val_loss did not improve from 1.02617\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6307 - accuracy: 0.7904 - val_loss: 1.0501 - val_accuracy: 0.6512\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6258 - accuracy: 0.7932\n",
      "Epoch 00003: val_loss did not improve from 1.02617\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6258 - accuracy: 0.7932 - val_loss: 1.0333 - val_accuracy: 0.6163\n",
      "-------Iteration : 9\n",
      "Fine-tuning whole network\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6462 - accuracy: 0.7705\n",
      "Epoch 00001: val_loss improved from inf to 1.03654, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-0401202215-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6462 - accuracy: 0.7705 - val_loss: 1.0365 - val_accuracy: 0.6628\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.7790\n",
      "Epoch 00002: val_loss did not improve from 1.03654\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6293 - accuracy: 0.7790 - val_loss: 1.0383 - val_accuracy: 0.6512\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6154 - accuracy: 0.7989\n",
      "Epoch 00003: val_loss did not improve from 1.03654\n",
      "12/12 [==============================] - 109s 9s/step - loss: 0.6154 - accuracy: 0.7989 - val_loss: 1.0457 - val_accuracy: 0.6628\n",
      "Evaluating casme2-sub16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\train.py:192: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  f1_1 = 0 if (tp1 != 0 and fn1 != 0 and fp1 != 0) else (2*tp1)/((2*tp1) + fp1 + fn1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#multiply no of epochs by 5\n",
    "%run 3classBaseExperiment --headEpoch 50 --ftEpoch 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(noOfHeadIter=1, noOfFtIter=10, noOfHeadEpoch=100, noOfFtEpoch=200, outp='output')\n",
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling_5 (Rescaling)        (None, 224, 224, 3)  0           ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " normalization_5 (Normalization  (None, 224, 224, 3)  7          ['rescaling_5[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['normalization_5[0][0]']        \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',     \n",
      "                                                                  'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Learn the head\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3562 - accuracy: 0.4986\n",
      "Epoch 00001: val_loss improved from inf to 2.11766, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.3562 - accuracy: 0.4986 - val_loss: 2.1177 - val_accuracy: 0.5747\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0643 - accuracy: 0.5637\n",
      "Epoch 00002: val_loss improved from 2.11766 to 1.92489, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 2.0643 - accuracy: 0.5637 - val_loss: 1.9249 - val_accuracy: 0.5747\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.8591 - accuracy: 0.5524\n",
      "Epoch 00003: val_loss improved from 1.92489 to 1.75971, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.8591 - accuracy: 0.5524 - val_loss: 1.7597 - val_accuracy: 0.5747\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.7276 - accuracy: 0.5779\n",
      "Epoch 00004: val_loss improved from 1.75971 to 1.61506, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 1.7276 - accuracy: 0.5779 - val_loss: 1.6151 - val_accuracy: 0.5747\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6028 - accuracy: 0.5524\n",
      "Epoch 00005: val_loss improved from 1.61506 to 1.49462, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 1.6028 - accuracy: 0.5524 - val_loss: 1.4946 - val_accuracy: 0.5747\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.4844 - accuracy: 0.5694\n",
      "Epoch 00006: val_loss improved from 1.49462 to 1.39551, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 1.4844 - accuracy: 0.5694 - val_loss: 1.3955 - val_accuracy: 0.5747\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.4071 - accuracy: 0.5666\n",
      "Epoch 00007: val_loss improved from 1.39551 to 1.32174, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 1.4071 - accuracy: 0.5666 - val_loss: 1.3217 - val_accuracy: 0.5747\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3085 - accuracy: 0.5722\n",
      "Epoch 00008: val_loss improved from 1.32174 to 1.25516, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.3085 - accuracy: 0.5722 - val_loss: 1.2552 - val_accuracy: 0.5747\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2677 - accuracy: 0.5637\n",
      "Epoch 00009: val_loss improved from 1.25516 to 1.20068, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.2677 - accuracy: 0.5637 - val_loss: 1.2007 - val_accuracy: 0.5747\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2084 - accuracy: 0.5779\n",
      "Epoch 00010: val_loss improved from 1.20068 to 1.15405, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.2084 - accuracy: 0.5779 - val_loss: 1.1540 - val_accuracy: 0.5747\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1694 - accuracy: 0.5779\n",
      "Epoch 00011: val_loss improved from 1.15405 to 1.12811, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.1694 - accuracy: 0.5779 - val_loss: 1.1281 - val_accuracy: 0.5747\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1173 - accuracy: 0.5666\n",
      "Epoch 00012: val_loss improved from 1.12811 to 1.11062, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.1173 - accuracy: 0.5666 - val_loss: 1.1106 - val_accuracy: 0.5747\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1011 - accuracy: 0.5779\n",
      "Epoch 00013: val_loss improved from 1.11062 to 1.08316, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.1011 - accuracy: 0.5779 - val_loss: 1.0832 - val_accuracy: 0.5747\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1026 - accuracy: 0.5779\n",
      "Epoch 00014: val_loss improved from 1.08316 to 1.06976, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.1026 - accuracy: 0.5779 - val_loss: 1.0698 - val_accuracy: 0.5747\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0891 - accuracy: 0.5694\n",
      "Epoch 00015: val_loss improved from 1.06976 to 1.05799, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0891 - accuracy: 0.5694 - val_loss: 1.0580 - val_accuracy: 0.5747\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0688 - accuracy: 0.5751\n",
      "Epoch 00016: val_loss improved from 1.05799 to 1.05039, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0688 - accuracy: 0.5751 - val_loss: 1.0504 - val_accuracy: 0.5747\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0621 - accuracy: 0.5751\n",
      "Epoch 00017: val_loss improved from 1.05039 to 1.04019, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0621 - accuracy: 0.5751 - val_loss: 1.0402 - val_accuracy: 0.5747\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0530 - accuracy: 0.5722\n",
      "Epoch 00018: val_loss improved from 1.04019 to 1.03598, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0530 - accuracy: 0.5722 - val_loss: 1.0360 - val_accuracy: 0.5747\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0643 - accuracy: 0.5722\n",
      "Epoch 00019: val_loss improved from 1.03598 to 1.02900, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0643 - accuracy: 0.5722 - val_loss: 1.0290 - val_accuracy: 0.5747\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0446 - accuracy: 0.5637\n",
      "Epoch 00020: val_loss improved from 1.02900 to 1.02367, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0446 - accuracy: 0.5637 - val_loss: 1.0237 - val_accuracy: 0.5747\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0359 - accuracy: 0.5751\n",
      "Epoch 00021: val_loss improved from 1.02367 to 1.01713, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0359 - accuracy: 0.5751 - val_loss: 1.0171 - val_accuracy: 0.5747\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0383 - accuracy: 0.5666\n",
      "Epoch 00022: val_loss improved from 1.01713 to 1.01206, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0383 - accuracy: 0.5666 - val_loss: 1.0121 - val_accuracy: 0.5747\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0193 - accuracy: 0.5751\n",
      "Epoch 00023: val_loss improved from 1.01206 to 1.00915, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0193 - accuracy: 0.5751 - val_loss: 1.0092 - val_accuracy: 0.5747\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0195 - accuracy: 0.5722\n",
      "Epoch 00024: val_loss improved from 1.00915 to 1.00890, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0195 - accuracy: 0.5722 - val_loss: 1.0089 - val_accuracy: 0.5747\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0196 - accuracy: 0.5836\n",
      "Epoch 00025: val_loss improved from 1.00890 to 1.00760, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0196 - accuracy: 0.5836 - val_loss: 1.0076 - val_accuracy: 0.5747\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0297 - accuracy: 0.5694\n",
      "Epoch 00026: val_loss improved from 1.00760 to 1.00425, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0297 - accuracy: 0.5694 - val_loss: 1.0042 - val_accuracy: 0.5747\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0155 - accuracy: 0.5722\n",
      "Epoch 00027: val_loss improved from 1.00425 to 1.00339, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0155 - accuracy: 0.5722 - val_loss: 1.0034 - val_accuracy: 0.5747\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0212 - accuracy: 0.5666\n",
      "Epoch 00028: val_loss did not improve from 1.00339\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0212 - accuracy: 0.5666 - val_loss: 1.0049 - val_accuracy: 0.5747\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0172 - accuracy: 0.5722\n",
      "Epoch 00029: val_loss improved from 1.00339 to 1.00268, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0172 - accuracy: 0.5722 - val_loss: 1.0027 - val_accuracy: 0.5747\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0030 - accuracy: 0.5779\n",
      "Epoch 00030: val_loss improved from 1.00268 to 1.00090, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0030 - accuracy: 0.5779 - val_loss: 1.0009 - val_accuracy: 0.5747\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0180 - accuracy: 0.5666\n",
      "Epoch 00031: val_loss did not improve from 1.00090\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0180 - accuracy: 0.5666 - val_loss: 1.0011 - val_accuracy: 0.5747\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9982 - accuracy: 0.5722\n",
      "Epoch 00032: val_loss improved from 1.00090 to 0.99959, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-0401202218-dropout50reg-3class.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9982 - accuracy: 0.5722 - val_loss: 0.9996 - val_accuracy: 0.5747\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0022 - accuracy: 0.5694\n",
      "Epoch 00033: val_loss did not improve from 0.99959\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0022 - accuracy: 0.5694 - val_loss: 1.0014 - val_accuracy: 0.5747\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0048 - accuracy: 0.5694\n",
      "Epoch 00034: val_loss did not improve from 0.99959\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0048 - accuracy: 0.5694 - val_loss: 1.0007 - val_accuracy: 0.5747\n",
      "-------Iteration : 0\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0295 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 1.00122, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0295 - accuracy: 0.5694 - val_loss: 1.0012 - val_accuracy: 0.5747\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0146 - accuracy: 0.5722\n",
      "Epoch 00002: val_loss improved from 1.00122 to 0.99696, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0146 - accuracy: 0.5722 - val_loss: 0.9970 - val_accuracy: 0.5747\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.5666\n",
      "Epoch 00003: val_loss improved from 0.99696 to 0.99353, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0086 - accuracy: 0.5666 - val_loss: 0.9935 - val_accuracy: 0.5747\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0023 - accuracy: 0.5722\n",
      "Epoch 00004: val_loss improved from 0.99353 to 0.98644, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0023 - accuracy: 0.5722 - val_loss: 0.9864 - val_accuracy: 0.5862\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9875 - accuracy: 0.5779\n",
      "Epoch 00005: val_loss improved from 0.98644 to 0.97843, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9875 - accuracy: 0.5779 - val_loss: 0.9784 - val_accuracy: 0.5862\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9829 - accuracy: 0.5864\n",
      "Epoch 00006: val_loss improved from 0.97843 to 0.96962, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9829 - accuracy: 0.5864 - val_loss: 0.9696 - val_accuracy: 0.5862\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9716 - accuracy: 0.5807\n",
      "Epoch 00007: val_loss improved from 0.96962 to 0.95942, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9716 - accuracy: 0.5807 - val_loss: 0.9594 - val_accuracy: 0.5862\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9645 - accuracy: 0.6006\n",
      "Epoch 00008: val_loss improved from 0.95942 to 0.95084, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9645 - accuracy: 0.6006 - val_loss: 0.9508 - val_accuracy: 0.6207\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9450 - accuracy: 0.6062\n",
      "Epoch 00009: val_loss improved from 0.95084 to 0.94655, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9450 - accuracy: 0.6062 - val_loss: 0.9466 - val_accuracy: 0.6437\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9441 - accuracy: 0.6034\n",
      "Epoch 00010: val_loss improved from 0.94655 to 0.94254, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.9441 - accuracy: 0.6034 - val_loss: 0.9425 - val_accuracy: 0.6437\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9109 - accuracy: 0.6147\n",
      "Epoch 00011: val_loss improved from 0.94254 to 0.93987, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.9109 - accuracy: 0.6147 - val_loss: 0.9399 - val_accuracy: 0.6552\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9244 - accuracy: 0.6091\n",
      "Epoch 00012: val_loss improved from 0.93987 to 0.93325, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9244 - accuracy: 0.6091 - val_loss: 0.9332 - val_accuracy: 0.6552\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8964 - accuracy: 0.6374\n",
      "Epoch 00013: val_loss improved from 0.93325 to 0.92825, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8964 - accuracy: 0.6374 - val_loss: 0.9282 - val_accuracy: 0.6552\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9024 - accuracy: 0.6487\n",
      "Epoch 00014: val_loss improved from 0.92825 to 0.92100, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.9024 - accuracy: 0.6487 - val_loss: 0.9210 - val_accuracy: 0.6782\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8875 - accuracy: 0.6119\n",
      "Epoch 00015: val_loss improved from 0.92100 to 0.91411, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8875 - accuracy: 0.6119 - val_loss: 0.9141 - val_accuracy: 0.6782\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8778 - accuracy: 0.6431\n",
      "Epoch 00016: val_loss did not improve from 0.91411\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.8778 - accuracy: 0.6431 - val_loss: 0.9174 - val_accuracy: 0.6667\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8699 - accuracy: 0.6516\n",
      "Epoch 00017: val_loss improved from 0.91411 to 0.90600, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.8699 - accuracy: 0.6516 - val_loss: 0.9060 - val_accuracy: 0.6897\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8502 - accuracy: 0.6657\n",
      "Epoch 00018: val_loss improved from 0.90600 to 0.90222, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.8502 - accuracy: 0.6657 - val_loss: 0.9022 - val_accuracy: 0.6782\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8539 - accuracy: 0.6516\n",
      "Epoch 00019: val_loss improved from 0.90222 to 0.89369, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.8539 - accuracy: 0.6516 - val_loss: 0.8937 - val_accuracy: 0.6782\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8404 - accuracy: 0.6657\n",
      "Epoch 00020: val_loss improved from 0.89369 to 0.88812, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8404 - accuracy: 0.6657 - val_loss: 0.8881 - val_accuracy: 0.6897\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8218 - accuracy: 0.6402\n",
      "Epoch 00021: val_loss did not improve from 0.88812\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8218 - accuracy: 0.6402 - val_loss: 0.8886 - val_accuracy: 0.6782\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8229 - accuracy: 0.6742\n",
      "Epoch 00022: val_loss improved from 0.88812 to 0.88716, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.8229 - accuracy: 0.6742 - val_loss: 0.8872 - val_accuracy: 0.6782\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7780 - accuracy: 0.6799\n",
      "Epoch 00023: val_loss did not improve from 0.88716\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7780 - accuracy: 0.6799 - val_loss: 0.8918 - val_accuracy: 0.6667\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8002 - accuracy: 0.6771\n",
      "Epoch 00024: val_loss improved from 0.88716 to 0.87385, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.8002 - accuracy: 0.6771 - val_loss: 0.8738 - val_accuracy: 0.6897\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7740 - accuracy: 0.7139\n",
      "Epoch 00025: val_loss did not improve from 0.87385\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7740 - accuracy: 0.7139 - val_loss: 0.8756 - val_accuracy: 0.6782\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7568 - accuracy: 0.7025\n",
      "Epoch 00026: val_loss did not improve from 0.87385\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7568 - accuracy: 0.7025 - val_loss: 0.8741 - val_accuracy: 0.6782\n",
      "-------Iteration : 1\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7949 - accuracy: 0.6686\n",
      "Epoch 00001: val_loss improved from inf to 0.88262, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7949 - accuracy: 0.6686 - val_loss: 0.8826 - val_accuracy: 0.6782\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7517 - accuracy: 0.7195\n",
      "Epoch 00002: val_loss improved from 0.88262 to 0.87710, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7517 - accuracy: 0.7195 - val_loss: 0.8771 - val_accuracy: 0.6897\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7606 - accuracy: 0.7167\n",
      "Epoch 00003: val_loss did not improve from 0.87710\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7606 - accuracy: 0.7167 - val_loss: 0.8875 - val_accuracy: 0.6782\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7391 - accuracy: 0.7110\n",
      "Epoch 00004: val_loss did not improve from 0.87710\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7391 - accuracy: 0.7110 - val_loss: 0.8829 - val_accuracy: 0.6782\n",
      "-------Iteration : 2\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7645 - accuracy: 0.7082\n",
      "Epoch 00001: val_loss improved from inf to 0.88862, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.7645 - accuracy: 0.7082 - val_loss: 0.8886 - val_accuracy: 0.6552\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7583 - accuracy: 0.7110\n",
      "Epoch 00002: val_loss improved from 0.88862 to 0.87835, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7583 - accuracy: 0.7110 - val_loss: 0.8784 - val_accuracy: 0.6782\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7300 - accuracy: 0.7110\n",
      "Epoch 00003: val_loss did not improve from 0.87835\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7300 - accuracy: 0.7110 - val_loss: 0.8795 - val_accuracy: 0.6782\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7137 - accuracy: 0.7394\n",
      "Epoch 00004: val_loss improved from 0.87835 to 0.87829, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7137 - accuracy: 0.7394 - val_loss: 0.8783 - val_accuracy: 0.6782\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7080 - accuracy: 0.7309\n",
      "Epoch 00005: val_loss improved from 0.87829 to 0.87442, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7080 - accuracy: 0.7309 - val_loss: 0.8744 - val_accuracy: 0.6897\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6906 - accuracy: 0.7139\n",
      "Epoch 00006: val_loss did not improve from 0.87442\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6906 - accuracy: 0.7139 - val_loss: 0.8814 - val_accuracy: 0.6667\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.7309\n",
      "Epoch 00007: val_loss did not improve from 0.87442\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6918 - accuracy: 0.7309 - val_loss: 0.8844 - val_accuracy: 0.6552\n",
      "-------Iteration : 3\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.7479\n",
      "Epoch 00001: val_loss improved from inf to 0.88362, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6869 - accuracy: 0.7479 - val_loss: 0.8836 - val_accuracy: 0.6782\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6781 - accuracy: 0.7649\n",
      "Epoch 00002: val_loss did not improve from 0.88362\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.6781 - accuracy: 0.7649 - val_loss: 0.8920 - val_accuracy: 0.6897\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6756 - accuracy: 0.7535\n",
      "Epoch 00003: val_loss did not improve from 0.88362\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.6756 - accuracy: 0.7535 - val_loss: 0.8873 - val_accuracy: 0.6667\n",
      "-------Iteration : 4\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.7252\n",
      "Epoch 00001: val_loss improved from inf to 0.88824, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6978 - accuracy: 0.7252 - val_loss: 0.8882 - val_accuracy: 0.6552\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6619 - accuracy: 0.7620\n",
      "Epoch 00002: val_loss did not improve from 0.88824\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6619 - accuracy: 0.7620 - val_loss: 0.8903 - val_accuracy: 0.6552\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.7620\n",
      "Epoch 00003: val_loss did not improve from 0.88824\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6507 - accuracy: 0.7620 - val_loss: 0.8954 - val_accuracy: 0.6667\n",
      "-------Iteration : 5\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.7450\n",
      "Epoch 00001: val_loss improved from inf to 0.89720, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7030 - accuracy: 0.7450 - val_loss: 0.8972 - val_accuracy: 0.6437\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6698 - accuracy: 0.7280\n",
      "Epoch 00002: val_loss did not improve from 0.89720\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6698 - accuracy: 0.7280 - val_loss: 0.9007 - val_accuracy: 0.6552\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.7649\n",
      "Epoch 00003: val_loss improved from 0.89720 to 0.89651, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6495 - accuracy: 0.7649 - val_loss: 0.8965 - val_accuracy: 0.7011\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6648 - accuracy: 0.7592\n",
      "Epoch 00004: val_loss improved from 0.89651 to 0.89035, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6648 - accuracy: 0.7592 - val_loss: 0.8903 - val_accuracy: 0.6782\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6275 - accuracy: 0.7819\n",
      "Epoch 00005: val_loss did not improve from 0.89035\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6275 - accuracy: 0.7819 - val_loss: 0.8910 - val_accuracy: 0.6782\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.7989\n",
      "Epoch 00006: val_loss did not improve from 0.89035\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.5981 - accuracy: 0.7989 - val_loss: 0.8956 - val_accuracy: 0.7011\n",
      "-------Iteration : 6\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.7790\n",
      "Epoch 00001: val_loss improved from inf to 0.89905, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.6162 - accuracy: 0.7790 - val_loss: 0.8991 - val_accuracy: 0.6667\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6058 - accuracy: 0.7734\n",
      "Epoch 00002: val_loss improved from 0.89905 to 0.89809, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6058 - accuracy: 0.7734 - val_loss: 0.8981 - val_accuracy: 0.6322\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6177 - accuracy: 0.7790\n",
      "Epoch 00003: val_loss improved from 0.89809 to 0.89420, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.6177 - accuracy: 0.7790 - val_loss: 0.8942 - val_accuracy: 0.6782\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6000 - accuracy: 0.7819\n",
      "Epoch 00004: val_loss did not improve from 0.89420\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.6000 - accuracy: 0.7819 - val_loss: 0.8982 - val_accuracy: 0.6782\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5891 - accuracy: 0.7989\n",
      "Epoch 00005: val_loss did not improve from 0.89420\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5891 - accuracy: 0.7989 - val_loss: 0.8974 - val_accuracy: 0.6667\n",
      "-------Iteration : 7\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6160 - accuracy: 0.7734 \n",
      "Epoch 00001: val_loss improved from inf to 0.91051, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.6160 - accuracy: 0.7734 - val_loss: 0.9105 - val_accuracy: 0.6322\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.7677\n",
      "Epoch 00002: val_loss improved from 0.91051 to 0.90973, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.5752 - accuracy: 0.7677 - val_loss: 0.9097 - val_accuracy: 0.6322\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.7960\n",
      "Epoch 00003: val_loss improved from 0.90973 to 0.90743, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5523 - accuracy: 0.7960 - val_loss: 0.9074 - val_accuracy: 0.6667\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.7875 \n",
      "Epoch 00004: val_loss did not improve from 0.90743\n",
      "12/12 [==============================] - 130s 11s/step - loss: 0.5608 - accuracy: 0.7875 - val_loss: 0.9127 - val_accuracy: 0.6667\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5567 - accuracy: 0.8074 \n",
      "Epoch 00005: val_loss did not improve from 0.90743\n",
      "12/12 [==============================] - 122s 10s/step - loss: 0.5567 - accuracy: 0.8074 - val_loss: 0.9170 - val_accuracy: 0.6552\n",
      "-------Iteration : 8\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.8045 \n",
      "Epoch 00001: val_loss improved from inf to 0.93613, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5754 - accuracy: 0.8045 - val_loss: 0.9361 - val_accuracy: 0.5977\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5561 - accuracy: 0.7960 \n",
      "Epoch 00002: val_loss improved from 0.93613 to 0.91823, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 0.5561 - accuracy: 0.7960 - val_loss: 0.9182 - val_accuracy: 0.6667\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.8272 \n",
      "Epoch 00003: val_loss improved from 0.91823 to 0.91415, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 0.5267 - accuracy: 0.8272 - val_loss: 0.9141 - val_accuracy: 0.7126\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5385 - accuracy: 0.8045 \n",
      "Epoch 00004: val_loss did not improve from 0.91415\n",
      "12/12 [==============================] - 124s 10s/step - loss: 0.5385 - accuracy: 0.8045 - val_loss: 0.9192 - val_accuracy: 0.6552\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.8102 \n",
      "Epoch 00005: val_loss did not improve from 0.91415\n",
      "12/12 [==============================] - 138s 12s/step - loss: 0.5355 - accuracy: 0.8102 - val_loss: 0.9357 - val_accuracy: 0.6207\n",
      "-------Iteration : 9\n",
      "Fine-tuning whole network\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5378 - accuracy: 0.8074 \n",
      "Epoch 00001: val_loss improved from inf to 0.92200, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-0401202218-dropout50reg-3class_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 0.5378 - accuracy: 0.8074 - val_loss: 0.9220 - val_accuracy: 0.6552\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5092 - accuracy: 0.8385 \n",
      "Epoch 00002: val_loss did not improve from 0.92200\n",
      "12/12 [==============================] - 129s 11s/step - loss: 0.5092 - accuracy: 0.8385 - val_loss: 0.9440 - val_accuracy: 0.6207\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5229 - accuracy: 0.8187 \n",
      "Epoch 00003: val_loss did not improve from 0.92200\n",
      "12/12 [==============================] - 133s 11s/step - loss: 0.5229 - accuracy: 0.8187 - val_loss: 0.9381 - val_accuracy: 0.6552\n",
      "Evaluating casme2-sub13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\train.py:190: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  tp0,fn0,fp0,tp1,fn1,fp1,tp2,fn2,fp2 = l\n",
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\train.py:192: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  f1_1 = 0 if (tp1 != 0 and fn1 != 0 and fp1 != 0) else (2*tp1)/((2*tp1) + fp1 + fn1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA7DklEQVR4nO3dd3wUdf7H8dcnBVJIIA1CQotKCb2EgCI2UDlUVJRiBwuKIijqT+TOE7EcNsSCKGAHRMTDdignJyjeIU0RqYIUSUJCSEhII/Xz+2OXGOICAbLZlM/z8dgHuzPznflkSfa9M9+Z74iqYowxxpTn5ekCjDHGVE8WEMYYY1yygDDGGOOSBYQxxhiXLCCMMca4ZAFhjDHGJQsIYwAReUdEnqzgsrtFpL+7azLG0ywgjDHGuGQBYUwtIiI+nq7B1B4WEKbGcB7aeUhENohIjoi8KSJNRORLEckSkaUiElJm+UEisklEMkRkuYjElpnXTUR+dLb7EPArt63LRWS9s+3/RKRzBWu8TER+EpFDIrJXRCaVm3+uc30ZzvkjnNP9ReQFEdkjIpki8r1z2gUikuDifejvfD5JRBaKyBwROQSMEJF4EVnp3MY+EXlVROqVad9BRL4WkXQRSRGRiSISKSK5IhJWZrnuIpIqIr4V+dlN7WMBYWqaa4CLgTbAFcCXwEQgAsfv81gAEWkDfADc55y3GPhcROo5Pyw/Ad4HQoGPnOvF2bYb8BZwJxAGvAF8JiL1K1BfDnAz0Ai4DBgtIlc519vSWe8rzpq6Auud7Z4HegDnOGv6P6Ckgu/JlcBC5zbnAsXA/UA4cDbQD7jbWUMQsBT4CogCzgL+o6rJwHJgaJn13gTMV9XCCtZhahkLCFPTvKKqKaqaCKwAVqnqT6p6GFgEdHMuNwz4l6p+7fyAex7wx/EB3BvwBaapaqGqLgTWlNnGKOANVV2lqsWq+i6Q72x3XKq6XFV/UdUSVd2AI6TOd86+Hliqqh84t5umqutFxAu4FRinqonObf5PVfMr+J6sVNVPnNvMU9V1qvqDqhap6m4cAXekhsuBZFV9QVUPq2qWqq5yznsXuBFARLyB63CEqKmjLCBMTZNS5nmei9cNnM+jgD1HZqhqCbAXiHbOS9SjR6rcU+Z5S+AB5yGaDBHJAJo72x2XiPQSkWXOQzOZwF04vsnjXMdvLpqF4zjE5WpeRewtV0MbEflCRJKdh52erkANAJ8C7UUkBsdeWqaqrj7FmkwtYAFhaqskHB/0AIiI4PhwTAT2AdHOaUe0KPN8L/CUqjYq8whQ1Q8qsN15wGdAc1VtCLwOHNnOXuBMF20OAIePMS8HCCjzc3jjODxVVvkhmWcAW4HWqhqM4xBc2RrOcFW4cy9sAY69iJuwvYc6zwLC1FYLgMtEpJ+zk/UBHIeJ/gesBIqAsSLiKyKDgfgybWcBdzn3BkREAp2dz0EV2G4QkK6qh0UkHsdhpSPmAv1FZKiI+IhImIh0de7dvAVMFZEoEfEWkbOdfR6/An7O7fsCfwNO1BcSBBwCskWkHTC6zLwvgKYicp+I1BeRIBHpVWb+e8AIYBAWEHWeBYSplVR1G45vwq/g+IZ+BXCFqhaoagEwGMcHYTqO/op/lmm7FrgDeBU4COxwLlsRdwOTRSQL+DuOoDqy3t+BgTjCKh1HB3UX5+wHgV9w9IWkA88AXqqa6VznbBx7PznAUWc1ufAgjmDKwhF2H5apIQvH4aMrgGRgO3Bhmfn/xdE5/qOqlj3sZuogsRsGGWPKEpFvgHmqOtvTtRjPsoAwxpQSkZ7A1zj6ULI8XY/xLDvEZIwBQETexXGNxH0WDgbcHBAiMkBEtonIDhGZ4GJ+C+cpgT+J4+rYgc7prUQkz3kl63oRed2ddRpjQFVvUdWGqvqOp2sx1YPbDjE5T8f7FUeHWAKOzrfrVHVzmWVmAj+p6gwRaQ8sVtVWItIK+EJVO7qlOGOMMSfkzoG94oEdqroTQETm4xgSYHOZZRQIdj5viOPc9VMSHh6urVq1OtXmxhhTJ61bt+6Aqpa/tgZwb0BEc/QVnglAr3LLTAL+LSL3AoFA2TH2Y0TkJxznc/9NVVccb2OtWrVi7dq1p120McbUJSJyzNOZPd1JfR3wjqo2w3F++PvOcWn2AS1UtRswHpgnIsHlG4vIKBFZKyJrU1NTq7RwY4yp7dwZEIk4hjY4oplzWlm34byQSFVX4hiPJlxV81U1zTl9HY6xY9qU34CqzlTVOFWNi4hwuYdkjDHmFLkzINYArUUkxjm88nAcY9SU9TuOoYgRx1j9fkCqiEQ4O7kRkTOA1sBON9ZqjDGmHLf1QahqkYiMAZYA3sBbqrpJRCYDa1X1MxxDDswSkftxdFiPUFUVkfNwDFdQiOOy/7tUNf1kaygsLCQhIYHDhw9X2s9V1/n5+dGsWTN8fe0eMsbUdrXmSuq4uDgt30m9a9cugoKCCAsL4+iBO82pUFXS0tLIysoiJibG0+UYYyqBiKxT1ThX8zzdSe1Whw8ftnCoRCJCWFiY7ZEZU0fU6oAALBwqmb2fxtQd7rwOwhhjjJuoKnsO7WFNyhpUlaFth5640UmygHCzjIwM5s2bx913331S7QYOHMi8efNo1KiRewozxtQoqsrvWb+zJnkNq5NXszZ5Lal5juu/Okd0toCoiTIyMnjttdf+FBBFRUX4+Bz77V+8eLG7SzPGVGOqyt6svaxJXsOalDWsSV7D/tz9AIT7h9OzSU96Nu1JzyY9aRnc8gRrOzUWEG42YcIEfvvtN7p27Yqvry9+fn6EhISwdetWfv31V6666ir27t3L4cOHGTduHKNGjQL+GDokOzubv/zlL5x77rn873//Izo6mk8//RR/f38P/2TGmMqkqiRkJZSGwZrkNaTkpgAQ5hdGz8iepY9Wwa2qpD+wzgTE459vYnPSoUpdZ/uoYB67osNxl5kyZQobN25k/fr1LF++nMsuu4yNGzeWnib61ltvERoaSl5eHj179uSaa64hLCzsqHVs376dDz74gFmzZjF06FA+/vhjbrzxxkr9WYwxVUtVSchOYG3y2tK9hOScZABC/UIdYeDcS4gJjvHICSJ1JiCqi/j4+KOuIXj55ZdZtGgRAHv37mX79u1/CoiYmBi6du0KQI8ePdi9e3dVlWuMqUSJ2Yms3reatSmOUNiXsw9wBEJckzhu63gb8ZHxxDT0TCCUV2cC4kTf9KtKYGBg6fPly5ezdOlSVq5cSUBAABdccIHLawzq169f+tzb25u8vLwqqdUYc3qSspOO6lROynHc0SCkfghxkXGM7DiS+Mh4zmh4RrUIhPLqTEB4SlBQEFlZru/emJmZSUhICAEBAWzdupUffvihiqszxlSmfdn7WJOypnQvITHbMT5po/qN6BnZk1s63EJ8ZDxnNjqzWgZCeRYQbhYWFkafPn3o2LEj/v7+NGnSpHTegAEDeP3114mNjaVt27b07t3bg5UaY05Wck5yaYfy6uTVRwVCXJM4bmp/U2kgeEnNuy65Vo/FtGXLFmJjYz1UUe1l76upy4pKilj6+1LmbJ7Dz6k/A9CwfkPimsSVnmV0VqOzakwgHG8sJtuDMMaYCsjMz+Sf2//JvK3zSM5JpnlQc8b3GM85UefQOqR1jQmEk2EBYYwxx7E7czdzt8zl098+Ja8oj/jIeCbGT+S8Zufh7eXt6fLcygLCGGPKUVVWJa9izuY5fJvwLb5evgyMGciN7W+kXWg7T5dXZSwgjDHGKb84n8U7F/P+lvfZfnA7oX6hjO4ymqFthxLuH+7p8qqcBYQxps47kHeAD7d9yIJtC0g/nE6bkDZMPmcyA88YSH3v+ideQS1lAWGMqbO2pm/l/c3v8+WuLykqKeL8ZudzY/sbiY+MrxHXKbhb7et2r+EaNGgAQFJSEtdee63LZS644ALKn9Jb3rRp08jNzS19PXDgQDIyMiqtTmNqquKSYr75/RtuXXIrQz4fwtd7vubaNtfy+dWf80q/V+jVtJeFg5PtQVRTUVFRLFy48JTbT5s2jRtvvJGAgADAhg83Jqcwh092fMLcLXPZm7WXpoFNeaDHA1zd+moa1m/o6fKqJduDcLMJEyYwffr00teTJk3iySefpF+/fnTv3p1OnTrx6aef/qnd7t276dixIwB5eXkMHz6c2NhYrr766qPGYho9ejRxcXF06NCBxx57DHAMAJiUlMSFF17IhRdeCDiGDz9w4AAAU6dOpWPHjnTs2JFp06aVbi82NpY77riDDh06cMkll9iYT6ZWSMxO5Lk1z9H/o/5MWT2FML8wnj//eRYPXsyIjiMsHI6j7uxBfDkBkn+p3HVGdoK/TDnuIsOGDeO+++7jnnvuAWDBggUsWbKEsWPHEhwczIEDB+jduzeDBg065m7tjBkzCAgIYMuWLWzYsIHu3buXznvqqacIDQ2luLiYfv36sWHDBsaOHcvUqVNZtmwZ4eFHn3mxbt063n77bVatWoWq0qtXL84//3xCQkJsWHFTa6gq61PX8/7m9/nP7/9BEC5peQk3tb+JThGdPF1ejVF3AsJDunXrxv79+0lKSiI1NZWQkBAiIyO5//77+e677/Dy8iIxMZGUlBQiIyNdruO7775j7NixAHTu3JnOnTuXzluwYAEzZ86kqKiIffv2sXnz5qPml/f9999z9dVXl44qO3jwYFasWMGgQYNsWHFT4xUWF/LvPf/m/c3vsyltE8H1ghnZYSTD2w0nMtD135c5troTECf4pu9OQ4YMYeHChSQnJzNs2DDmzp1Lamoq69atw9fXl1atWrkc5vtEdu3axfPPP8+aNWsICQlhxIgRp7SeI2xYcVNTZRzOYOH2hXyw5QP25+2nVXArHu39KJefcTkBvgGeLq/Gsj6IKjBs2DDmz5/PwoULGTJkCJmZmTRu3BhfX1+WLVvGnj17jtv+vPPOY968eQBs3LiRDRs2AHDo0CECAwNp2LAhKSkpfPnll6VtjjXMeN++ffnkk0/Izc0lJyeHRYsW0bdv30r8aY2pOjszdjJ55WQuXngxL/34Emc2OpPX+r3Gp1d9ytC2Qy0cTlPd2YPwoA4dOpCVlUV0dDRNmzblhhtu4IorrqBTp07ExcXRrt3xL90fPXo0I0eOJDY2ltjYWHr06AFAly5d6NatG+3ataN58+b06dOntM2oUaMYMGAAUVFRLFu2rHR69+7dGTFiBPHx8QDcfvvtdOvWzQ4nmRqjoLiA1cmrmbNlDv9N/C/1vOpxxZlXcEPsDbQOae3p8moVG+7bnDR7X01VyczP5NeDv7IlbQvbDm5ja/pWdmbspEiLCPcPZ3jb4QxpO4RQv1BPl1pj2XDfxphqTVXZl7OPrelbSx/b0reV3qITIMI/grahbTm/2fl0COvAec3Ow9fb14NV134WEMaYKlVYUsjOjJ1sO7jtqD2DrAJHn5kgtGrYii4RXRjadiixobG0CW1TJwfL8zS3BoSIDABeAryB2ao6pdz8FsC7QCPnMhNUdbFz3iPAbUAxMFZVl7izVmNM5csuyC4NgCN7BTsydlBYUgiAn7cfbULaMKDVANqFtqNtaFtaN2ptncvVhNsCQkS8genAxUACsEZEPlPVzWUW+xuwQFVniEh7YDHQyvl8ONABiAKWikgbVS12V73GmFOnqqTkprAt3REGR/YOErITSpcJ9QulXWg7xz0VQtrRLrQdLYNb1vqb7tRk7tyDiAd2qOpOABGZD1wJlA0IBYKdzxsCRw44XgnMV9V8YJeI7HCub6Ub6zXGVEBRSRG7M3ez9eDWPwIhfRsH8w+WLtMiqAXtw9ozuPVg2oa2pV1oOyL8I2wQvBrGnQERDewt8zoB6FVumUnAv0XkXiAQ6F+m7Q/l2kaX34CIjAJGAbRo0aJSijamujmQd4CnVz3Nvux9ni6FwpJCdh/aTX5xPgD1vOpxVshZXNjiQtqFOvYK2oS0IdA30MOVmsrg6U7q64B3VPUFETkbeF9EOla0sarOBGaC4zRXN9V4WjIyMpg3bx533333SbUbOHAg8+bNo1GjRu4pzNQImw5sYuyysWQVZNG9SXcEz34D9xZvejXtVRoGrRq2wtfLziSqrdwZEIlA8zKvmzmnlXUbMABAVVeKiB8QXsG2NUJGRgavvfbanwKiqKgIH59jv/02PLf5YucXTPrfJML8wnjvL+/VqXshm+rBnUNtrAFai0iMiNTD0en8Wbllfgf6AYhILOAHpDqXGy4i9UUkBmgNrHZjrW4zYcIEfvvtN7p27UrPnj3p27cvgwYNon379gBcddVV9OjRgw4dOjBz5szSdkeG57ZhuOue4pJipq6byiMrHqFTeCc+uPwDCwfjEW7bg1DVIhEZAyzBcQrrW6q6SUQmA2tV9TPgAWCWiNyPo8N6hDou7d4kIgtwdGgXAfec7hlMz6x+hq3pW09nFX/SLrQdD8c/fNxlpkyZwsaNG1m/fj3Lly/nsssuY+PGjcTExADw1ltvERoaSl5eHj179uSaa64hLCzsqHXYMNx1x6GCQzz83cN8n/g9w9oO4+H4h+0QjvEYt/ZBOK9pWFxu2t/LPN8M9CnfzjnvKeApd9bnCfHx8aXhAI6b+yxatAiAvXv3sn379j8FhA3DXTfsytzF2G/GkpCVwKO9H2Vo26GeLsnUcZ7upK4yJ/qmX1WO3IcBYPny5SxdupSVK1cSEBDABRdc4HK4bhuGu/ZbkbCCh797GB8vH2ZdMou4SJdD4xhTpepMQHjKsYbdBsjMzCQkJISAgAC2bt3KDz/84HI5U3upKu9seocX171Im5A2vHzRy0Q1iPJ0WcYAFhBuFxYWRp8+fejYsSP+/v40adKkdN6AAQN4/fXXiY2NpW3btvTu3duDlZqqdrjoMI+vfJwvdn7BJS0v4Yk+T9gQE6ZaseG+zUmz9/X0peSkMG7ZODalbWJM1zGM6jzKrjI2HmHDfRtTjazfv577l99PbmEuL134Ehe1uMjTJRnjkt1y1JgqtGj7Im5dciv+Pv7MHTjXwsFUa7YHYUwVKCop4oW1LzBnyxx6N+3N8+c/T8P6DT1dljHHZQFhjJtl5mfy4LcP8sO+H7gx9kYeiHsAHy/70zPVn/2WGuNGOw7uYOyysSTnJDP5nMlc3fpqT5dkTIVZQBjjJst+X8aEFRPw9/HnrUvfomvjrp4uyZiTYp3U1UyDBg0ASEpK4tprr3W5zAUXXED5U3rLmzZtGrm5uaWvBw4cSEZGRqXVaY5NVZm5YSbjlo0jpmEM8y+fb+FgaiQLiGoqKiqKhQsXnnL78gGxePFiu7dEFcgtzOXBbx/klZ9eYeAZA3lnwDtEBkZ6uixjTokFhJtNmDCB6dOnl76eNGkSTz75JP369aN79+506tSJTz/99E/tdu/eTceOjnsn5eXlMXz4cGJjY7n66quPGotp9OjRxMXF0aFDBx577DHAMQBgUlISF154IRdeeCHwx/DhAFOnTqVjx4507NiRadOmlW7PhhU/PUnZSdzy1S18vedrxvcYzz/O/Qd+Pn6eLsuYU1Zn+iCSn36a/C2VO9x3/dh2RE6ceNxlhg0bxn333cc999wDwIIFC1iyZAljx44lODiYAwcO0Lt3bwYNGnTMK2lnzJhBQEAAW7ZsYcOGDXTv3r103lNPPUVoaCjFxcX069ePDRs2MHbsWKZOncqyZcsIDw8/al3r1q3j7bffZtWqVagqvXr14vzzzyckJMSGFT8N61LWMX75eAqLC5nebzp9m/X1dEnGnDbbg3Czbt26sX//fpKSkvj5558JCQkhMjKSiRMn0rlzZ/r3709iYiIpKSnHXMd3331X+kHduXNnOnfuXDpvwYIFdO/enW7durFp0yY2b9583Hq+//57rr76agIDA2nQoAGDBw9mxYoVgA0rfqoWbFvA7UtuJ7heMHMvm2vhYGqNOrMHcaJv+u40ZMgQFi5cSHJyMsOGDWPu3Lmkpqaybt06fH19adWqlcthvk9k165dPP/886xZs4aQkBBGjBhxSus5woYVPzmFJYU8s/oZPtz2IX2i+/Dsec8SXC/Y02UZU2lsD6IKDBs2jPnz57Nw4UKGDBlCZmYmjRs3xtfXl2XLlrFnz57jtj/vvPOYN28eABs3bmTDhg0AHDp0iMDAQBo2bEhKSgpffvllaZtjDTPet29fPvnkE3Jzc8nJyWHRokX07WvfeE9W+uF0Rv17FB9u+5CRHUYy/aLpFg6m1qkzexCe1KFDB7KysoiOjqZp06bccMMNXHHFFXTq1Im4uDjatTv+/YZHjx7NyJEjiY2NJTY2lh49egDQpUsXunXrRrt27WjevDl9+vxxc75Ro0YxYMAAoqKiWLZsWen07t27M2LECOLj4wG4/fbb6datmx1OOgnb0rcx9puxpB1O4x99/8HlZ1zu6ZKMcQsb7tuctLr8vn6952v++v1fCaoXxMsXvkyH8A6eLsmY02LDfZsaJzknmV8P/koD3wYE1QsiqF4QwfWC8ffx98h9E0q0hBk/z+D1n1+nc0Rnpl0wjYiAiCqvw5iqZAFhqpWcwhze/OVN3tv8HvnF+X+a7y3epYFxJDTK/utqXpDvH9NOJWByCnP46/d/5T+//4erzrqKR3s/Sj3vepX1IxtTbdX6gFBVu1NXJXLXIcnikmI+2fEJr/z0CmmH0xgYM5ChbYeSX5xPVkHWUY9DBYeOer0/d3/p88PFxz+Ly8fLx2VwHCtkfL18eXbNs+zK3MXDPR/mhtgb7PfJ1Bm1OiD8/PxIS0sjLCzM/qgrgaqSlpaGn1/lXh28at8qnlvzHNsObqNLRBdevuhlOkd0PnFDFwqKC/4cKIWHXIbMkaBJzk0une5qryW4XjAz+s/g7KizT/dHNaZGqdUB0axZMxISEkhNTfV0KbWGn58fzZo1q5R17c7czQtrX2B5wnKiAqN47rznuLTVpacV5vW86xHmH0aYf9gptXe1x9ImpI31N5g6qVYHhK+vLzExMZ4uw5STmZ/J6z+/zvyt86nvU59x3cdxU/ubqO9d/8SN3ay+d33q+9cn3D/8xAsbU8vV6oAw1UthSSEfbv2QGT/PILswm8GtB3NP13vsw9iYasoCwridqrJ873KmrpvK7kO76d20Nw/1fIg2IW08XZox5jgsIIxbbU3fyvNrnmdV8ipiGsY4RjqN7msnDRhTA7g1IERkAPAS4A3MVtUp5ea/CFzofBkANFbVRs55xcAvznm/q+ogd9ZqKldqbiqvrn+VRdsXEVw/mEfiH2FI2yH4evl6ujRjTAW5LSBExBuYDlwMJABrROQzVS0dj1pV7y+z/L1AtzKryFPVru6qz7jH4aLDvLf5PWb/MpvCkkJuan8TozqPomH9hp4uzRhzkty5BxEP7FDVnQAiMh+4EjjWDQuuAx5zYz3GjVSVxbsWM+3HaSTnJNOvRT/u73E/LYNbero0Y8wpcmdARAN7y7xOAHq5WlBEWgIxwDdlJvuJyFqgCJiiqp+4qU5zmtbvX89za55jw4ENxIbG8vS5T9MzsqenyzLGnKbq0kk9HFioqsVlprVU1UQROQP4RkR+UdXfyjYSkVHAKIAWLVpUXbUGcNyD+cV1L/LV7q+I8I/giT5PMOjMQXiJ3WbEmNrAnQGRCDQv87qZc5orw4F7yk5Q1UTnvztFZDmO/onfyi0zE5gJjuG+K6Vqc0LZBdm8ufFN3tv0Hl7ixV1d7mJkh5EE+AZ4ujRjTCVyZ0CsAVqLSAyOYBgOXF9+IRFpB4QAK8tMCwFyVTVfRMKBPsCzbqzVVEBxSTGLdizilZ9eIf1wOpefcTnjuo8jMjDS06UZY9zAbQGhqkUiMgZYguM017dUdZOITAbWqupnzkWHA/P16GFCY4E3RKQEx21Rp5Q9+8lUvZVJK3lu7XNsP7id7o27M73fdDqGd/R0WcYYN6rVd5Qzp29n5k6mrp3KtwnfEt0gmvE9xnNxy4vtQjdjagm7o5w5aRmHM5jx8wwWbFuAn48f43uM5/rY66vFgHrGmKphAWGOUlBcwPyt83l9w+vkFOZwbetrubvr3ac8fLYxpuaygDAApOWlsWDbAuZvm0/64XT6RPXhwbgHOSvkLE+XZozxEAuIOm5b+jbmbJnDv3b+i8KSQvpG9+XmDjfTu2lvT5dmjPEwC4g6qERLWJGwgvc3v8+q5FX4+/gzuPVgboi9gZiGdoMlY4yDBUQdkluYy6e/fcrcLXPZc2gPTQKacF/3+7i2zbU2mJ4x5k8sIOqAfdn7+GDrByzcvpCsgiw6hXfi2fOepX/L/jb8tjHmmCoUECLyT+BN4EtVLXFvSaayrN+/njlb5rB0z1IA+rfsz42xN9K1cVfPFmaMqREqugfxGjASeFlEPgLeVtVt7ivLnKrCkkKW7lnKnM1z2HBgA0G+Qdzc/maua3cdTRs09XR5xpgapEIBoapLgaUi0hDHfRuWisheYBYwR1UL3VijqYDM/EwW/rqQD7Z+QEpuCi2DWzKx10SuPPNKG0TPGHNKKtwHISJhwI3ATcBPwFzgXOAW4AJ3FGdObFfmLuZumctnv31GXlEevSJ78WjvR+nbrK8Nu22MOS0V7YNYBLQF3geuUNV9zlkfOm/qY6qQqrJy30rmbJ7DisQV+Hr5ctkZl3Fj7I20DW3r6fKMMbVERfcgXlbVZa5mHGuQJ1P5Dhcd5l87/8WcLXPYkbGDUL9Q7u5yN0PaDiHcP9zT5RljapmKBkR7EflJVTOg9H4N16nqa26rzJRKzU1l/rb5fLTtIw7mH6RNSBue6PMEA2MGUs+7nqfLM8bUUhUNiDtUdfqRF6p6UETuwHF2k3GTLWlbmLNlDot3Laa4pJjzm5/PTbE30TOypw23bYxxu4oGhLeIyJGb+oiIN2BfXd2guKSY5QnLeX/z+6xLWYe/jz9D2wzl+tjraRnc0tPlGWPqkIoGxFc4OqTfcL6+0znNnKbikmJS81JJzE5k44GNzN86n4TsBJoGNuXBuAe5uvXVBNcL9nSZxpg6qKIB8TCOUBjtfP01MNstFdUyRwIgKTuJxOxEErMTScpOKn2dnJNMkRaVLt+tcTfu73E/F7W4CB8vGwnFGOM5Fb1QrgSY4XyYMkq0hNTcVJJynAGQlVj6PCk7iX05+ygqKTqqTYR/BFENougU0Ym/xPyFqAZRRDWIonlQc5oHNffQT2KMMUer6HUQrYF/AO0BvyPTVfUMN9VVbZRoCQfyDvxpD6BsABSWHH0hebh/OFENougY1pFLWl5CdFA00YHRRDWIommDpnbbTmNMjVDRYxhvA48BLwIX4hiXqVZcpluiJaTlpf1x6CcniYSshNLnSdlJfwqAML8wohtE0z6sPf1b9ie6gePDP7pBNE0Dm+Ln43eMrRljTM1R0YDwV9X/OM9k2gNMEpF1wN/dWFuV2J+7n4sXXnzUtFC/UKIbRNMutB0Xtbio9Nt/dINomjZoir+Pv4eqNcZUF1pcTElWFt6NGnm6FLepaEDki4gXsF1ExgCJQAP3lVV1IvwjmNhrItENokv3AGxwO2PM8ZTk5bF39N0c3rCBVh/Op37r1p4uyS3EeWnD8RcS6QlsARoBTwDBwHOq+oNbqzsJcXFxunatDQtljHGvkvx8EkbfTc7KlXgFBeETGkqrjxbgHRTk6dJOiYisO9aQSSfsR3BeFDdMVbNVNUFVR6rqNdUpHIwxpiqUFBSQMOZeclaupOlTT9F8+qsU7N1L0iOPoCW1715qJwwIVS3GMay3McbUWVpQQOK9Y8lZsYLIyY/TaPDVBPTsSZP/e4jspf8hbVbtuzSson0QP4nIZ8BHQM6Riar6T7dUZYwx1YgWFpIwfjzZ335L5KTHCBkypHReyM03k/fzBlJfegm/jh1o0KePByutXBU9VdUPSAMuAq5wPi53V1HGGFNdaFERiQ869hKa/O1vhAwfftR8EaHpk09Q/8wzSXrgQQoTEz1UaeWrUCd1TWCd1MaYyqZFRST938McWryYxhMeJmzEiGMuW7B7N7uuHUK9li1pOW8uXvVrxgWxp9VJ7VzB2yLyVvlHBdoNEJFtIrJDRCa4mP+iiKx3Pn4VkYwy824Rke3Oxy0VqdMYYyqLFheT9MhERzg89OBxwwGgXqtWRD37LIc3bSJ58mRqw5fvivZBfFHmuR9wNZB0vAbOs5+mAxcDCcAaEflMVTcfWUZV7y+z/L1AN+fzUBxXbscBCqxztj1YwXqNqTW0oID0OXOpf9aZBPbta/cCqQJaUsK+vz3Koc8/J+K++wi77bYKtQu66ELCRt9F2ozX8e/chZBhQ91cqXtVdLC+j8u+FpEPgO9P0Cwe2KGqO51t5gNXApuPsfx1OEIB4FLga1VNd7b9GhgAfFCReo2pLQoTE0m4fzyHN2wAwL9rVyLG3kvA2WdbULiJlpSQ/NhjZC5aRPi9Ywi/686Tah8xZgyHf9lIypNP4teuLf5duripUvc71fGUWgONT7BMNLC3zOsE57Q/EZGWQAzwzcm0FZFRIrJWRNampqZWsHRjaobs775j1+BrKNi5k+ipLxD5+OMUpqTw+623seemm8hZvdrTJdY6qkry5MlkfLSQsNF3EXHPPSe9DvH2Jvr55/Bp3JiEseMoSktzQ6VVo6J9EFkicujIA/gcxz0iKstwYKHzmosKU9WZqhqnqnERERGVWI4xnqPFxeyfNo29o+7EJzKSmIUfETxwICHDhnLmkq9o8ujfKPx9L7/ffAt7Rowk98cfPV1yraCqpDz5FBnzPyTsjtuJGDv2lNfl3agRzV55meKMDBLvH48WFZ24UTVUoYBQ1SBVDS7zaFP+sJMLiUDZmxs0c05zZThHHz46mbbG1BpFBw7w+223k/b6GzS89hpafTifeq1alc73qleP0Btu4Mx/L6HJxEfI37GDPdffwO+330Ge8zCUOXmqyv4pz3Bw7lxCR4wgYvz40z6E59e+PZGTJpG7ejX7p75YSZVWMVU94QNHp3TDMq8bAVedoI0PsBPHoaN6wM9ABxfLtQN24zzl1jktFNgFhDgfu4DQ422vR48eakxNlrN6tf56bl/d0rmLHlz4cYXaFOfk6IHZb+q23mfr5rbt9Pc779LcjRvdXGntUlJSosnPPqub27bTfU8+pSUlJZW6/qRJk3Rz23aa+eWXlbreygKs1WN8rla0D+IxVc0sEyoZ/NGhfKzgKQLGAEtwDPS3QFU3ichkERlUZtHhwHxnoUfapuMYFHCN8zHZOc2YWkdVSZs9mz0jRuIVEECrBR/S6JrBFWrrFRBA2G23ctbSr4m4/35yf/qJ3ddcy94xYzi8bZubK6/5VJXUaS+R/uZbhFx/HU0mPlLpnf+RjzyCf5cuJE38K/k7dlTqut2toqO5blDVzuWm/aKqndxW2UmyC+VMTVScmUnSIxPJ/uYbgi69lKZPPYl3g1MfSb84O5v0d98l/Z13KcnKImjAACLG3EP9s86qxKprj9RXXuXA9Ok0GjKEyMcnIV7uuQ9aYUoKuwZfg3dQEK0WfnRa/8eV7bQvlAPWishUETnT+ZgKrKu8Eo2pe/I2bmLXNdeS/d13NJk4kehpL572B4d3gwZE3HMPZy39mvC7R5OzYgU7rxhE4oMPkb9zVyVVXjscmDGDA9On03DwYLeGA4BvkyZEvziVgr172ffIIzXmIrqK7kEEAo8C/XFcuPY18JSq5hy3YRU61T2IovR0dvTr74aKzOnyjY4i/M47CR44EPH29nQ5lUZVyfhwASlPPYV3WBjRL04loFs3t2yr6OBB0t96m/Q5c9D8fBoOGkT43aOp16KFW7ZXUxyYNYvUF6bS8MpBNH366Sr7/Up7+x32P/MMEePHEz7qjirZ5okcbw+izo/FVJKTQ+r019xQkTktquR8/z3527dT78wziRhzD0GXXurWb3lVoSQnh32THufQ558TeO65RD33LD4hIW7fblFaGmmz3+TgvHloURENr76KiNGj8Y12eWlSrXbkQzr4ssuIevaZKv3yoaokPfAAh75aQovZswg855wq2/axnHZAOK9kHuLsnEZEQnB0LF9amYWeDuuDqH20pISsJUtIfXU6Bb/9Rv02bQi/dwxB/fvXyKuI83/7jYSx4yjYudN5he5dVR54hfv3kzZrNhkffoiq0uiawYTfdRe+kZFVWoenpL/3PilPP03QgAFEP/8c4lPR0YYqT0lODruHD6co9QAxHy/0eEhXRkD8pKrdTjTNkywgai8tLubQ4sUceHU6BXv2UL99LBH33kuDCy6oMUGR+fkX7HvsMbz8/Ih+/jmPf3MsTE4mbeZMDn60EAEaDRtG2Kg78G18ogESaq70efNImfwEQRdfTPTUFxBfX4/Vkr9rF7uHDK0WI79WRid1iYiUHrQUkVY4+iKMcTvx9qbhFVdwxr++oOnTT1NyKIuE0Xeze9hwsld8X607/EoKCtj3+OMkPfQQfrGxxCz6p8fDAcA3MpLIv/+ds776koZXXcXB+fP57eJLSPnHlBo9NMSxHPxwASmTn6DBhRcS/cLzHg0HgPoxMUQ9M8Ux8usTT3i0luOp6B7EAGAm8C0gQF9glKoucW95FWd7EHWHFhaS8cknHJgxg6Kkffh37+4YwK5Xr2q1R1GQkEjiffdxeONGQm+9lcb33+fxD6ZjKdi7lwOvzSDz00+R+vUJveF6Qm+7rUr6R9wt4+N/su+vfyXw/PNo9soreNWr5+mSSu2fNo20198gcvLjhAz1zMivldJJLSKNgVHAT4A/sF9Vv6u0Kk+TBUTdU1JQQObHH3Pg9TcoSkkhoGdPIsaNJSDO5e96lcpatoykCY9ASQlR/3iaoP4140y5/F27OPDaDA598QVe/v6E3HwTYSNH4t2woadLOyWZn35K0oRHCDznHJq9Nr3a3cRHi4vZe+dd5K5aRcu5c/Dv3PnEjSpZZfRB3A6MwzEm0nqgN7BSVS+qxDpPiwVE3VWSn0/Ghws4MGsmxakHCDznbMLvvddtp44ejxYVkfrSy6TNmkX99rE0mzatRp5Smv/bbxyYPp1Di7/Eq0EDQkeMIPSWm/EOCvJ0aRWW+fkXJD38MAG94mk+YwZefn6eLsmlooMH2X3tELSkhJiPF+ITGlql26+MgPgF6An8oKpdRaQd8LSqVmw8gCpgAWFK8vI4+MF80mbPpjg9ncDz+hJx7734d6qaC/4L9+8n6YEHyV2zhkZDh9LkrxOr3TfWk3V4268cePVVsr7+Gq+GDQkbOYLggQPxbd68Wh3OK+/QV1+ROP4BAnr0oPnMN/Dy9/d0SceVt2kTe66/Af9u3Wgxe1aVnl1VGQGxRlV7ish6oJeq5ovIJlXtUMm1njILCHNESU4O6fPmkT77TYozM2lw0UVE3DsGv9hYt20zZ9VqEh94gJLsbCInPUajq65y27Y84fDmzaS+8irZy5YB4NO0KYHx8QT07k1gr3h8o6I8XOEfDn39NYn33Y9/1660mPkGXoGBni6pQjL+uYh9EycSdvttNH7wwSrbbmUExCJgJHAfcBFwEPBV1YGVWOdpsYAw5RVnZ3Pw/fdJe/sdSg4dIujiiwm/dwx+bdpU2ja0pIS0WbNJfekl6rVsSfRL0yp1/dVN/q5d5P7wAzk/rCJ39WqKDzruAuzbvDkBveIJ7NWLgPhe+DbxzOmyWd98Q8LYcfh37Ejz2bPxblAzwuGIfZMmkTH/Q6JfeongSy+pkm1W6pXUInI+0BD4SlULKqG+SmEBYY6l+NAh0t95l/R336UkN5fgvwwgfMwY6p9xxumtNyODpIcnkP3ttwQP/AuRk5+ocR9Ip0NLSsjfvoPcVavIWb2K3NVrKDl0CIB6MTFlAiMen7Awt9eT/e237B1zL36xsbR4c3aN6i85oqSggD033UTB9h20+mgB9c880+3btKE2jMHxgZ729jukv/8+evgwwZdfRsTddx91Q56KyvvlFxLH3UdhaipNJjxMyPXXV+tj8lVBi4s5vHUruatWk7tqFblr11KS4xiurX7rswjo1dsRGj174t2oUaVuO/v7/5Jw993Ub92aFm+/hXdwcKWuvyoVJiez65pr8W7YkFYLFrj9S4cFhDFlFKWnk/bmmxycOw8tLKThlVc6BrBr1uyEbVWVg/PmkTLlGXwiwmk2bZpHTk2sCbSoiMObNzsOR61aRe6PP6J5eSBC/XbtHH0YvXoR0DPutL7t56xcyd67RlPvjDNo+fZblR4+npCzajW/33orQf36Ef3SNLd++bCAMMaFotRU0mbP5uAH89GSEhoNHkz4XXces8O1ODuH5L//nUOLFxN4/nlETZlSKy4kqypaUEDexo2OQ1I/rCLvp5/QggLw8sKvQwcCezkDo3v3Cncs56xazd4776Reixa0ePedWvX/kfbW2+x/9lkaP/gAYbff7rbtWEAYcxyFKftJe+MNMj76CAVChlxL2J134tukSeky+du3Owba27OHiHHjCLvj9ho/sqynleTnk7f+Z0dgrFrluKd2YSH4+ODfqVNpH4Z/t24ur2HIXbeO3+8YhW9UU1q++26V9HNUJVUlcfx4spb8mxZvzibw7LPdsh0LCGMqoDApiQNvzCTj448RLy8aDR9G+B13kPO//7Fv0uN4BQYS/fzzBPbu5elSa6WS3Fxyf/qptA8jb+NGKC5GfH3x79Kl9JRavy5dOLxpE3tvux2fJk1o+d67+EREeLp8tyjJyWHXsGEUp6U7Rn51w+nEFhDGnISChAQOzJhB5iefIl5eaGEhAXFxRE19oVaPdlrdFGfnkLduLTnOwDi8eTOoIn5+IIJv48a0eO89j51SW1Xyd+5i95Ah1IuJoeXcOZV+8aUFhDGnoGDPHg7MmoVv4yaE3z3aI/cOMH8ozswkd+1aclatojgtncYPPVhn7mORtXQpCWPupdGQa2layaO/WkAYY0wNt3/qi6TNnEnkE5MJGTKk0tZbGfeDMMYY40ER48YSeM45pEx+grxffqmSbVpAGGNMDSDe3kS98DzeEeEkjB1HUXq627dpAWGMMTWET0gIzV5+heK0NBIfeAAtKnLr9iwgjDGmBvHv2IHIx/5O7sofSH3pJbduywLCGGNqmEbXXEOjoUNJmzWbQ//+t9u2YwFhjDE1UJO//RW/zp3ZN+ER8nfudMs2LCCMMaYG8qpXj2YvTUP8/Ei8fzxaUlLp23DrlT8iMgB4CfAGZqvqFBfLDAUmAQr8rKrXO6cXA0fO5fpdVQe5s1ZjjKlpfJs2JXrai4iXl1vGBnNbQIiINzAduBhIANaIyGequrnMMq2BR4A+qnpQRMpeM5+nql3dVZ8xxtQGgfHxblu3Ow8xxQM7VHWn885z84Eryy1zBzBdVQ8CqOp+N9ZjjDHmJLgzIKKBvWVeJzinldUGaCMi/xWRH5yHpI7wE5G1zulXubFOY4wxLnh69DEfoDVwAdAM+E5EOqlqBtBSVRNF5AzgGxH5RVV/K9tYREYBowBatGhRpYUbY0xt5849iESgeZnXzZzTykoAPlPVQlXdBfyKIzBQ1UTnvzuB5UC38htQ1ZmqGqeqcRG1dDx4Y4zxFHcGxBqgtYjEiEg9YDjwWbllPsGx94CIhOM45LRTREJEpH6Z6X2AzRhjjKkybjvEpKpFIjIGWILjNNe3VHWTiEwG1qrqZ855l4jIZqAYeEhV00TkHOANESnBEWJTyp79ZIwxxv3sfhDGGFOH2f0gjDHGnDQLCGOMMS5ZQBhjjHHJAsIYY4xLFhDGGGNcsoAwxhjjkgWEMcYYlywgjDHGuGQBYYwxxiULCGOMMS5ZQBhjjHHJAsIYY4xLFhDGGGNcsoAwxhjjkgWEMcYYlywgjDHGuGQBYYwxxiULCGOMMS5ZQBhjjHHJAsIYY4xLFhDGGGNcsoAwxhjjkgWEMcYYlywgjDHGuGQBYYwxxiULCGOMMS5ZQBhjjHHJAsIYY4xLFhDGGGNccmtAiMgAEdkmIjtEZMIxlhkqIptFZJOIzCsz/RYR2e583OLOOo0xxvyZj7tWLCLewHTgYiABWCMin6nq5jLLtAYeAfqo6kERaeycHgo8BsQBCqxztj3ornqNMcYczZ17EPHADlXdqaoFwHzgynLL3AFMP/LBr6r7ndMvBb5W1XTnvK+BAW6s1RhjTDnuDIhoYG+Z1wnOaWW1AdqIyH9F5AcRGXASbRGRUSKyVkTWpqamVmLpxhhjPN1J7QO0Bi4ArgNmiUijijZW1ZmqGqeqcREREe6p0Bhj6ih3BkQi0LzM62bOaWUlAJ+paqGq7gJ+xREYFWlrjDHGjdwZEGuA1iISIyL1gOHAZ+WW+QTH3gMiEo7jkNNOYAlwiYiEiEgIcIlzmjHGmCritrOYVLVIRMbg+GD3Bt5S1U0iMhlYq6qf8UcQbAaKgYdUNQ1ARJ7AETIAk1U13V21GmOM+TNRVU/XUCni4uJ07dq1ni7DGGNqFBFZp6pxruZ5upPaGGNMNWUBYYwxxiULCGOMMS5ZQBhjjHHJAsIYY4xLFhDGGGNcsoAwxhjjkgWEMcYYlywgjDHGuGQBYYwxxiULCGOMMS5ZQBhjjHHJAsIYY4xLFhDGGGNcsoAwxhjjkgWEMcYYlywgjDHGuGQBYYwxxiULCGOMMS5ZQBhjjHHJAsIYY4xLFhDGGGNcsoAwxhjjkqiqp2uoFCKSCuw5jVWEAwcqqZyazt6Lo9n7cTR7P/5QG96Llqoa4WpGrQmI0yUia1U1ztN1VAf2XhzN3o+j2fvxh9r+XtghJmOMMS5ZQBhjjHHJAuIPMz1dQDVi78XR7P04mr0ff6jV74X1QRhjjHHJ9iCMMca4ZAFhjDHGpTofECIyQES2icgOEZng6Xo8SUSai8gyEdksIptEZJyna/I0EfEWkZ9E5AtP1+JpItJIRBaKyFYR2SIiZ3u6Jk8SkfudfycbReQDEfHzdE2VrU4HhIh4A9OBvwDtgetEpL1nq/KoIuABVW0P9AbuqePvB8A4YIuni6gmXgK+UtV2QBfq8PsiItHAWCBOVTsC3sBwz1ZV+ep0QADxwA5V3amqBcB84EoP1+QxqrpPVX90Ps/C8QEQ7dmqPEdEmgGXAbM9XYuniUhD4DzgTQBVLVDVDI8W5Xk+gL+I+AABQJKH66l0dT0gooG9ZV4nUIc/EMsSkVZAN2CVh0vxpGnA/wElHq6jOogBUoG3nYfcZotIoKeL8hRVTQSeB34H9gGZqvpvz1ZV+ep6QBgXRKQB8DFwn6oe8nQ9niAilwP7VXWdp2upJnyA7sAMVe0G5AB1ts9OREJwHG2IAaKAQBG50bNVVb66HhCJQPMyr5s5p9VZIuKLIxzmquo/PV2PB/UBBonIbhyHHi8SkTmeLcmjEoAEVT2yR7kQR2DUVf2BXaqaqqqFwD+BczxcU6Wr6wGxBmgtIjEiUg9HJ9NnHq7JY0REcBxj3qKqUz1djyep6iOq2kxVW+H4vfhGVWvdN8SKUtVkYK+ItHVO6gds9mBJnvY70FtEApx/N/2ohZ32Pp4uwJNUtUhExgBLcJyF8JaqbvJwWZ7UB7gJ+EVE1junTVTVxZ4ryVQj9wJznV+mdgIjPVyPx6jqKhFZCPyI4+y/n6iFw27YUBvGGGNcquuHmIwxxhyDBYQxxhiXLCCMMca4ZAFhjDHGJQsIY4wxLllAGOOCiPzP+W8rEbm+ktc90dW2jKlu7DRXY45DRC4AHlTVy0+ijY+qFh1nfraqNqiE8oxxK9uDMMYFEcl2Pp0C9BWR9c7x/71F5DkRWSMiG0TkTufyF4jIChH5DOcVxiLyiYisc94zYJRz2hQcI4CuF5G5ZbclDs857y/wi4gMK7Pu5WXuxTDXefWuMW5Vp6+kNqYCJlBmD8L5QZ+pqj1FpD7wXxE5Mopnd6Cjqu5yvr5VVdNFxB9YIyIfq+oEERmjql1dbGsw0BXHvRbCnW2+c87rBnTAMaT0f3Fc9f59Zf+wxpRlexDGnJxLgJudQ5GsAsKA1s55q8uEA8BYEfkZ+AHHoJCtOb5zgQ9UtVhVU4BvgZ5l1p2gqiXAeqBVJfwsxhyX7UEYc3IEuFdVlxw10dFXkVPudX/gbFXNFZHlwOnckjK/zPNi7G/XVAHbgzDm+LKAoDKvlwCjncOiIyJtjnHjnIbAQWc4tMNxC9cjCo+0L2cFMMzZzxGB4w5uqyvlpzDmFNi3EGOObwNQ7DxU9A6O+zK3An50dhSnAle5aPcVcJeIbAG24TjMdMRMYIOI/KiqN5SZvgg4G/gZUOD/VDXZGTDGVDk7zdUYY4xLdojJGGOMSxYQxhhjXLKAMMYY45IFhDHGGJcsIIwxxrhkAWGMMcYlCwhjjDEu/T+uMZ+jf48bdQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#multiply no of epochs by 10\n",
    "%run 3classBaseExperiment --headEpoch 100 --ftEpoch 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(noOfHeadIter=5, noOfFtIter=50, noOfHeadEpoch=10, noOfFtEpoch=20, outp='output', expName='multIterBy5')\n",
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling_8 (Rescaling)        (None, 224, 224, 3)  0           ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " normalization_8 (Normalization  (None, 224, 224, 3)  7          ['rescaling_8[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['normalization_8[0][0]']        \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',     \n",
      "                                                                  'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4252 - accuracy: 0.4731\n",
      "Epoch 00001: val_loss improved from inf to 2.05414, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.4252 - accuracy: 0.4731 - val_loss: 2.0541 - val_accuracy: 0.6548\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1849 - accuracy: 0.5212\n",
      "Epoch 00002: val_loss improved from 2.05414 to 1.92747, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 106s 9s/step - loss: 2.1849 - accuracy: 0.5212 - val_loss: 1.9275 - val_accuracy: 0.6429\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0170 - accuracy: 0.4476\n",
      "Epoch 00003: val_loss improved from 1.92747 to 1.77789, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.0170 - accuracy: 0.4476 - val_loss: 1.7779 - val_accuracy: 0.6548\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.8572 - accuracy: 0.4986\n",
      "Epoch 00004: val_loss improved from 1.77789 to 1.64958, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.8572 - accuracy: 0.4986 - val_loss: 1.6496 - val_accuracy: 0.6548\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.7135 - accuracy: 0.5439\n",
      "Epoch 00005: val_loss improved from 1.64958 to 1.55187, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.7135 - accuracy: 0.5439 - val_loss: 1.5519 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6395 - accuracy: 0.5326\n",
      "Epoch 00006: val_loss improved from 1.55187 to 1.47092, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 1.6395 - accuracy: 0.5326 - val_loss: 1.4709 - val_accuracy: 0.6548\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.5249 - accuracy: 0.5156\n",
      "Epoch 00007: val_loss improved from 1.47092 to 1.37861, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 1.5249 - accuracy: 0.5156 - val_loss: 1.3786 - val_accuracy: 0.6548\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.4627 - accuracy: 0.5411\n",
      "Epoch 00008: val_loss improved from 1.37861 to 1.32532, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 1.4627 - accuracy: 0.5411 - val_loss: 1.3253 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3787 - accuracy: 0.5637\n",
      "Epoch 00009: val_loss improved from 1.32532 to 1.26470, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.3787 - accuracy: 0.5637 - val_loss: 1.2647 - val_accuracy: 0.6667\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3497 - accuracy: 0.5212\n",
      "Epoch 00010: val_loss improved from 1.26470 to 1.21748, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 1.3497 - accuracy: 0.5212 - val_loss: 1.2175 - val_accuracy: 0.6667\n",
      "-------Iteration Head: 1\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3058 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 1.13698, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 1.3058 - accuracy: 0.5666 - val_loss: 1.1370 - val_accuracy: 0.6548\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2554 - accuracy: 0.5467\n",
      "Epoch 00002: val_loss improved from 1.13698 to 1.12654, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 1.2554 - accuracy: 0.5467 - val_loss: 1.1265 - val_accuracy: 0.6786\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2135 - accuracy: 0.5439\n",
      "Epoch 00003: val_loss improved from 1.12654 to 1.09564, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 1.2135 - accuracy: 0.5439 - val_loss: 1.0956 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1792 - accuracy: 0.5411\n",
      "Epoch 00004: val_loss improved from 1.09564 to 1.08446, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.1792 - accuracy: 0.5411 - val_loss: 1.0845 - val_accuracy: 0.6905\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1761 - accuracy: 0.5354\n",
      "Epoch 00005: val_loss improved from 1.08446 to 1.05739, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.1761 - accuracy: 0.5354 - val_loss: 1.0574 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1668 - accuracy: 0.5552\n",
      "Epoch 00006: val_loss improved from 1.05739 to 1.04544, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.1668 - accuracy: 0.5552 - val_loss: 1.0454 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1184 - accuracy: 0.5807\n",
      "Epoch 00007: val_loss improved from 1.04544 to 1.03284, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.1184 - accuracy: 0.5807 - val_loss: 1.0328 - val_accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1231 - accuracy: 0.5807\n",
      "Epoch 00008: val_loss improved from 1.03284 to 1.02321, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.1231 - accuracy: 0.5807 - val_loss: 1.0232 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1014 - accuracy: 0.5949\n",
      "Epoch 00009: val_loss improved from 1.02321 to 1.01508, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.1014 - accuracy: 0.5949 - val_loss: 1.0151 - val_accuracy: 0.6667\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.5864\n",
      "Epoch 00010: val_loss improved from 1.01508 to 1.00877, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0973 - accuracy: 0.5864 - val_loss: 1.0088 - val_accuracy: 0.6667\n",
      "-------Iteration Head: 2\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1054 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 0.99096, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.1054 - accuracy: 0.5722 - val_loss: 0.9910 - val_accuracy: 0.6548\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0875 - accuracy: 0.5949\n",
      "Epoch 00002: val_loss did not improve from 0.99096\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0875 - accuracy: 0.5949 - val_loss: 1.0130 - val_accuracy: 0.6786\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0865 - accuracy: 0.5722\n",
      "Epoch 00003: val_loss improved from 0.99096 to 0.98947, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0865 - accuracy: 0.5722 - val_loss: 0.9895 - val_accuracy: 0.6786\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0798 - accuracy: 0.5892\n",
      "Epoch 00004: val_loss improved from 0.98947 to 0.98893, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0798 - accuracy: 0.5892 - val_loss: 0.9889 - val_accuracy: 0.6786\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0814 - accuracy: 0.5524\n",
      "Epoch 00005: val_loss improved from 0.98893 to 0.98769, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0814 - accuracy: 0.5524 - val_loss: 0.9877 - val_accuracy: 0.6786\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0543 - accuracy: 0.5836\n",
      "Epoch 00006: val_loss did not improve from 0.98769\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0543 - accuracy: 0.5836 - val_loss: 0.9879 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0512 - accuracy: 0.5609\n",
      "Epoch 00007: val_loss improved from 0.98769 to 0.97804, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0512 - accuracy: 0.5609 - val_loss: 0.9780 - val_accuracy: 0.6786\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0393 - accuracy: 0.5807\n",
      "Epoch 00008: val_loss improved from 0.97804 to 0.97565, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0393 - accuracy: 0.5807 - val_loss: 0.9757 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0365 - accuracy: 0.5921\n",
      "Epoch 00009: val_loss improved from 0.97565 to 0.97209, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0365 - accuracy: 0.5921 - val_loss: 0.9721 - val_accuracy: 0.6786\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0530 - accuracy: 0.5637\n",
      "Epoch 00010: val_loss did not improve from 0.97209\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0530 - accuracy: 0.5637 - val_loss: 0.9765 - val_accuracy: 0.6786\n",
      "-------Iteration Head: 3\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0706 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 0.96763, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0706 - accuracy: 0.5751 - val_loss: 0.9676 - val_accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0658 - accuracy: 0.5581\n",
      "Epoch 00002: val_loss did not improve from 0.96763\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0658 - accuracy: 0.5581 - val_loss: 0.9786 - val_accuracy: 0.6905\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0339 - accuracy: 0.6006\n",
      "Epoch 00003: val_loss did not improve from 0.96763\n",
      "12/12 [==============================] - 114s 10s/step - loss: 1.0339 - accuracy: 0.6006 - val_loss: 0.9720 - val_accuracy: 0.6786\n",
      "-------Iteration Head: 4\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 0.98392, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0912 - accuracy: 0.5439 - val_loss: 0.9839 - val_accuracy: 0.6548\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0646 - accuracy: 0.5666 \n",
      "Epoch 00002: val_loss did not improve from 0.98392\n",
      "12/12 [==============================] - 122s 10s/step - loss: 1.0646 - accuracy: 0.5666 - val_loss: 1.0040 - val_accuracy: 0.6429\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0772 - accuracy: 0.5411\n",
      "Epoch 00003: val_loss did not improve from 0.98392\n",
      "12/12 [==============================] - 115s 9s/step - loss: 1.0772 - accuracy: 0.5411 - val_loss: 0.9913 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0747 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 0.98144, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 1.0747 - accuracy: 0.5637 - val_loss: 0.9814 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0404 - accuracy: 0.5779\n",
      "Epoch 00002: val_loss improved from 0.98144 to 0.98080, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0404 - accuracy: 0.5779 - val_loss: 0.9808 - val_accuracy: 0.6548\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0407 - accuracy: 0.5892 \n",
      "Epoch 00003: val_loss improved from 0.98080 to 0.97070, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 1.0407 - accuracy: 0.5892 - val_loss: 0.9707 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0403 - accuracy: 0.6147 \n",
      "Epoch 00004: val_loss improved from 0.97070 to 0.96168, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 1.0403 - accuracy: 0.6147 - val_loss: 0.9617 - val_accuracy: 0.6667\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0136 - accuracy: 0.5921 \n",
      "Epoch 00005: val_loss improved from 0.96168 to 0.95822, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.0136 - accuracy: 0.5921 - val_loss: 0.9582 - val_accuracy: 0.6786\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0094 - accuracy: 0.5921 \n",
      "Epoch 00006: val_loss improved from 0.95822 to 0.94923, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 1.0094 - accuracy: 0.5921 - val_loss: 0.9492 - val_accuracy: 0.6905\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0130 - accuracy: 0.5892\n",
      "Epoch 00007: val_loss improved from 0.94923 to 0.94436, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0130 - accuracy: 0.5892 - val_loss: 0.9444 - val_accuracy: 0.6905\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9983 - accuracy: 0.5921\n",
      "Epoch 00008: val_loss improved from 0.94436 to 0.93635, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.9983 - accuracy: 0.5921 - val_loss: 0.9363 - val_accuracy: 0.7024\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9609 - accuracy: 0.6402\n",
      "Epoch 00009: val_loss improved from 0.93635 to 0.92979, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9609 - accuracy: 0.6402 - val_loss: 0.9298 - val_accuracy: 0.7024\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9588 - accuracy: 0.6204\n",
      "Epoch 00010: val_loss improved from 0.92979 to 0.92485, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.9588 - accuracy: 0.6204 - val_loss: 0.9248 - val_accuracy: 0.6905\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9499 - accuracy: 0.6204\n",
      "Epoch 00011: val_loss improved from 0.92485 to 0.92062, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.9499 - accuracy: 0.6204 - val_loss: 0.9206 - val_accuracy: 0.6905\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9420 - accuracy: 0.6317\n",
      "Epoch 00012: val_loss improved from 0.92062 to 0.90858, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.9420 - accuracy: 0.6317 - val_loss: 0.9086 - val_accuracy: 0.6905\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9342 - accuracy: 0.6232\n",
      "Epoch 00013: val_loss did not improve from 0.90858\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.9342 - accuracy: 0.6232 - val_loss: 0.9087 - val_accuracy: 0.6905\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9204 - accuracy: 0.6374\n",
      "Epoch 00014: val_loss improved from 0.90858 to 0.90506, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.9204 - accuracy: 0.6374 - val_loss: 0.9051 - val_accuracy: 0.6905\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8875 - accuracy: 0.6601\n",
      "Epoch 00015: val_loss improved from 0.90506 to 0.89646, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8875 - accuracy: 0.6601 - val_loss: 0.8965 - val_accuracy: 0.6905\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8963 - accuracy: 0.6487\n",
      "Epoch 00016: val_loss improved from 0.89646 to 0.89098, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.8963 - accuracy: 0.6487 - val_loss: 0.8910 - val_accuracy: 0.6905\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8834 - accuracy: 0.6686\n",
      "Epoch 00017: val_loss improved from 0.89098 to 0.88328, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.8834 - accuracy: 0.6686 - val_loss: 0.8833 - val_accuracy: 0.6905\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8650 - accuracy: 0.6601\n",
      "Epoch 00018: val_loss improved from 0.88328 to 0.87909, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.8650 - accuracy: 0.6601 - val_loss: 0.8791 - val_accuracy: 0.7024\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8638 - accuracy: 0.6686 \n",
      "Epoch 00019: val_loss improved from 0.87909 to 0.87253, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 0.8638 - accuracy: 0.6686 - val_loss: 0.8725 - val_accuracy: 0.6786\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8454 - accuracy: 0.6601 \n",
      "Epoch 00020: val_loss improved from 0.87253 to 0.86750, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 0.8454 - accuracy: 0.6601 - val_loss: 0.8675 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8410 - accuracy: 0.6657\n",
      "Epoch 00001: val_loss improved from inf to 0.86545, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8410 - accuracy: 0.6657 - val_loss: 0.8655 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8264 - accuracy: 0.6827\n",
      "Epoch 00002: val_loss did not improve from 0.86545\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8264 - accuracy: 0.6827 - val_loss: 0.8658 - val_accuracy: 0.7024\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.6771\n",
      "Epoch 00003: val_loss improved from 0.86545 to 0.86373, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.8296 - accuracy: 0.6771 - val_loss: 0.8637 - val_accuracy: 0.7024\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8191 - accuracy: 0.6742\n",
      "Epoch 00004: val_loss improved from 0.86373 to 0.85919, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.8191 - accuracy: 0.6742 - val_loss: 0.8592 - val_accuracy: 0.7024\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8134 - accuracy: 0.6827\n",
      "Epoch 00005: val_loss improved from 0.85919 to 0.85679, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.8134 - accuracy: 0.6827 - val_loss: 0.8568 - val_accuracy: 0.7024\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.6969\n",
      "Epoch 00006: val_loss improved from 0.85679 to 0.85410, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.7947 - accuracy: 0.6969 - val_loss: 0.8541 - val_accuracy: 0.7024\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7776 - accuracy: 0.7025\n",
      "Epoch 00007: val_loss improved from 0.85410 to 0.85329, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.7776 - accuracy: 0.7025 - val_loss: 0.8533 - val_accuracy: 0.7262\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7773 - accuracy: 0.6941\n",
      "Epoch 00008: val_loss improved from 0.85329 to 0.84379, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.7773 - accuracy: 0.6941 - val_loss: 0.8438 - val_accuracy: 0.7143\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7451 - accuracy: 0.7394\n",
      "Epoch 00009: val_loss did not improve from 0.84379\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.7451 - accuracy: 0.7394 - val_loss: 0.8465 - val_accuracy: 0.7262\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7681 - accuracy: 0.7252\n",
      "Epoch 00010: val_loss improved from 0.84379 to 0.84187, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.7681 - accuracy: 0.7252 - val_loss: 0.8419 - val_accuracy: 0.7143\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7536 - accuracy: 0.7110\n",
      "Epoch 00011: val_loss did not improve from 0.84187\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.7536 - accuracy: 0.7110 - val_loss: 0.8419 - val_accuracy: 0.7381\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.7139\n",
      "Epoch 00012: val_loss did not improve from 0.84187\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.7307 - accuracy: 0.7139 - val_loss: 0.8434 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7682 - accuracy: 0.7110\n",
      "Epoch 00001: val_loss improved from inf to 0.83650, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.7682 - accuracy: 0.7110 - val_loss: 0.8365 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7616 - accuracy: 0.6941\n",
      "Epoch 00002: val_loss did not improve from 0.83650\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.7616 - accuracy: 0.6941 - val_loss: 0.8392 - val_accuracy: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7181 - accuracy: 0.7252\n",
      "Epoch 00003: val_loss did not improve from 0.83650\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.7181 - accuracy: 0.7252 - val_loss: 0.8431 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7490 - accuracy: 0.7224\n",
      "Epoch 00001: val_loss improved from inf to 0.84097, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.7490 - accuracy: 0.7224 - val_loss: 0.8410 - val_accuracy: 0.6786\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7237 - accuracy: 0.7365\n",
      "Epoch 00002: val_loss improved from 0.84097 to 0.83598, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.7237 - accuracy: 0.7365 - val_loss: 0.8360 - val_accuracy: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7041 - accuracy: 0.7507\n",
      "Epoch 00003: val_loss did not improve from 0.83598\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.7041 - accuracy: 0.7507 - val_loss: 0.8379 - val_accuracy: 0.7024\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.7280 \n",
      "Epoch 00004: val_loss did not improve from 0.83598\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.7141 - accuracy: 0.7280 - val_loss: 0.8390 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.7422\n",
      "Epoch 00001: val_loss improved from inf to 0.83356, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.7341 - accuracy: 0.7422 - val_loss: 0.8336 - val_accuracy: 0.6905\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.7082\n",
      "Epoch 00002: val_loss did not improve from 0.83356\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.7116 - accuracy: 0.7082 - val_loss: 0.8400 - val_accuracy: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.7365\n",
      "Epoch 00003: val_loss improved from 0.83356 to 0.82862, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.6898 - accuracy: 0.7365 - val_loss: 0.8286 - val_accuracy: 0.7024\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6965 - accuracy: 0.7365\n",
      "Epoch 00004: val_loss did not improve from 0.82862\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.6965 - accuracy: 0.7365 - val_loss: 0.8336 - val_accuracy: 0.6905\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6745 - accuracy: 0.7649\n",
      "Epoch 00005: val_loss did not improve from 0.82862\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6745 - accuracy: 0.7649 - val_loss: 0.8360 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 5\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7026 - accuracy: 0.7479\n",
      "Epoch 00001: val_loss improved from inf to 0.82694, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.7026 - accuracy: 0.7479 - val_loss: 0.8269 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6826 - accuracy: 0.7450\n",
      "Epoch 00002: val_loss did not improve from 0.82694\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.6826 - accuracy: 0.7450 - val_loss: 0.8432 - val_accuracy: 0.6548\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6892 - accuracy: 0.7507\n",
      "Epoch 00003: val_loss did not improve from 0.82694\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6892 - accuracy: 0.7507 - val_loss: 0.8341 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 6\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7294 - accuracy: 0.7337\n",
      "Epoch 00001: val_loss improved from inf to 0.83246, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.7294 - accuracy: 0.7337 - val_loss: 0.8325 - val_accuracy: 0.6905\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.7479\n",
      "Epoch 00002: val_loss did not improve from 0.83246\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6948 - accuracy: 0.7479 - val_loss: 0.8398 - val_accuracy: 0.6786\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6675 - accuracy: 0.7479\n",
      "Epoch 00003: val_loss improved from 0.83246 to 0.82681, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.6675 - accuracy: 0.7479 - val_loss: 0.8268 - val_accuracy: 0.7024\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.7734\n",
      "Epoch 00004: val_loss did not improve from 0.82681\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6590 - accuracy: 0.7734 - val_loss: 0.8338 - val_accuracy: 0.6905\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6327 - accuracy: 0.7790\n",
      "Epoch 00005: val_loss improved from 0.82681 to 0.82644, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.6327 - accuracy: 0.7790 - val_loss: 0.8264 - val_accuracy: 0.7024\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6425 - accuracy: 0.7734\n",
      "Epoch 00006: val_loss did not improve from 0.82644\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6425 - accuracy: 0.7734 - val_loss: 0.8271 - val_accuracy: 0.7024\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6408 - accuracy: 0.7677\n",
      "Epoch 00007: val_loss did not improve from 0.82644\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6408 - accuracy: 0.7677 - val_loss: 0.8273 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 7\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.7705\n",
      "Epoch 00001: val_loss improved from inf to 0.82431, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.6751 - accuracy: 0.7705 - val_loss: 0.8243 - val_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6455 - accuracy: 0.7734\n",
      "Epoch 00002: val_loss did not improve from 0.82431\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.6455 - accuracy: 0.7734 - val_loss: 0.8268 - val_accuracy: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.8074\n",
      "Epoch 00003: val_loss did not improve from 0.82431\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6191 - accuracy: 0.8074 - val_loss: 0.8298 - val_accuracy: 0.6786\n",
      "-------Iteration FT: 8\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.7875\n",
      "Epoch 00001: val_loss improved from inf to 0.82881, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.6334 - accuracy: 0.7875 - val_loss: 0.8288 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6038 - accuracy: 0.7904 \n",
      "Epoch 00002: val_loss did not improve from 0.82881\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.6038 - accuracy: 0.7904 - val_loss: 0.8390 - val_accuracy: 0.6786\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6190 - accuracy: 0.7960\n",
      "Epoch 00003: val_loss did not improve from 0.82881\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.6190 - accuracy: 0.7960 - val_loss: 0.8349 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 9\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6611 - accuracy: 0.7620\n",
      "Epoch 00001: val_loss improved from inf to 0.83467, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.6611 - accuracy: 0.7620 - val_loss: 0.8347 - val_accuracy: 0.6905\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.7790\n",
      "Epoch 00002: val_loss improved from 0.83467 to 0.82758, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6215 - accuracy: 0.7790 - val_loss: 0.8276 - val_accuracy: 0.7024\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.7790\n",
      "Epoch 00003: val_loss did not improve from 0.82758\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.6121 - accuracy: 0.7790 - val_loss: 0.8362 - val_accuracy: 0.6786\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5871 - accuracy: 0.8130\n",
      "Epoch 00004: val_loss improved from 0.82758 to 0.82598, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5871 - accuracy: 0.8130 - val_loss: 0.8260 - val_accuracy: 0.6786\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.8017\n",
      "Epoch 00005: val_loss did not improve from 0.82598\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.5754 - accuracy: 0.8017 - val_loss: 0.8321 - val_accuracy: 0.6786\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.8272\n",
      "Epoch 00006: val_loss did not improve from 0.82598\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5574 - accuracy: 0.8272 - val_loss: 0.8443 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 10\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.8045\n",
      "Epoch 00001: val_loss improved from inf to 0.82817, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.6015 - accuracy: 0.8045 - val_loss: 0.8282 - val_accuracy: 0.6905\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.7989\n",
      "Epoch 00002: val_loss improved from 0.82817 to 0.82772, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.5993 - accuracy: 0.7989 - val_loss: 0.8277 - val_accuracy: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.8130\n",
      "Epoch 00003: val_loss improved from 0.82772 to 0.82437, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5617 - accuracy: 0.8130 - val_loss: 0.8244 - val_accuracy: 0.6786\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.8102\n",
      "Epoch 00004: val_loss did not improve from 0.82437\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.5645 - accuracy: 0.8102 - val_loss: 0.8381 - val_accuracy: 0.6786\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.8329\n",
      "Epoch 00005: val_loss did not improve from 0.82437\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5375 - accuracy: 0.8329 - val_loss: 0.8264 - val_accuracy: 0.6786\n",
      "-------Iteration FT: 11\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5766 - accuracy: 0.8017\n",
      "Epoch 00001: val_loss improved from inf to 0.83842, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.5766 - accuracy: 0.8017 - val_loss: 0.8384 - val_accuracy: 0.6786\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5345 - accuracy: 0.8300\n",
      "Epoch 00002: val_loss improved from 0.83842 to 0.83555, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5345 - accuracy: 0.8300 - val_loss: 0.8356 - val_accuracy: 0.7143\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.8357\n",
      "Epoch 00003: val_loss did not improve from 0.83555\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.5337 - accuracy: 0.8357 - val_loss: 0.8392 - val_accuracy: 0.6786\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5091 - accuracy: 0.8555\n",
      "Epoch 00004: val_loss did not improve from 0.83555\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5091 - accuracy: 0.8555 - val_loss: 0.8391 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 12\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5727 - accuracy: 0.8102\n",
      "Epoch 00001: val_loss improved from inf to 0.83022, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.5727 - accuracy: 0.8102 - val_loss: 0.8302 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.8300\n",
      "Epoch 00002: val_loss did not improve from 0.83022\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.5352 - accuracy: 0.8300 - val_loss: 0.8376 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5256 - accuracy: 0.8300\n",
      "Epoch 00003: val_loss improved from 0.83022 to 0.82435, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5256 - accuracy: 0.8300 - val_loss: 0.8244 - val_accuracy: 0.6786\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.8527\n",
      "Epoch 00004: val_loss did not improve from 0.82435\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.4952 - accuracy: 0.8527 - val_loss: 0.8354 - val_accuracy: 0.6786\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.8300\n",
      "Epoch 00005: val_loss did not improve from 0.82435\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.5100 - accuracy: 0.8300 - val_loss: 0.8316 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 13\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5534 - accuracy: 0.8045\n",
      "Epoch 00001: val_loss improved from inf to 0.82938, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5534 - accuracy: 0.8045 - val_loss: 0.8294 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8272\n",
      "Epoch 00002: val_loss did not improve from 0.82938\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.5023 - accuracy: 0.8272 - val_loss: 0.8296 - val_accuracy: 0.6548\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.8640\n",
      "Epoch 00003: val_loss improved from 0.82938 to 0.82522, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.4813 - accuracy: 0.8640 - val_loss: 0.8252 - val_accuracy: 0.7024\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.8470\n",
      "Epoch 00004: val_loss improved from 0.82522 to 0.82277, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.4898 - accuracy: 0.8470 - val_loss: 0.8228 - val_accuracy: 0.7024\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.8584\n",
      "Epoch 00005: val_loss did not improve from 0.82277\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.4708 - accuracy: 0.8584 - val_loss: 0.8403 - val_accuracy: 0.7024\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.8669\n",
      "Epoch 00006: val_loss did not improve from 0.82277\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.4791 - accuracy: 0.8669 - val_loss: 0.8438 - val_accuracy: 0.6786\n",
      "-------Iteration FT: 14\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.8414\n",
      "Epoch 00001: val_loss improved from inf to 0.82662, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.5036 - accuracy: 0.8414 - val_loss: 0.8266 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.8754\n",
      "Epoch 00002: val_loss did not improve from 0.82662\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.4566 - accuracy: 0.8754 - val_loss: 0.8482 - val_accuracy: 0.6548\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4544 - accuracy: 0.8584\n",
      "Epoch 00003: val_loss did not improve from 0.82662\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.4544 - accuracy: 0.8584 - val_loss: 0.8356 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 15\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.8385\n",
      "Epoch 00001: val_loss improved from inf to 0.83022, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.5018 - accuracy: 0.8385 - val_loss: 0.8302 - val_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.8612\n",
      "Epoch 00002: val_loss did not improve from 0.83022\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.4718 - accuracy: 0.8612 - val_loss: 0.8348 - val_accuracy: 0.7024\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4341 - accuracy: 0.8867\n",
      "Epoch 00003: val_loss did not improve from 0.83022\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.4341 - accuracy: 0.8867 - val_loss: 0.8390 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 16\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.8527\n",
      "Epoch 00001: val_loss improved from inf to 0.83906, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.4813 - accuracy: 0.8527 - val_loss: 0.8391 - val_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8867\n",
      "Epoch 00002: val_loss did not improve from 0.83906\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.4255 - accuracy: 0.8867 - val_loss: 0.8395 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8867\n",
      "Epoch 00003: val_loss improved from 0.83906 to 0.83393, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.4268 - accuracy: 0.8867 - val_loss: 0.8339 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8924\n",
      "Epoch 00004: val_loss did not improve from 0.83393\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.4296 - accuracy: 0.8924 - val_loss: 0.8470 - val_accuracy: 0.7024\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4122 - accuracy: 0.8895\n",
      "Epoch 00005: val_loss improved from 0.83393 to 0.83093, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.4122 - accuracy: 0.8895 - val_loss: 0.8309 - val_accuracy: 0.7500\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4062 - accuracy: 0.8924\n",
      "Epoch 00006: val_loss did not improve from 0.83093\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.4062 - accuracy: 0.8924 - val_loss: 0.8406 - val_accuracy: 0.7262\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8810\n",
      "Epoch 00007: val_loss did not improve from 0.83093\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3949 - accuracy: 0.8810 - val_loss: 0.8432 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 17\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.8697\n",
      "Epoch 00001: val_loss improved from inf to 0.84107, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.4427 - accuracy: 0.8697 - val_loss: 0.8411 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.8924\n",
      "Epoch 00002: val_loss did not improve from 0.84107\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.4249 - accuracy: 0.8924 - val_loss: 0.8488 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8782 \n",
      "Epoch 00003: val_loss did not improve from 0.84107\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.4001 - accuracy: 0.8782 - val_loss: 0.8456 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 18\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4126 - accuracy: 0.8697\n",
      "Epoch 00001: val_loss improved from inf to 0.84558, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.4126 - accuracy: 0.8697 - val_loss: 0.8456 - val_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.8952\n",
      "Epoch 00002: val_loss improved from 0.84558 to 0.84454, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.4100 - accuracy: 0.8952 - val_loss: 0.8445 - val_accuracy: 0.7143\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.9037\n",
      "Epoch 00003: val_loss improved from 0.84454 to 0.83019, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3780 - accuracy: 0.9037 - val_loss: 0.8302 - val_accuracy: 0.7619\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8980\n",
      "Epoch 00004: val_loss did not improve from 0.83019\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3968 - accuracy: 0.8980 - val_loss: 0.8319 - val_accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.9008\n",
      "Epoch 00005: val_loss did not improve from 0.83019\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3773 - accuracy: 0.9008 - val_loss: 0.8461 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 19\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8867\n",
      "Epoch 00001: val_loss improved from inf to 0.84499, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.4036 - accuracy: 0.8867 - val_loss: 0.8450 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.9235\n",
      "Epoch 00002: val_loss improved from 0.84499 to 0.84385, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3787 - accuracy: 0.9235 - val_loss: 0.8438 - val_accuracy: 0.7381\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3714 - accuracy: 0.9150\n",
      "Epoch 00003: val_loss improved from 0.84385 to 0.83858, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3714 - accuracy: 0.9150 - val_loss: 0.8386 - val_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.9093\n",
      "Epoch 00004: val_loss did not improve from 0.83858\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.3502 - accuracy: 0.9093 - val_loss: 0.8520 - val_accuracy: 0.7143\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.8952\n",
      "Epoch 00005: val_loss improved from 0.83858 to 0.83462, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3627 - accuracy: 0.8952 - val_loss: 0.8346 - val_accuracy: 0.7619\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.9093\n",
      "Epoch 00006: val_loss did not improve from 0.83462\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.3656 - accuracy: 0.9093 - val_loss: 0.8536 - val_accuracy: 0.7262\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9207\n",
      "Epoch 00007: val_loss did not improve from 0.83462\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3281 - accuracy: 0.9207 - val_loss: 0.8704 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 20\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.9008\n",
      "Epoch 00001: val_loss improved from inf to 0.86316, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3580 - accuracy: 0.9008 - val_loss: 0.8632 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.9065\n",
      "Epoch 00002: val_loss improved from 0.86316 to 0.85744, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3572 - accuracy: 0.9065 - val_loss: 0.8574 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.9207\n",
      "Epoch 00003: val_loss did not improve from 0.85744\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.3217 - accuracy: 0.9207 - val_loss: 0.8749 - val_accuracy: 0.7262\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 0.9320\n",
      "Epoch 00004: val_loss did not improve from 0.85744\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3249 - accuracy: 0.9320 - val_loss: 0.8867 - val_accuracy: 0.7500\n",
      "-------Iteration FT: 21\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.9065\n",
      "Epoch 00001: val_loss improved from inf to 0.87284, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3572 - accuracy: 0.9065 - val_loss: 0.8728 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.9178\n",
      "Epoch 00002: val_loss improved from 0.87284 to 0.86849, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3287 - accuracy: 0.9178 - val_loss: 0.8685 - val_accuracy: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9320\n",
      "Epoch 00003: val_loss did not improve from 0.86849\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3127 - accuracy: 0.9320 - val_loss: 0.8685 - val_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3076 - accuracy: 0.9348\n",
      "Epoch 00004: val_loss improved from 0.86849 to 0.86707, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3076 - accuracy: 0.9348 - val_loss: 0.8671 - val_accuracy: 0.7500\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3199 - accuracy: 0.9178\n",
      "Epoch 00005: val_loss did not improve from 0.86707\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3199 - accuracy: 0.9178 - val_loss: 0.8923 - val_accuracy: 0.7143\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9575\n",
      "Epoch 00006: val_loss did not improve from 0.86707\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2733 - accuracy: 0.9575 - val_loss: 0.8910 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 22\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.8952\n",
      "Epoch 00001: val_loss improved from inf to 0.87658, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3396 - accuracy: 0.8952 - val_loss: 0.8766 - val_accuracy: 0.6905\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.9263\n",
      "Epoch 00002: val_loss did not improve from 0.87658\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.3102 - accuracy: 0.9263 - val_loss: 0.8848 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.9603\n",
      "Epoch 00003: val_loss did not improve from 0.87658\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2732 - accuracy: 0.9603 - val_loss: 0.8829 - val_accuracy: 0.6548\n",
      "-------Iteration FT: 23\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.9348\n",
      "Epoch 00001: val_loss improved from inf to 0.90279, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3142 - accuracy: 0.9348 - val_loss: 0.9028 - val_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.9263 \n",
      "Epoch 00002: val_loss improved from 0.90279 to 0.86542, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3000 - accuracy: 0.9263 - val_loss: 0.8654 - val_accuracy: 0.7619\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.9433 \n",
      "Epoch 00003: val_loss did not improve from 0.86542\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2786 - accuracy: 0.9433 - val_loss: 0.8807 - val_accuracy: 0.6905\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9518 \n",
      "Epoch 00004: val_loss did not improve from 0.86542\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2702 - accuracy: 0.9518 - val_loss: 0.8897 - val_accuracy: 0.7500\n",
      "-------Iteration FT: 24\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9292\n",
      "Epoch 00001: val_loss improved from inf to 0.90735, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2996 - accuracy: 0.9292 - val_loss: 0.9073 - val_accuracy: 0.6905\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9518\n",
      "Epoch 00002: val_loss improved from 0.90735 to 0.89561, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2801 - accuracy: 0.9518 - val_loss: 0.8956 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9348 \n",
      "Epoch 00003: val_loss did not improve from 0.89561\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2668 - accuracy: 0.9348 - val_loss: 0.9322 - val_accuracy: 0.6905\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.9377 \n",
      "Epoch 00004: val_loss did not improve from 0.89561\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2753 - accuracy: 0.9377 - val_loss: 0.8971 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 25\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9292 \n",
      "Epoch 00001: val_loss improved from inf to 0.89332, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.3068 - accuracy: 0.9292 - val_loss: 0.8933 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2826 - accuracy: 0.9462 \n",
      "Epoch 00002: val_loss improved from 0.89332 to 0.89255, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2826 - accuracy: 0.9462 - val_loss: 0.8926 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9547 \n",
      "Epoch 00003: val_loss did not improve from 0.89255\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2620 - accuracy: 0.9547 - val_loss: 0.9046 - val_accuracy: 0.7262\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.9688 \n",
      "Epoch 00004: val_loss did not improve from 0.89255\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.2493 - accuracy: 0.9688 - val_loss: 0.9268 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 26\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9405 \n",
      "Epoch 00001: val_loss improved from inf to 0.92162, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.2657 - accuracy: 0.9405 - val_loss: 0.9216 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9490 \n",
      "Epoch 00002: val_loss improved from 0.92162 to 0.91295, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2449 - accuracy: 0.9490 - val_loss: 0.9130 - val_accuracy: 0.7381\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9490\n",
      "Epoch 00003: val_loss did not improve from 0.91295\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2518 - accuracy: 0.9490 - val_loss: 0.9380 - val_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9518\n",
      "Epoch 00004: val_loss did not improve from 0.91295\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2486 - accuracy: 0.9518 - val_loss: 0.9289 - val_accuracy: 0.7381\n",
      "-------Iteration FT: 27\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.9490 \n",
      "Epoch 00001: val_loss improved from inf to 0.96522, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.2774 - accuracy: 0.9490 - val_loss: 0.9652 - val_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2315 - accuracy: 0.9603 \n",
      "Epoch 00002: val_loss improved from 0.96522 to 0.93320, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2315 - accuracy: 0.9603 - val_loss: 0.9332 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.9688 \n",
      "Epoch 00003: val_loss did not improve from 0.93320\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2233 - accuracy: 0.9688 - val_loss: 0.9633 - val_accuracy: 0.7262\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9518 \n",
      "Epoch 00004: val_loss did not improve from 0.93320\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2285 - accuracy: 0.9518 - val_loss: 0.9715 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 28\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9547 \n",
      "Epoch 00001: val_loss improved from inf to 0.96000, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.2526 - accuracy: 0.9547 - val_loss: 0.9600 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9603 \n",
      "Epoch 00002: val_loss improved from 0.96000 to 0.95203, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2482 - accuracy: 0.9603 - val_loss: 0.9520 - val_accuracy: 0.7619\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9632 \n",
      "Epoch 00003: val_loss did not improve from 0.95203\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2290 - accuracy: 0.9632 - val_loss: 0.9749 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9688 \n",
      "Epoch 00004: val_loss did not improve from 0.95203\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2220 - accuracy: 0.9688 - val_loss: 0.9714 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 29\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9632 \n",
      "Epoch 00001: val_loss improved from inf to 0.99591, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.2454 - accuracy: 0.9632 - val_loss: 0.9959 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2237 - accuracy: 0.9490 \n",
      "Epoch 00002: val_loss improved from 0.99591 to 0.95261, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2237 - accuracy: 0.9490 - val_loss: 0.9526 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9547 \n",
      "Epoch 00003: val_loss did not improve from 0.95261\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2411 - accuracy: 0.9547 - val_loss: 0.9760 - val_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.9773\n",
      "Epoch 00004: val_loss did not improve from 0.95261\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.1978 - accuracy: 0.9773 - val_loss: 0.9588 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 30\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9575 \n",
      "Epoch 00001: val_loss improved from inf to 0.97090, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2335 - accuracy: 0.9575 - val_loss: 0.9709 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9603 \n",
      "Epoch 00002: val_loss improved from 0.97090 to 0.96919, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2249 - accuracy: 0.9603 - val_loss: 0.9692 - val_accuracy: 0.7024\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9717 \n",
      "Epoch 00003: val_loss did not improve from 0.96919\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2175 - accuracy: 0.9717 - val_loss: 0.9832 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9717 \n",
      "Epoch 00004: val_loss did not improve from 0.96919\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.2020 - accuracy: 0.9717 - val_loss: 0.9756 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 31\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.9603\n",
      "Epoch 00001: val_loss improved from inf to 0.98845, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2281 - accuracy: 0.9603 - val_loss: 0.9884 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9575\n",
      "Epoch 00002: val_loss did not improve from 0.98845\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.2255 - accuracy: 0.9575 - val_loss: 1.0080 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9717\n",
      "Epoch 00003: val_loss did not improve from 0.98845\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.2041 - accuracy: 0.9717 - val_loss: 1.0163 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 32\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.9490\n",
      "Epoch 00001: val_loss improved from inf to 1.01573, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.2373 - accuracy: 0.9490 - val_loss: 1.0157 - val_accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9802\n",
      "Epoch 00002: val_loss improved from 1.01573 to 1.00897, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1930 - accuracy: 0.9802 - val_loss: 1.0090 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9717\n",
      "Epoch 00003: val_loss did not improve from 1.00897\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.2063 - accuracy: 0.9717 - val_loss: 1.0251 - val_accuracy: 0.6905\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9745\n",
      "Epoch 00004: val_loss did not improve from 1.00897\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.2004 - accuracy: 0.9745 - val_loss: 1.0325 - val_accuracy: 0.7619\n",
      "-------Iteration FT: 33\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2207 - accuracy: 0.9575\n",
      "Epoch 00001: val_loss improved from inf to 1.00469, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.2207 - accuracy: 0.9575 - val_loss: 1.0047 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9688\n",
      "Epoch 00002: val_loss did not improve from 1.00469\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2153 - accuracy: 0.9688 - val_loss: 1.0168 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9660\n",
      "Epoch 00003: val_loss did not improve from 1.00469\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1953 - accuracy: 0.9660 - val_loss: 1.0086 - val_accuracy: 0.7381\n",
      "-------Iteration FT: 34\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2023 - accuracy: 0.9632\n",
      "Epoch 00001: val_loss improved from inf to 1.02066, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.2023 - accuracy: 0.9632 - val_loss: 1.0207 - val_accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9660\n",
      "Epoch 00002: val_loss improved from 1.02066 to 1.01010, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2058 - accuracy: 0.9660 - val_loss: 1.0101 - val_accuracy: 0.7381\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9773\n",
      "Epoch 00003: val_loss did not improve from 1.01010\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1925 - accuracy: 0.9773 - val_loss: 1.0687 - val_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9688\n",
      "Epoch 00004: val_loss did not improve from 1.01010\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1937 - accuracy: 0.9688 - val_loss: 1.0430 - val_accuracy: 0.7500\n",
      "-------Iteration FT: 35\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9802\n",
      "Epoch 00001: val_loss improved from inf to 1.05193, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1970 - accuracy: 0.9802 - val_loss: 1.0519 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9688\n",
      "Epoch 00002: val_loss did not improve from 1.05193\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1981 - accuracy: 0.9688 - val_loss: 1.0548 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9773\n",
      "Epoch 00003: val_loss improved from 1.05193 to 1.01635, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1844 - accuracy: 0.9773 - val_loss: 1.0163 - val_accuracy: 0.7381\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9773\n",
      "Epoch 00004: val_loss did not improve from 1.01635\n",
      "12/12 [==============================] - 115s 9s/step - loss: 0.1790 - accuracy: 0.9773 - val_loss: 1.0355 - val_accuracy: 0.7381\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9802\n",
      "Epoch 00005: val_loss did not improve from 1.01635\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1774 - accuracy: 0.9802 - val_loss: 1.0304 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 36\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9745\n",
      "Epoch 00001: val_loss improved from inf to 1.06763, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1918 - accuracy: 0.9745 - val_loss: 1.0676 - val_accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9802\n",
      "Epoch 00002: val_loss improved from 1.06763 to 1.01922, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.1777 - accuracy: 0.9802 - val_loss: 1.0192 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9858\n",
      "Epoch 00003: val_loss did not improve from 1.01922\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1631 - accuracy: 0.9858 - val_loss: 1.0629 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9915\n",
      "Epoch 00004: val_loss did not improve from 1.01922\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1550 - accuracy: 0.9915 - val_loss: 1.0689 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 37\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9887\n",
      "Epoch 00001: val_loss improved from inf to 1.06306, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1671 - accuracy: 0.9887 - val_loss: 1.0631 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9745\n",
      "Epoch 00002: val_loss improved from 1.06306 to 1.04687, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1681 - accuracy: 0.9745 - val_loss: 1.0469 - val_accuracy: 0.7262\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9830\n",
      "Epoch 00003: val_loss did not improve from 1.04687\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1639 - accuracy: 0.9830 - val_loss: 1.0998 - val_accuracy: 0.7381\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9887\n",
      "Epoch 00004: val_loss did not improve from 1.04687\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1670 - accuracy: 0.9887 - val_loss: 1.1230 - val_accuracy: 0.7262\n",
      "-------Iteration FT: 38\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9802\n",
      "Epoch 00001: val_loss improved from inf to 1.10886, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.1692 - accuracy: 0.9802 - val_loss: 1.1089 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9802\n",
      "Epoch 00002: val_loss improved from 1.10886 to 1.05607, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1626 - accuracy: 0.9802 - val_loss: 1.0561 - val_accuracy: 0.7619\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9802\n",
      "Epoch 00003: val_loss did not improve from 1.05607\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1704 - accuracy: 0.9802 - val_loss: 1.1198 - val_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9915\n",
      "Epoch 00004: val_loss did not improve from 1.05607\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.1630 - accuracy: 0.9915 - val_loss: 1.1522 - val_accuracy: 0.7619\n",
      "-------Iteration FT: 39\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9773\n",
      "Epoch 00001: val_loss improved from inf to 1.09966, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.1649 - accuracy: 0.9773 - val_loss: 1.0997 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.9887\n",
      "Epoch 00002: val_loss did not improve from 1.09966\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1445 - accuracy: 0.9887 - val_loss: 1.1367 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9943\n",
      "Epoch 00003: val_loss did not improve from 1.09966\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.1522 - accuracy: 0.9943 - val_loss: 1.1256 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 40\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9802\n",
      "Epoch 00001: val_loss improved from inf to 1.10224, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1626 - accuracy: 0.9802 - val_loss: 1.1022 - val_accuracy: 0.7262\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9773\n",
      "Epoch 00002: val_loss did not improve from 1.10224\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1558 - accuracy: 0.9773 - val_loss: 1.2397 - val_accuracy: 0.7143\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1693 - accuracy: 0.9858\n",
      "Epoch 00003: val_loss did not improve from 1.10224\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1693 - accuracy: 0.9858 - val_loss: 1.1667 - val_accuracy: 0.7500\n",
      "-------Iteration FT: 41\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9773\n",
      "Epoch 00001: val_loss improved from inf to 1.18125, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1622 - accuracy: 0.9773 - val_loss: 1.1813 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9830\n",
      "Epoch 00002: val_loss improved from 1.18125 to 1.12219, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1623 - accuracy: 0.9830 - val_loss: 1.1222 - val_accuracy: 0.7381\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9773\n",
      "Epoch 00003: val_loss did not improve from 1.12219\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.1590 - accuracy: 0.9773 - val_loss: 1.1803 - val_accuracy: 0.7262\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9858\n",
      "Epoch 00004: val_loss did not improve from 1.12219\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1492 - accuracy: 0.9858 - val_loss: 1.1895 - val_accuracy: 0.7381\n",
      "-------Iteration FT: 42\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9830\n",
      "Epoch 00001: val_loss improved from inf to 1.10900, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1679 - accuracy: 0.9830 - val_loss: 1.1090 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9830\n",
      "Epoch 00002: val_loss did not improve from 1.10900\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.1470 - accuracy: 0.9830 - val_loss: 1.1580 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9858\n",
      "Epoch 00003: val_loss did not improve from 1.10900\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1502 - accuracy: 0.9858 - val_loss: 1.1153 - val_accuracy: 0.7381\n",
      "-------Iteration FT: 43\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9830\n",
      "Epoch 00001: val_loss improved from inf to 1.15384, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1554 - accuracy: 0.9830 - val_loss: 1.1538 - val_accuracy: 0.7381\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.9887\n",
      "Epoch 00002: val_loss did not improve from 1.15384\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1464 - accuracy: 0.9887 - val_loss: 1.1833 - val_accuracy: 0.7619\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9943\n",
      "Epoch 00003: val_loss did not improve from 1.15384\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1466 - accuracy: 0.9943 - val_loss: 1.2389 - val_accuracy: 0.7500\n",
      "-------Iteration FT: 44\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9887\n",
      "Epoch 00001: val_loss improved from inf to 1.15052, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1406 - accuracy: 0.9887 - val_loss: 1.1505 - val_accuracy: 0.7143\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9887\n",
      "Epoch 00002: val_loss did not improve from 1.15052\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1451 - accuracy: 0.9887 - val_loss: 1.1556 - val_accuracy: 0.7619\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9887\n",
      "Epoch 00003: val_loss did not improve from 1.15052\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1386 - accuracy: 0.9887 - val_loss: 1.2192 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 45\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9887\n",
      "Epoch 00001: val_loss improved from inf to 1.21345, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1419 - accuracy: 0.9887 - val_loss: 1.2135 - val_accuracy: 0.7857\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9830\n",
      "Epoch 00002: val_loss improved from 1.21345 to 1.20783, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1479 - accuracy: 0.9830 - val_loss: 1.2078 - val_accuracy: 0.7024\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 0.9943\n",
      "Epoch 00003: val_loss improved from 1.20783 to 1.15260, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1376 - accuracy: 0.9943 - val_loss: 1.1526 - val_accuracy: 0.7381\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9915\n",
      "Epoch 00004: val_loss did not improve from 1.15260\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1341 - accuracy: 0.9915 - val_loss: 1.1565 - val_accuracy: 0.7619\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9830\n",
      "Epoch 00005: val_loss did not improve from 1.15260\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1397 - accuracy: 0.9830 - val_loss: 1.2184 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 46\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9887\n",
      "Epoch 00001: val_loss improved from inf to 1.15554, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1419 - accuracy: 0.9887 - val_loss: 1.1555 - val_accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9887\n",
      "Epoch 00002: val_loss did not improve from 1.15554\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1388 - accuracy: 0.9887 - val_loss: 1.2690 - val_accuracy: 0.7381\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9858\n",
      "Epoch 00003: val_loss did not improve from 1.15554\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1377 - accuracy: 0.9858 - val_loss: 1.2748 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 47\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9773\n",
      "Epoch 00001: val_loss improved from inf to 1.20062, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.1685 - accuracy: 0.9773 - val_loss: 1.2006 - val_accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9773\n",
      "Epoch 00002: val_loss did not improve from 1.20062\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1551 - accuracy: 0.9773 - val_loss: 1.2193 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9745\n",
      "Epoch 00003: val_loss did not improve from 1.20062\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1482 - accuracy: 0.9745 - val_loss: 1.2246 - val_accuracy: 0.7500\n",
      "-------Iteration FT: 48\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9830\n",
      "Epoch 00001: val_loss improved from inf to 1.22832, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.1509 - accuracy: 0.9830 - val_loss: 1.2283 - val_accuracy: 0.7619\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9943\n",
      "Epoch 00002: val_loss improved from 1.22832 to 1.21473, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1301 - accuracy: 0.9943 - val_loss: 1.2147 - val_accuracy: 0.7381\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9858\n",
      "Epoch 00003: val_loss did not improve from 1.21473\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1380 - accuracy: 0.9858 - val_loss: 1.2457 - val_accuracy: 0.7619\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9915\n",
      "Epoch 00004: val_loss did not improve from 1.21473\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1274 - accuracy: 0.9915 - val_loss: 1.2594 - val_accuracy: 0.7619\n",
      "-------Iteration FT: 49\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9943\n",
      "Epoch 00001: val_loss improved from inf to 1.23520, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.1344 - accuracy: 0.9943 - val_loss: 1.2352 - val_accuracy: 0.7024\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9773\n",
      "Epoch 00002: val_loss did not improve from 1.23520\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1680 - accuracy: 0.9773 - val_loss: 1.3049 - val_accuracy: 0.7381\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9858\n",
      "Epoch 00003: val_loss improved from 1.23520 to 1.19578, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1412 - accuracy: 0.9858 - val_loss: 1.1958 - val_accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9943\n",
      "Epoch 00004: val_loss did not improve from 1.19578\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1253 - accuracy: 0.9943 - val_loss: 1.2817 - val_accuracy: 0.7619\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9972\n",
      "Epoch 00005: val_loss did not improve from 1.19578\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1160 - accuracy: 0.9972 - val_loss: 1.2318 - val_accuracy: 0.7619\n",
      "Evaluating casme2-sub03\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4469 - accuracy: 0.4419\n",
      "Epoch 00001: val_loss improved from inf to 2.15876, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 110s 9s/step - loss: 2.4469 - accuracy: 0.4419 - val_loss: 2.1588 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1496 - accuracy: 0.5666\n",
      "Epoch 00002: val_loss improved from 2.15876 to 1.99060, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1496 - accuracy: 0.5666 - val_loss: 1.9906 - val_accuracy: 0.6250\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.9631 - accuracy: 0.5297\n",
      "Epoch 00003: val_loss improved from 1.99060 to 1.82140, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.9631 - accuracy: 0.5297 - val_loss: 1.8214 - val_accuracy: 0.6250\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.7717 - accuracy: 0.5779\n",
      "Epoch 00004: val_loss improved from 1.82140 to 1.67669, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.7717 - accuracy: 0.5779 - val_loss: 1.6767 - val_accuracy: 0.6250\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6420 - accuracy: 0.5637\n",
      "Epoch 00005: val_loss improved from 1.67669 to 1.55756, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.6420 - accuracy: 0.5637 - val_loss: 1.5576 - val_accuracy: 0.6364\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.5609\n",
      "Epoch 00006: val_loss improved from 1.55756 to 1.44027, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.5289 - accuracy: 0.5609 - val_loss: 1.4403 - val_accuracy: 0.6364\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3830 - accuracy: 0.5609\n",
      "Epoch 00007: val_loss improved from 1.44027 to 1.34986, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.3830 - accuracy: 0.5609 - val_loss: 1.3499 - val_accuracy: 0.6364\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3241 - accuracy: 0.5807\n",
      "Epoch 00008: val_loss improved from 1.34986 to 1.28118, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.3241 - accuracy: 0.5807 - val_loss: 1.2812 - val_accuracy: 0.6364\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2400 - accuracy: 0.5722\n",
      "Epoch 00009: val_loss improved from 1.28118 to 1.22636, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.2400 - accuracy: 0.5722 - val_loss: 1.2264 - val_accuracy: 0.6477\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2015 - accuracy: 0.5751\n",
      "Epoch 00010: val_loss improved from 1.22636 to 1.17725, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.2015 - accuracy: 0.5751 - val_loss: 1.1773 - val_accuracy: 0.6364\n",
      "-------Iteration Head: 1\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1597 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 1.15583, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.1597 - accuracy: 0.5666 - val_loss: 1.1558 - val_accuracy: 0.6591\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1388 - accuracy: 0.5609\n",
      "Epoch 00002: val_loss improved from 1.15583 to 1.11818, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.1388 - accuracy: 0.5609 - val_loss: 1.1182 - val_accuracy: 0.6250\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0966 - accuracy: 0.5864\n",
      "Epoch 00003: val_loss improved from 1.11818 to 1.10043, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0966 - accuracy: 0.5864 - val_loss: 1.1004 - val_accuracy: 0.6364\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0945 - accuracy: 0.5892\n",
      "Epoch 00004: val_loss improved from 1.10043 to 1.08677, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0945 - accuracy: 0.5892 - val_loss: 1.0868 - val_accuracy: 0.6364\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.5722\n",
      "Epoch 00005: val_loss improved from 1.08677 to 1.07440, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0737 - accuracy: 0.5722 - val_loss: 1.0744 - val_accuracy: 0.6364\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0665 - accuracy: 0.5921\n",
      "Epoch 00006: val_loss improved from 1.07440 to 1.06572, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0665 - accuracy: 0.5921 - val_loss: 1.0657 - val_accuracy: 0.6477\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0532 - accuracy: 0.5864\n",
      "Epoch 00007: val_loss improved from 1.06572 to 1.05779, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0532 - accuracy: 0.5864 - val_loss: 1.0578 - val_accuracy: 0.6477\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0636 - accuracy: 0.5864\n",
      "Epoch 00008: val_loss improved from 1.05779 to 1.05376, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0636 - accuracy: 0.5864 - val_loss: 1.0538 - val_accuracy: 0.6477\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0592 - accuracy: 0.5751\n",
      "Epoch 00009: val_loss improved from 1.05376 to 1.05162, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 1.0592 - accuracy: 0.5751 - val_loss: 1.0516 - val_accuracy: 0.6477\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0522 - accuracy: 0.5864\n",
      "Epoch 00010: val_loss improved from 1.05162 to 1.04983, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0522 - accuracy: 0.5864 - val_loss: 1.0498 - val_accuracy: 0.6364\n",
      "-------Iteration Head: 2\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0473 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 1.06411, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0473 - accuracy: 0.5892 - val_loss: 1.0641 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0477 - accuracy: 0.5751\n",
      "Epoch 00002: val_loss improved from 1.06411 to 1.04388, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 1.0477 - accuracy: 0.5751 - val_loss: 1.0439 - val_accuracy: 0.6364\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0423 - accuracy: 0.5666\n",
      "Epoch 00003: val_loss improved from 1.04388 to 1.03969, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 1.0423 - accuracy: 0.5666 - val_loss: 1.0397 - val_accuracy: 0.6477\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0275 - accuracy: 0.5751\n",
      "Epoch 00004: val_loss improved from 1.03969 to 1.03566, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0275 - accuracy: 0.5751 - val_loss: 1.0357 - val_accuracy: 0.6477\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0314 - accuracy: 0.6147\n",
      "Epoch 00005: val_loss improved from 1.03566 to 1.03015, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0314 - accuracy: 0.6147 - val_loss: 1.0302 - val_accuracy: 0.6477\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0087 - accuracy: 0.5892\n",
      "Epoch 00006: val_loss improved from 1.03015 to 1.02938, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0087 - accuracy: 0.5892 - val_loss: 1.0294 - val_accuracy: 0.6477\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0155 - accuracy: 0.5864\n",
      "Epoch 00007: val_loss did not improve from 1.02938\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0155 - accuracy: 0.5864 - val_loss: 1.0316 - val_accuracy: 0.6477\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0153 - accuracy: 0.5892\n",
      "Epoch 00008: val_loss improved from 1.02938 to 1.02650, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0153 - accuracy: 0.5892 - val_loss: 1.0265 - val_accuracy: 0.6477\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0035 - accuracy: 0.5722\n",
      "Epoch 00009: val_loss improved from 1.02650 to 1.02501, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0035 - accuracy: 0.5722 - val_loss: 1.0250 - val_accuracy: 0.6477\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.5779\n",
      "Epoch 00010: val_loss improved from 1.02501 to 1.02087, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0086 - accuracy: 0.5779 - val_loss: 1.0209 - val_accuracy: 0.6477\n",
      "-------Iteration Head: 3\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0305 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 1.04311, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0305 - accuracy: 0.5694 - val_loss: 1.0431 - val_accuracy: 0.6250\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0093 - accuracy: 0.5892\n",
      "Epoch 00002: val_loss improved from 1.04311 to 1.02519, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0093 - accuracy: 0.5892 - val_loss: 1.0252 - val_accuracy: 0.6477\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9995 - accuracy: 0.5836\n",
      "Epoch 00003: val_loss improved from 1.02519 to 1.02358, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.9995 - accuracy: 0.5836 - val_loss: 1.0236 - val_accuracy: 0.6477\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.5694\n",
      "Epoch 00004: val_loss improved from 1.02358 to 1.02292, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 1.0068 - accuracy: 0.5694 - val_loss: 1.0229 - val_accuracy: 0.6477\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9977 - accuracy: 0.5864\n",
      "Epoch 00005: val_loss improved from 1.02292 to 1.02000, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.9977 - accuracy: 0.5864 - val_loss: 1.0200 - val_accuracy: 0.6477\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9912 - accuracy: 0.5864\n",
      "Epoch 00006: val_loss improved from 1.02000 to 1.01565, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.9912 - accuracy: 0.5864 - val_loss: 1.0157 - val_accuracy: 0.6477\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0001 - accuracy: 0.5921\n",
      "Epoch 00007: val_loss did not improve from 1.01565\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0001 - accuracy: 0.5921 - val_loss: 1.0159 - val_accuracy: 0.6477\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0025 - accuracy: 0.5892\n",
      "Epoch 00008: val_loss improved from 1.01565 to 1.01397, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0025 - accuracy: 0.5892 - val_loss: 1.0140 - val_accuracy: 0.6477\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9951 - accuracy: 0.6062\n",
      "Epoch 00009: val_loss improved from 1.01397 to 1.01211, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9951 - accuracy: 0.6062 - val_loss: 1.0121 - val_accuracy: 0.6477\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0035 - accuracy: 0.5836\n",
      "Epoch 00010: val_loss did not improve from 1.01211\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0035 - accuracy: 0.5836 - val_loss: 1.0137 - val_accuracy: 0.6477\n",
      "-------Iteration Head: 4\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0164 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 1.03359, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0164 - accuracy: 0.5807 - val_loss: 1.0336 - val_accuracy: 0.6364\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0250 - accuracy: 0.5836\n",
      "Epoch 00002: val_loss improved from 1.03359 to 1.01848, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 1.0250 - accuracy: 0.5836 - val_loss: 1.0185 - val_accuracy: 0.6477\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0124 - accuracy: 0.5977\n",
      "Epoch 00003: val_loss did not improve from 1.01848\n",
      "12/12 [==============================] - 110s 9s/step - loss: 1.0124 - accuracy: 0.5977 - val_loss: 1.0194 - val_accuracy: 0.6477\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9950 - accuracy: 0.6034\n",
      "Epoch 00004: val_loss improved from 1.01848 to 1.01243, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.9950 - accuracy: 0.6034 - val_loss: 1.0124 - val_accuracy: 0.6477\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9922 - accuracy: 0.5864\n",
      "Epoch 00005: val_loss improved from 1.01243 to 1.01139, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9922 - accuracy: 0.5864 - val_loss: 1.0114 - val_accuracy: 0.6477\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9938 - accuracy: 0.5977\n",
      "Epoch 00006: val_loss improved from 1.01139 to 1.00819, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.9938 - accuracy: 0.5977 - val_loss: 1.0082 - val_accuracy: 0.6477\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9898 - accuracy: 0.5892\n",
      "Epoch 00007: val_loss did not improve from 1.00819\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9898 - accuracy: 0.5892 - val_loss: 1.0091 - val_accuracy: 0.6477\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9920 - accuracy: 0.5836\n",
      "Epoch 00008: val_loss did not improve from 1.00819\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9920 - accuracy: 0.5836 - val_loss: 1.0102 - val_accuracy: 0.6477\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0218 - accuracy: 0.5977\n",
      "Epoch 00001: val_loss improved from inf to 1.01754, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 1.0218 - accuracy: 0.5977 - val_loss: 1.0175 - val_accuracy: 0.6477\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0004 - accuracy: 0.5892\n",
      "Epoch 00002: val_loss improved from 1.01754 to 1.01255, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 1.0004 - accuracy: 0.5892 - val_loss: 1.0126 - val_accuracy: 0.6477\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9829 - accuracy: 0.5977\n",
      "Epoch 00003: val_loss improved from 1.01255 to 1.00895, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.9829 - accuracy: 0.5977 - val_loss: 1.0090 - val_accuracy: 0.6477\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9811 - accuracy: 0.5864\n",
      "Epoch 00004: val_loss improved from 1.00895 to 1.00842, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 0.9811 - accuracy: 0.5864 - val_loss: 1.0084 - val_accuracy: 0.6477\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9652 - accuracy: 0.6062\n",
      "Epoch 00005: val_loss improved from 1.00842 to 1.00635, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9652 - accuracy: 0.6062 - val_loss: 1.0063 - val_accuracy: 0.6477\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9753 - accuracy: 0.5949\n",
      "Epoch 00006: val_loss improved from 1.00635 to 1.00012, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.9753 - accuracy: 0.5949 - val_loss: 1.0001 - val_accuracy: 0.6477\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9545 - accuracy: 0.6006\n",
      "Epoch 00007: val_loss improved from 1.00012 to 0.99492, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9545 - accuracy: 0.6006 - val_loss: 0.9949 - val_accuracy: 0.6477\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9279 - accuracy: 0.6204\n",
      "Epoch 00008: val_loss improved from 0.99492 to 0.99017, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.9279 - accuracy: 0.6204 - val_loss: 0.9902 - val_accuracy: 0.6364\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9313 - accuracy: 0.6091\n",
      "Epoch 00009: val_loss improved from 0.99017 to 0.98791, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.9313 - accuracy: 0.6091 - val_loss: 0.9879 - val_accuracy: 0.6364\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.6232\n",
      "Epoch 00010: val_loss improved from 0.98791 to 0.98292, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 0.9312 - accuracy: 0.6232 - val_loss: 0.9829 - val_accuracy: 0.6364\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9057 - accuracy: 0.6346\n",
      "Epoch 00011: val_loss improved from 0.98292 to 0.97896, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 0.9057 - accuracy: 0.6346 - val_loss: 0.9790 - val_accuracy: 0.6364\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9072 - accuracy: 0.6402\n",
      "Epoch 00012: val_loss improved from 0.97896 to 0.97756, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.9072 - accuracy: 0.6402 - val_loss: 0.9776 - val_accuracy: 0.6364\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9024 - accuracy: 0.6431\n",
      "Epoch 00013: val_loss improved from 0.97756 to 0.97704, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.9024 - accuracy: 0.6431 - val_loss: 0.9770 - val_accuracy: 0.6364\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8806 - accuracy: 0.6629\n",
      "Epoch 00014: val_loss improved from 0.97704 to 0.97139, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8806 - accuracy: 0.6629 - val_loss: 0.9714 - val_accuracy: 0.6364\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8741 - accuracy: 0.6544\n",
      "Epoch 00015: val_loss improved from 0.97139 to 0.96851, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.8741 - accuracy: 0.6544 - val_loss: 0.9685 - val_accuracy: 0.6364\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8561 - accuracy: 0.6884\n",
      "Epoch 00016: val_loss improved from 0.96851 to 0.96788, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8561 - accuracy: 0.6884 - val_loss: 0.9679 - val_accuracy: 0.6250\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.6771\n",
      "Epoch 00017: val_loss did not improve from 0.96788\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.8623 - accuracy: 0.6771 - val_loss: 0.9700 - val_accuracy: 0.6250\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8472 - accuracy: 0.6686\n",
      "Epoch 00018: val_loss improved from 0.96788 to 0.95990, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8472 - accuracy: 0.6686 - val_loss: 0.9599 - val_accuracy: 0.6364\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8257 - accuracy: 0.6771\n",
      "Epoch 00019: val_loss improved from 0.95990 to 0.95338, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8257 - accuracy: 0.6771 - val_loss: 0.9534 - val_accuracy: 0.6364\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8264 - accuracy: 0.6742\n",
      "Epoch 00020: val_loss improved from 0.95338 to 0.95243, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8264 - accuracy: 0.6742 - val_loss: 0.9524 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8515 - accuracy: 0.6459\n",
      "Epoch 00001: val_loss improved from inf to 0.96049, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8515 - accuracy: 0.6459 - val_loss: 0.9605 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8256 - accuracy: 0.6544\n",
      "Epoch 00002: val_loss improved from 0.96049 to 0.95060, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.8256 - accuracy: 0.6544 - val_loss: 0.9506 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8063 - accuracy: 0.6799\n",
      "Epoch 00003: val_loss did not improve from 0.95060\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.8063 - accuracy: 0.6799 - val_loss: 0.9560 - val_accuracy: 0.6250\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7912 - accuracy: 0.6657\n",
      "Epoch 00004: val_loss did not improve from 0.95060\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.7912 - accuracy: 0.6657 - val_loss: 0.9547 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.6912\n",
      "Epoch 00001: val_loss improved from inf to 0.95695, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.8286 - accuracy: 0.6912 - val_loss: 0.9569 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7836 - accuracy: 0.6941\n",
      "Epoch 00002: val_loss improved from 0.95695 to 0.95389, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.7836 - accuracy: 0.6941 - val_loss: 0.9539 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7933 - accuracy: 0.6969\n",
      "Epoch 00003: val_loss did not improve from 0.95389\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7933 - accuracy: 0.6969 - val_loss: 0.9561 - val_accuracy: 0.6250\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7725 - accuracy: 0.6827\n",
      "Epoch 00004: val_loss did not improve from 0.95389\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7725 - accuracy: 0.6827 - val_loss: 0.9550 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8095 - accuracy: 0.6912\n",
      "Epoch 00001: val_loss improved from inf to 0.96285, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.8095 - accuracy: 0.6912 - val_loss: 0.9628 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7630 - accuracy: 0.7167\n",
      "Epoch 00002: val_loss improved from 0.96285 to 0.94961, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7630 - accuracy: 0.7167 - val_loss: 0.9496 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7682 - accuracy: 0.6941\n",
      "Epoch 00003: val_loss improved from 0.94961 to 0.94924, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7682 - accuracy: 0.6941 - val_loss: 0.9492 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7541 - accuracy: 0.7167\n",
      "Epoch 00004: val_loss did not improve from 0.94924\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7541 - accuracy: 0.7167 - val_loss: 0.9494 - val_accuracy: 0.6136\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.7025\n",
      "Epoch 00005: val_loss did not improve from 0.94924\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7661 - accuracy: 0.7025 - val_loss: 0.9540 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7715 - accuracy: 0.6997\n",
      "Epoch 00001: val_loss improved from inf to 0.96367, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7715 - accuracy: 0.6997 - val_loss: 0.9637 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7644 - accuracy: 0.7110\n",
      "Epoch 00002: val_loss improved from 0.96367 to 0.95390, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7644 - accuracy: 0.7110 - val_loss: 0.9539 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7327 - accuracy: 0.7195\n",
      "Epoch 00003: val_loss did not improve from 0.95390\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7327 - accuracy: 0.7195 - val_loss: 0.9548 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7158 - accuracy: 0.7280\n",
      "Epoch 00004: val_loss did not improve from 0.95390\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7158 - accuracy: 0.7280 - val_loss: 0.9549 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 5\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7555 - accuracy: 0.7082\n",
      "Epoch 00001: val_loss improved from inf to 0.95319, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.7555 - accuracy: 0.7082 - val_loss: 0.9532 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.7224\n",
      "Epoch 00002: val_loss did not improve from 0.95319\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6976 - accuracy: 0.7224 - val_loss: 0.9582 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7075 - accuracy: 0.7479\n",
      "Epoch 00003: val_loss improved from 0.95319 to 0.94727, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7075 - accuracy: 0.7479 - val_loss: 0.9473 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7177 - accuracy: 0.7280\n",
      "Epoch 00004: val_loss did not improve from 0.94727\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.7177 - accuracy: 0.7280 - val_loss: 0.9539 - val_accuracy: 0.6136\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6969 - accuracy: 0.7280\n",
      "Epoch 00005: val_loss did not improve from 0.94727\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6969 - accuracy: 0.7280 - val_loss: 0.9500 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 6\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.7422\n",
      "Epoch 00001: val_loss improved from inf to 0.96369, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7113 - accuracy: 0.7422 - val_loss: 0.9637 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.7337\n",
      "Epoch 00002: val_loss improved from 0.96369 to 0.94347, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6992 - accuracy: 0.7337 - val_loss: 0.9435 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.7450\n",
      "Epoch 00003: val_loss did not improve from 0.94347\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6766 - accuracy: 0.7450 - val_loss: 0.9480 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6839 - accuracy: 0.7394\n",
      "Epoch 00004: val_loss did not improve from 0.94347\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6839 - accuracy: 0.7394 - val_loss: 0.9542 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 7\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7015 - accuracy: 0.7309\n",
      "Epoch 00001: val_loss improved from inf to 0.96660, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.7015 - accuracy: 0.7309 - val_loss: 0.9666 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.7365\n",
      "Epoch 00002: val_loss improved from 0.96660 to 0.95972, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6627 - accuracy: 0.7365 - val_loss: 0.9597 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6577 - accuracy: 0.7592\n",
      "Epoch 00003: val_loss improved from 0.95972 to 0.95449, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6577 - accuracy: 0.7592 - val_loss: 0.9545 - val_accuracy: 0.6364\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.7649\n",
      "Epoch 00004: val_loss did not improve from 0.95449\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6406 - accuracy: 0.7649 - val_loss: 0.9647 - val_accuracy: 0.6250\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.7875\n",
      "Epoch 00005: val_loss did not improve from 0.95449\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6121 - accuracy: 0.7875 - val_loss: 0.9638 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 8\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6516 - accuracy: 0.7705\n",
      "Epoch 00001: val_loss improved from inf to 0.96218, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6516 - accuracy: 0.7705 - val_loss: 0.9622 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6525 - accuracy: 0.7592\n",
      "Epoch 00002: val_loss did not improve from 0.96218\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6525 - accuracy: 0.7592 - val_loss: 0.9687 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6245 - accuracy: 0.7564\n",
      "Epoch 00003: val_loss did not improve from 0.96218\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6245 - accuracy: 0.7564 - val_loss: 0.9629 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 9\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.7479\n",
      "Epoch 00001: val_loss improved from inf to 0.96155, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.6537 - accuracy: 0.7479 - val_loss: 0.9616 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.7705\n",
      "Epoch 00002: val_loss improved from 0.96155 to 0.95800, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6188 - accuracy: 0.7705 - val_loss: 0.9580 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6270 - accuracy: 0.7564\n",
      "Epoch 00003: val_loss did not improve from 0.95800\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.6270 - accuracy: 0.7564 - val_loss: 0.9632 - val_accuracy: 0.6477\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6087 - accuracy: 0.7904\n",
      "Epoch 00004: val_loss did not improve from 0.95800\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6087 - accuracy: 0.7904 - val_loss: 0.9718 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 10\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.7535\n",
      "Epoch 00001: val_loss improved from inf to 0.95839, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6497 - accuracy: 0.7535 - val_loss: 0.9584 - val_accuracy: 0.6591\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6234 - accuracy: 0.7847\n",
      "Epoch 00002: val_loss improved from 0.95839 to 0.95522, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6234 - accuracy: 0.7847 - val_loss: 0.9552 - val_accuracy: 0.6705\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.7819\n",
      "Epoch 00003: val_loss did not improve from 0.95522\n",
      "12/12 [==============================] - 109s 9s/step - loss: 0.6152 - accuracy: 0.7819 - val_loss: 0.9615 - val_accuracy: 0.6477\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.7649\n",
      "Epoch 00004: val_loss did not improve from 0.95522\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6083 - accuracy: 0.7649 - val_loss: 0.9688 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 11\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.7592\n",
      "Epoch 00001: val_loss improved from inf to 0.96948, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.6317 - accuracy: 0.7592 - val_loss: 0.9695 - val_accuracy: 0.6591\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.7932\n",
      "Epoch 00002: val_loss did not improve from 0.96948\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.5933 - accuracy: 0.7932 - val_loss: 0.9745 - val_accuracy: 0.6364\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.8017\n",
      "Epoch 00003: val_loss did not improve from 0.96948\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.5894 - accuracy: 0.8017 - val_loss: 0.9793 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 12\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6117 - accuracy: 0.7904\n",
      "Epoch 00001: val_loss improved from inf to 0.97138, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.6117 - accuracy: 0.7904 - val_loss: 0.9714 - val_accuracy: 0.6591\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5769 - accuracy: 0.8045 \n",
      "Epoch 00002: val_loss did not improve from 0.97138\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.5769 - accuracy: 0.8045 - val_loss: 0.9918 - val_accuracy: 0.6364\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.7875\n",
      "Epoch 00003: val_loss did not improve from 0.97138\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.5691 - accuracy: 0.7875 - val_loss: 0.9910 - val_accuracy: 0.6477\n",
      "-------Iteration FT: 13\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.7762 \n",
      "Epoch 00001: val_loss improved from inf to 0.97467, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 0.6181 - accuracy: 0.7762 - val_loss: 0.9747 - val_accuracy: 0.6591\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.7904\n",
      "Epoch 00002: val_loss did not improve from 0.97467\n",
      "12/12 [==============================] - 122s 10s/step - loss: 0.5856 - accuracy: 0.7904 - val_loss: 0.9840 - val_accuracy: 0.6364\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.7790 \n",
      "Epoch 00003: val_loss did not improve from 0.97467\n",
      "12/12 [==============================] - 122s 10s/step - loss: 0.5832 - accuracy: 0.7790 - val_loss: 0.9909 - val_accuracy: 0.6477\n",
      "-------Iteration FT: 14\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5963 - accuracy: 0.7790\n",
      "Epoch 00001: val_loss improved from inf to 0.97727, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5963 - accuracy: 0.7790 - val_loss: 0.9773 - val_accuracy: 0.6477\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.7960 \n",
      "Epoch 00002: val_loss did not improve from 0.97727\n",
      "12/12 [==============================] - 192s 16s/step - loss: 0.5979 - accuracy: 0.7960 - val_loss: 0.9923 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.8357 \n",
      "Epoch 00003: val_loss did not improve from 0.97727\n",
      "12/12 [==============================] - 140s 12s/step - loss: 0.5563 - accuracy: 0.8357 - val_loss: 0.9962 - val_accuracy: 0.6591\n",
      "-------Iteration FT: 15\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5871 - accuracy: 0.8045 \n",
      "Epoch 00001: val_loss improved from inf to 0.98242, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 0.5871 - accuracy: 0.8045 - val_loss: 0.9824 - val_accuracy: 0.6477\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5600 - accuracy: 0.7932 \n",
      "Epoch 00002: val_loss did not improve from 0.98242\n",
      "12/12 [==============================] - 131s 11s/step - loss: 0.5600 - accuracy: 0.7932 - val_loss: 0.9911 - val_accuracy: 0.6364\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.8215 \n",
      "Epoch 00003: val_loss did not improve from 0.98242\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5541 - accuracy: 0.8215 - val_loss: 0.9888 - val_accuracy: 0.6591\n",
      "-------Iteration FT: 16\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7989 \n",
      "Epoch 00001: val_loss improved from inf to 0.98781, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 0.5783 - accuracy: 0.7989 - val_loss: 0.9878 - val_accuracy: 0.6477\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.7762 \n",
      "Epoch 00002: val_loss did not improve from 0.98781\n",
      "12/12 [==============================] - 131s 11s/step - loss: 0.5644 - accuracy: 0.7762 - val_loss: 1.0048 - val_accuracy: 0.6364\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.8045 \n",
      "Epoch 00003: val_loss did not improve from 0.98781\n",
      "12/12 [==============================] - 128s 11s/step - loss: 0.5572 - accuracy: 0.8045 - val_loss: 1.0086 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 17\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.8017 \n",
      "Epoch 00001: val_loss improved from inf to 1.01154, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 0.5741 - accuracy: 0.8017 - val_loss: 1.0115 - val_accuracy: 0.6591\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.7847 \n",
      "Epoch 00002: val_loss improved from 1.01154 to 1.00311, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 0.5551 - accuracy: 0.7847 - val_loss: 1.0031 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5179 - accuracy: 0.8357\n",
      "Epoch 00003: val_loss improved from 1.00311 to 0.99607, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.5179 - accuracy: 0.8357 - val_loss: 0.9961 - val_accuracy: 0.6364\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.8272 \n",
      "Epoch 00004: val_loss did not improve from 0.99607\n",
      "12/12 [==============================] - 122s 10s/step - loss: 0.5306 - accuracy: 0.8272 - val_loss: 1.0034 - val_accuracy: 0.6136\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5145 - accuracy: 0.8130 \n",
      "Epoch 00005: val_loss did not improve from 0.99607\n",
      "12/12 [==============================] - 142s 12s/step - loss: 0.5145 - accuracy: 0.8130 - val_loss: 1.0169 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 18\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5377 - accuracy: 0.8159 \n",
      "Epoch 00001: val_loss improved from inf to 1.00473, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 0.5377 - accuracy: 0.8159 - val_loss: 1.0047 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5351 - accuracy: 0.8130 \n",
      "Epoch 00002: val_loss did not improve from 1.00473\n",
      "12/12 [==============================] - 165s 14s/step - loss: 0.5351 - accuracy: 0.8130 - val_loss: 1.0307 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8754 \n",
      "Epoch 00003: val_loss did not improve from 1.00473\n",
      "12/12 [==============================] - 132s 11s/step - loss: 0.4830 - accuracy: 0.8754 - val_loss: 1.0349 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 19\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5383 - accuracy: 0.8159 \n",
      "Epoch 00001: val_loss improved from inf to 1.02090, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 0.5383 - accuracy: 0.8159 - val_loss: 1.0209 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.8215 \n",
      "Epoch 00002: val_loss improved from 1.02090 to 1.02023, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 0.5041 - accuracy: 0.8215 - val_loss: 1.0202 - val_accuracy: 0.5795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.8187 \n",
      "Epoch 00003: val_loss did not improve from 1.02023\n",
      "12/12 [==============================] - 128s 11s/step - loss: 0.4931 - accuracy: 0.8187 - val_loss: 1.0328 - val_accuracy: 0.5909\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4687 - accuracy: 0.8555 \n",
      "Epoch 00004: val_loss did not improve from 1.02023\n",
      "12/12 [==============================] - 132s 11s/step - loss: 0.4687 - accuracy: 0.8555 - val_loss: 1.0409 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 20\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.8074 \n",
      "Epoch 00001: val_loss improved from inf to 1.04292, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 0.5374 - accuracy: 0.8074 - val_loss: 1.0429 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.8470 \n",
      "Epoch 00002: val_loss improved from 1.04292 to 1.04071, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 0.4843 - accuracy: 0.8470 - val_loss: 1.0407 - val_accuracy: 0.6250\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4808 - accuracy: 0.8385 \n",
      "Epoch 00003: val_loss did not improve from 1.04071\n",
      "12/12 [==============================] - 143s 12s/step - loss: 0.4808 - accuracy: 0.8385 - val_loss: 1.0536 - val_accuracy: 0.6023\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4633 - accuracy: 0.8527 \n",
      "Epoch 00004: val_loss did not improve from 1.04071\n",
      "12/12 [==============================] - 131s 11s/step - loss: 0.4633 - accuracy: 0.8527 - val_loss: 1.0548 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 21\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.8215 \n",
      "Epoch 00001: val_loss improved from inf to 1.05344, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 141s 12s/step - loss: 0.5038 - accuracy: 0.8215 - val_loss: 1.0534 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.8584 \n",
      "Epoch 00002: val_loss did not improve from 1.05344\n",
      "12/12 [==============================] - 130s 11s/step - loss: 0.4574 - accuracy: 0.8584 - val_loss: 1.0758 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.8499 \n",
      "Epoch 00003: val_loss did not improve from 1.05344\n",
      "12/12 [==============================] - 134s 11s/step - loss: 0.4628 - accuracy: 0.8499 - val_loss: 1.0864 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 22\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4759 - accuracy: 0.8385 \n",
      "Epoch 00001: val_loss improved from inf to 1.04641, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 0.4759 - accuracy: 0.8385 - val_loss: 1.0464 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.8470 \n",
      "Epoch 00002: val_loss did not improve from 1.04641\n",
      "12/12 [==============================] - 138s 11s/step - loss: 0.4700 - accuracy: 0.8470 - val_loss: 1.0728 - val_accuracy: 0.5795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.8470 \n",
      "Epoch 00003: val_loss did not improve from 1.04641\n",
      "12/12 [==============================] - 139s 11s/step - loss: 0.4720 - accuracy: 0.8470 - val_loss: 1.0729 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 23\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.8470 \n",
      "Epoch 00001: val_loss improved from inf to 1.05786, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 145s 12s/step - loss: 0.4559 - accuracy: 0.8470 - val_loss: 1.0579 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4874 - accuracy: 0.8329 \n",
      "Epoch 00002: val_loss did not improve from 1.05786\n",
      "12/12 [==============================] - 153s 13s/step - loss: 0.4874 - accuracy: 0.8329 - val_loss: 1.0809 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.8697 \n",
      "Epoch 00003: val_loss did not improve from 1.05786\n",
      "12/12 [==============================] - 145s 12s/step - loss: 0.4362 - accuracy: 0.8697 - val_loss: 1.0731 - val_accuracy: 0.6477\n",
      "-------Iteration FT: 24\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.8470 \n",
      "Epoch 00001: val_loss improved from inf to 1.09818, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 143s 12s/step - loss: 0.4603 - accuracy: 0.8470 - val_loss: 1.0982 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4721 - accuracy: 0.8442 \n",
      "Epoch 00002: val_loss did not improve from 1.09818\n",
      "12/12 [==============================] - 140s 12s/step - loss: 0.4721 - accuracy: 0.8442 - val_loss: 1.1079 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4365 - accuracy: 0.8640 \n",
      "Epoch 00003: val_loss did not improve from 1.09818\n",
      "12/12 [==============================] - 141s 12s/step - loss: 0.4365 - accuracy: 0.8640 - val_loss: 1.1152 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 25\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.8357 \n",
      "Epoch 00001: val_loss improved from inf to 1.07705, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 139s 12s/step - loss: 0.4613 - accuracy: 0.8357 - val_loss: 1.0770 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4468 - accuracy: 0.8669 \n",
      "Epoch 00002: val_loss did not improve from 1.07705\n",
      "12/12 [==============================] - 134s 11s/step - loss: 0.4468 - accuracy: 0.8669 - val_loss: 1.0918 - val_accuracy: 0.6477\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8725 \n",
      "Epoch 00003: val_loss did not improve from 1.07705\n",
      "12/12 [==============================] - 146s 12s/step - loss: 0.4202 - accuracy: 0.8725 - val_loss: 1.1138 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 26\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.8555     \n",
      "Epoch 00001: val_loss improved from inf to 1.10014, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 53987s 3600s/step - loss: 0.4489 - accuracy: 0.8555 - val_loss: 1.1001 - val_accuracy: 0.6477\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.8385 \n",
      "Epoch 00002: val_loss did not improve from 1.10014\n",
      "12/12 [==============================] - 128s 11s/step - loss: 0.4584 - accuracy: 0.8385 - val_loss: 1.1478 - val_accuracy: 0.5909\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.8754 \n",
      "Epoch 00003: val_loss did not improve from 1.10014\n",
      "12/12 [==============================] - 125s 10s/step - loss: 0.4184 - accuracy: 0.8754 - val_loss: 1.1113 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 27\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4455 - accuracy: 0.8499 \n",
      "Epoch 00001: val_loss improved from inf to 1.10295, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 0.4455 - accuracy: 0.8499 - val_loss: 1.1030 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.8669 \n",
      "Epoch 00002: val_loss did not improve from 1.10295\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.4207 - accuracy: 0.8669 - val_loss: 1.1323 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8782 \n",
      "Epoch 00003: val_loss did not improve from 1.10295\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.3974 - accuracy: 0.8782 - val_loss: 1.1495 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 28\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4399 - accuracy: 0.8584 \n",
      "Epoch 00001: val_loss improved from inf to 1.12883, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 0.4399 - accuracy: 0.8584 - val_loss: 1.1288 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.8782 \n",
      "Epoch 00002: val_loss did not improve from 1.12883\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.4211 - accuracy: 0.8782 - val_loss: 1.1492 - val_accuracy: 0.6364\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.8555 \n",
      "Epoch 00003: val_loss did not improve from 1.12883\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.4361 - accuracy: 0.8555 - val_loss: 1.1669 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 29\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4423 - accuracy: 0.8470 \n",
      "Epoch 00001: val_loss improved from inf to 1.11725, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 0.4423 - accuracy: 0.8470 - val_loss: 1.1173 - val_accuracy: 0.6705\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.8612 \n",
      "Epoch 00002: val_loss did not improve from 1.11725\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.4380 - accuracy: 0.8612 - val_loss: 1.1745 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.8669 \n",
      "Epoch 00003: val_loss did not improve from 1.11725\n",
      "12/12 [==============================] - 125s 10s/step - loss: 0.3931 - accuracy: 0.8669 - val_loss: 1.1695 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 30\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8725 \n",
      "Epoch 00001: val_loss improved from inf to 1.13181, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 231s 19s/step - loss: 0.4297 - accuracy: 0.8725 - val_loss: 1.1318 - val_accuracy: 0.6591\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4198 - accuracy: 0.8669 \n",
      "Epoch 00002: val_loss did not improve from 1.13181\n",
      "12/12 [==============================] - 255s 22s/step - loss: 0.4198 - accuracy: 0.8669 - val_loss: 1.1888 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8839 \n",
      "Epoch 00003: val_loss did not improve from 1.13181\n",
      "12/12 [==============================] - 183s 15s/step - loss: 0.3969 - accuracy: 0.8839 - val_loss: 1.1930 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 31\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8697 \n",
      "Epoch 00001: val_loss improved from inf to 1.17590, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 184s 15s/step - loss: 0.4107 - accuracy: 0.8697 - val_loss: 1.1759 - val_accuracy: 0.6477\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.8867 \n",
      "Epoch 00002: val_loss did not improve from 1.17590\n",
      "12/12 [==============================] - 188s 16s/step - loss: 0.3720 - accuracy: 0.8867 - val_loss: 1.1984 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3866 - accuracy: 0.8754 \n",
      "Epoch 00003: val_loss did not improve from 1.17590\n",
      "12/12 [==============================] - 230s 19s/step - loss: 0.3866 - accuracy: 0.8754 - val_loss: 1.1830 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 32\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.8754 \n",
      "Epoch 00001: val_loss improved from inf to 1.15867, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 203s 17s/step - loss: 0.4314 - accuracy: 0.8754 - val_loss: 1.1587 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.8782 \n",
      "Epoch 00002: val_loss did not improve from 1.15867\n",
      "12/12 [==============================] - 188s 16s/step - loss: 0.4001 - accuracy: 0.8782 - val_loss: 1.2245 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8895 \n",
      "Epoch 00003: val_loss did not improve from 1.15867\n",
      "12/12 [==============================] - 185s 15s/step - loss: 0.3825 - accuracy: 0.8895 - val_loss: 1.2042 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 33\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4164 - accuracy: 0.8584 \n",
      "Epoch 00001: val_loss improved from inf to 1.19431, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 203s 17s/step - loss: 0.4164 - accuracy: 0.8584 - val_loss: 1.1943 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8782 \n",
      "Epoch 00002: val_loss did not improve from 1.19431\n",
      "12/12 [==============================] - 191s 16s/step - loss: 0.4060 - accuracy: 0.8782 - val_loss: 1.2310 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.9008 \n",
      "Epoch 00003: val_loss did not improve from 1.19431\n",
      "12/12 [==============================] - 215s 18s/step - loss: 0.3769 - accuracy: 0.9008 - val_loss: 1.2287 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 34\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4037 - accuracy: 0.8782 \n",
      "Epoch 00001: val_loss improved from inf to 1.19179, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 243s 20s/step - loss: 0.4037 - accuracy: 0.8782 - val_loss: 1.1918 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.9037 \n",
      "Epoch 00002: val_loss did not improve from 1.19179\n",
      "12/12 [==============================] - 269s 22s/step - loss: 0.3622 - accuracy: 0.9037 - val_loss: 1.2356 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8952 \n",
      "Epoch 00003: val_loss did not improve from 1.19179\n",
      "12/12 [==============================] - 277s 23s/step - loss: 0.3715 - accuracy: 0.8952 - val_loss: 1.2402 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 35\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.8669 \n",
      "Epoch 00001: val_loss improved from inf to 1.22139, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 205s 17s/step - loss: 0.4203 - accuracy: 0.8669 - val_loss: 1.2214 - val_accuracy: 0.6477\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3962 - accuracy: 0.8867 \n",
      "Epoch 00002: val_loss did not improve from 1.22139\n",
      "12/12 [==============================] - 204s 17s/step - loss: 0.3962 - accuracy: 0.8867 - val_loss: 1.2643 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3585 - accuracy: 0.8867 \n",
      "Epoch 00003: val_loss did not improve from 1.22139\n",
      "12/12 [==============================] - 196s 16s/step - loss: 0.3585 - accuracy: 0.8867 - val_loss: 1.2699 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 36\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.8867 \n",
      "Epoch 00001: val_loss improved from inf to 1.26203, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 200s 17s/step - loss: 0.3940 - accuracy: 0.8867 - val_loss: 1.2620 - val_accuracy: 0.6364\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.9008 \n",
      "Epoch 00002: val_loss improved from 1.26203 to 1.25861, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 193s 16s/step - loss: 0.3486 - accuracy: 0.9008 - val_loss: 1.2586 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3699 - accuracy: 0.8839 \n",
      "Epoch 00003: val_loss did not improve from 1.25861\n",
      "12/12 [==============================] - 194s 16s/step - loss: 0.3699 - accuracy: 0.8839 - val_loss: 1.2851 - val_accuracy: 0.6023\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8782 \n",
      "Epoch 00004: val_loss did not improve from 1.25861\n",
      "12/12 [==============================] - 193s 16s/step - loss: 0.3410 - accuracy: 0.8782 - val_loss: 1.3139 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 37\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3708 - accuracy: 0.8612 \n",
      "Epoch 00001: val_loss improved from inf to 1.28886, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 195s 16s/step - loss: 0.3708 - accuracy: 0.8612 - val_loss: 1.2889 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.9008 \n",
      "Epoch 00002: val_loss did not improve from 1.28886\n",
      "12/12 [==============================] - 198s 16s/step - loss: 0.3726 - accuracy: 0.9008 - val_loss: 1.3330 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.9122 \n",
      "Epoch 00003: val_loss did not improve from 1.28886\n",
      "12/12 [==============================] - 195s 16s/step - loss: 0.3306 - accuracy: 0.9122 - val_loss: 1.3104 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 38\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.9093 \n",
      "Epoch 00001: val_loss improved from inf to 1.28718, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 197s 16s/step - loss: 0.3525 - accuracy: 0.9093 - val_loss: 1.2872 - val_accuracy: 0.6023\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3623 - accuracy: 0.8839 \n",
      "Epoch 00002: val_loss did not improve from 1.28718\n",
      "12/12 [==============================] - 190s 16s/step - loss: 0.3623 - accuracy: 0.8839 - val_loss: 1.3344 - val_accuracy: 0.6136\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.9093 \n",
      "Epoch 00003: val_loss did not improve from 1.28718\n",
      "12/12 [==============================] - 186s 15s/step - loss: 0.3173 - accuracy: 0.9093 - val_loss: 1.3377 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 39\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3651 - accuracy: 0.8839 \n",
      "Epoch 00001: val_loss improved from inf to 1.31290, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 174s 14s/step - loss: 0.3651 - accuracy: 0.8839 - val_loss: 1.3129 - val_accuracy: 0.5909\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.8952 \n",
      "Epoch 00002: val_loss did not improve from 1.31290\n",
      "12/12 [==============================] - 137s 11s/step - loss: 0.3351 - accuracy: 0.8952 - val_loss: 1.3534 - val_accuracy: 0.5795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3192 - accuracy: 0.9235 \n",
      "Epoch 00003: val_loss did not improve from 1.31290\n",
      "12/12 [==============================] - 155s 13s/step - loss: 0.3192 - accuracy: 0.9235 - val_loss: 1.3571 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 40\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3505 - accuracy: 0.8867 \n",
      "Epoch 00001: val_loss improved from inf to 1.37169, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 178s 15s/step - loss: 0.3505 - accuracy: 0.8867 - val_loss: 1.3717 - val_accuracy: 0.6023\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.8924 \n",
      "Epoch 00002: val_loss did not improve from 1.37169\n",
      "12/12 [==============================] - 173s 14s/step - loss: 0.3555 - accuracy: 0.8924 - val_loss: 1.4065 - val_accuracy: 0.5682\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.9065 \n",
      "Epoch 00003: val_loss improved from 1.37169 to 1.35871, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 174s 14s/step - loss: 0.3197 - accuracy: 0.9065 - val_loss: 1.3587 - val_accuracy: 0.6250\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.9207 \n",
      "Epoch 00004: val_loss did not improve from 1.35871\n",
      "12/12 [==============================] - 174s 14s/step - loss: 0.3120 - accuracy: 0.9207 - val_loss: 1.3607 - val_accuracy: 0.5795\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.9178 \n",
      "Epoch 00005: val_loss did not improve from 1.35871\n",
      "12/12 [==============================] - 176s 15s/step - loss: 0.3035 - accuracy: 0.9178 - val_loss: 1.3606 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 41\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9178 \n",
      "Epoch 00001: val_loss improved from inf to 1.32647, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 176s 15s/step - loss: 0.3281 - accuracy: 0.9178 - val_loss: 1.3265 - val_accuracy: 0.5909\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9065 \n",
      "Epoch 00002: val_loss did not improve from 1.32647\n",
      "12/12 [==============================] - 179s 15s/step - loss: 0.3008 - accuracy: 0.9065 - val_loss: 1.3398 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.9207 \n",
      "Epoch 00003: val_loss did not improve from 1.32647\n",
      "12/12 [==============================] - 177s 15s/step - loss: 0.3128 - accuracy: 0.9207 - val_loss: 1.4107 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 42\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.9178 \n",
      "Epoch 00001: val_loss improved from inf to 1.36053, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 176s 15s/step - loss: 0.3330 - accuracy: 0.9178 - val_loss: 1.3605 - val_accuracy: 0.5909\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2965 - accuracy: 0.9320 \n",
      "Epoch 00002: val_loss did not improve from 1.36053\n",
      "12/12 [==============================] - 173s 14s/step - loss: 0.2965 - accuracy: 0.9320 - val_loss: 1.4466 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.9122 \n",
      "Epoch 00003: val_loss improved from 1.36053 to 1.35920, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 176s 15s/step - loss: 0.3004 - accuracy: 0.9122 - val_loss: 1.3592 - val_accuracy: 0.6023\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9235 \n",
      "Epoch 00004: val_loss did not improve from 1.35920\n",
      "12/12 [==============================] - 173s 14s/step - loss: 0.2685 - accuracy: 0.9235 - val_loss: 1.3969 - val_accuracy: 0.6023\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.9405 \n",
      "Epoch 00005: val_loss did not improve from 1.35920\n",
      "12/12 [==============================] - 174s 14s/step - loss: 0.2760 - accuracy: 0.9405 - val_loss: 1.4025 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 43\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.9122 \n",
      "Epoch 00001: val_loss improved from inf to 1.37642, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 175s 15s/step - loss: 0.3057 - accuracy: 0.9122 - val_loss: 1.3764 - val_accuracy: 0.6023\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9348 \n",
      "Epoch 00002: val_loss did not improve from 1.37642\n",
      "12/12 [==============================] - 173s 14s/step - loss: 0.2839 - accuracy: 0.9348 - val_loss: 1.4858 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9263 \n",
      "Epoch 00003: val_loss did not improve from 1.37642\n",
      "12/12 [==============================] - 174s 14s/step - loss: 0.2757 - accuracy: 0.9263 - val_loss: 1.3984 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 44\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.9178 \n",
      "Epoch 00001: val_loss improved from inf to 1.37066, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 176s 15s/step - loss: 0.2997 - accuracy: 0.9178 - val_loss: 1.3707 - val_accuracy: 0.6136\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.9207 \n",
      "Epoch 00002: val_loss did not improve from 1.37066\n",
      "12/12 [==============================] - 184s 15s/step - loss: 0.2843 - accuracy: 0.9207 - val_loss: 1.5028 - val_accuracy: 0.5909\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9405 \n",
      "Epoch 00003: val_loss did not improve from 1.37066\n",
      "12/12 [==============================] - 181s 15s/step - loss: 0.2639 - accuracy: 0.9405 - val_loss: 1.4704 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 45\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.9235 \n",
      "Epoch 00001: val_loss improved from inf to 1.42143, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 173s 14s/step - loss: 0.2945 - accuracy: 0.9235 - val_loss: 1.4214 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2931 - accuracy: 0.9263 \n",
      "Epoch 00002: val_loss did not improve from 1.42143\n",
      "12/12 [==============================] - 171s 14s/step - loss: 0.2931 - accuracy: 0.9263 - val_loss: 1.5411 - val_accuracy: 0.5909\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9405 \n",
      "Epoch 00003: val_loss did not improve from 1.42143\n",
      "12/12 [==============================] - 196s 17s/step - loss: 0.2586 - accuracy: 0.9405 - val_loss: 1.4277 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 46\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.9093 \n",
      "Epoch 00001: val_loss improved from inf to 1.43909, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 202s 17s/step - loss: 0.2984 - accuracy: 0.9093 - val_loss: 1.4391 - val_accuracy: 0.6023\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2899 - accuracy: 0.9320 \n",
      "Epoch 00002: val_loss did not improve from 1.43909\n",
      "12/12 [==============================] - 211s 18s/step - loss: 0.2899 - accuracy: 0.9320 - val_loss: 1.5505 - val_accuracy: 0.5909\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9377 \n",
      "Epoch 00003: val_loss did not improve from 1.43909\n",
      "12/12 [==============================] - 152s 12s/step - loss: 0.2648 - accuracy: 0.9377 - val_loss: 1.5173 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 47\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3076 - accuracy: 0.9122\n",
      "Epoch 00001: val_loss improved from inf to 1.52828, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3076 - accuracy: 0.9122 - val_loss: 1.5283 - val_accuracy: 0.6023\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.9207\n",
      "Epoch 00002: val_loss did not improve from 1.52828\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2761 - accuracy: 0.9207 - val_loss: 1.5681 - val_accuracy: 0.5909\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9292\n",
      "Epoch 00003: val_loss did not improve from 1.52828\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.2514 - accuracy: 0.9292 - val_loss: 1.5619 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 48\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.9320\n",
      "Epoch 00001: val_loss improved from inf to 1.52004, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2742 - accuracy: 0.9320 - val_loss: 1.5200 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9433\n",
      "Epoch 00002: val_loss did not improve from 1.52004\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.2641 - accuracy: 0.9433 - val_loss: 1.5383 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.9292\n",
      "Epoch 00003: val_loss improved from 1.52004 to 1.51672, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2603 - accuracy: 0.9292 - val_loss: 1.5167 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9405\n",
      "Epoch 00004: val_loss did not improve from 1.51672\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2407 - accuracy: 0.9405 - val_loss: 1.5632 - val_accuracy: 0.6136\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.9405\n",
      "Epoch 00005: val_loss did not improve from 1.51672\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2379 - accuracy: 0.9405 - val_loss: 1.5365 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 49\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9462\n",
      "Epoch 00001: val_loss improved from inf to 1.59283, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2600 - accuracy: 0.9462 - val_loss: 1.5928 - val_accuracy: 0.6023\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.9518\n",
      "Epoch 00002: val_loss did not improve from 1.59283\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2375 - accuracy: 0.9518 - val_loss: 1.6345 - val_accuracy: 0.6023\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9462 \n",
      "Epoch 00003: val_loss improved from 1.59283 to 1.57620, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 0.2358 - accuracy: 0.9462 - val_loss: 1.5762 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.9405 \n",
      "Epoch 00004: val_loss did not improve from 1.57620\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.2251 - accuracy: 0.9405 - val_loss: 1.6725 - val_accuracy: 0.5795\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 0.9603 \n",
      "Epoch 00005: val_loss did not improve from 1.57620\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.2141 - accuracy: 0.9603 - val_loss: 1.5924 - val_accuracy: 0.6364\n",
      "Evaluating samm-019\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4207 - accuracy: 0.4674\n",
      "Epoch 00001: val_loss improved from inf to 2.18354, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 10s/step - loss: 2.4207 - accuracy: 0.4674 - val_loss: 2.1835 - val_accuracy: 0.5897\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1281 - accuracy: 0.5552 \n",
      "Epoch 00002: val_loss improved from 2.18354 to 1.96972, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 142s 12s/step - loss: 2.1281 - accuracy: 0.5552 - val_loss: 1.9697 - val_accuracy: 0.5897\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.9413 - accuracy: 0.5411 \n",
      "Epoch 00003: val_loss improved from 1.96972 to 1.80136, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 127s 10s/step - loss: 1.9413 - accuracy: 0.5411 - val_loss: 1.8014 - val_accuracy: 0.5897\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.7872 - accuracy: 0.5779 \n",
      "Epoch 00004: val_loss improved from 1.80136 to 1.64827, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 1.7872 - accuracy: 0.5779 - val_loss: 1.6483 - val_accuracy: 0.5897\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.5956 - accuracy: 0.5751 \n",
      "Epoch 00005: val_loss improved from 1.64827 to 1.50447, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 1.5956 - accuracy: 0.5751 - val_loss: 1.5045 - val_accuracy: 0.5897\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.4972 - accuracy: 0.5751 \n",
      "Epoch 00006: val_loss improved from 1.50447 to 1.39004, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 1.4972 - accuracy: 0.5751 - val_loss: 1.3900 - val_accuracy: 0.5897\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.3634 - accuracy: 0.5949 \n",
      "Epoch 00007: val_loss improved from 1.39004 to 1.29787, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 1.3634 - accuracy: 0.5949 - val_loss: 1.2979 - val_accuracy: 0.5897\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2729 - accuracy: 0.5892 \n",
      "Epoch 00008: val_loss improved from 1.29787 to 1.22788, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.2729 - accuracy: 0.5892 - val_loss: 1.2279 - val_accuracy: 0.5897\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2135 - accuracy: 0.5807\n",
      "Epoch 00009: val_loss improved from 1.22788 to 1.17141, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.2135 - accuracy: 0.5807 - val_loss: 1.1714 - val_accuracy: 0.5897\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1413 - accuracy: 0.6034\n",
      "Epoch 00010: val_loss improved from 1.17141 to 1.13114, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.1413 - accuracy: 0.6034 - val_loss: 1.1311 - val_accuracy: 0.6026\n",
      "-------Iteration Head: 1\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1361 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 1.10795, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.1361 - accuracy: 0.5807 - val_loss: 1.1080 - val_accuracy: 0.6026\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.1082 - accuracy: 0.5892\n",
      "Epoch 00002: val_loss improved from 1.10795 to 1.08027, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.1082 - accuracy: 0.5892 - val_loss: 1.0803 - val_accuracy: 0.5897\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0658 - accuracy: 0.5807 \n",
      "Epoch 00003: val_loss improved from 1.08027 to 1.06175, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0658 - accuracy: 0.5807 - val_loss: 1.0618 - val_accuracy: 0.5897\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.5864\n",
      "Epoch 00004: val_loss improved from 1.06175 to 1.04888, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0619 - accuracy: 0.5864 - val_loss: 1.0489 - val_accuracy: 0.5897\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0713 - accuracy: 0.5836\n",
      "Epoch 00005: val_loss improved from 1.04888 to 1.04234, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0713 - accuracy: 0.5836 - val_loss: 1.0423 - val_accuracy: 0.5897\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0415 - accuracy: 0.5864\n",
      "Epoch 00006: val_loss improved from 1.04234 to 1.03377, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0415 - accuracy: 0.5864 - val_loss: 1.0338 - val_accuracy: 0.5897\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0390 - accuracy: 0.5949 \n",
      "Epoch 00007: val_loss improved from 1.03377 to 1.02888, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.0390 - accuracy: 0.5949 - val_loss: 1.0289 - val_accuracy: 0.6026\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0352 - accuracy: 0.5892 \n",
      "Epoch 00008: val_loss improved from 1.02888 to 1.01796, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 1.0352 - accuracy: 0.5892 - val_loss: 1.0180 - val_accuracy: 0.6026\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0137 - accuracy: 0.5892 \n",
      "Epoch 00009: val_loss improved from 1.01796 to 1.01242, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0137 - accuracy: 0.5892 - val_loss: 1.0124 - val_accuracy: 0.5897\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0223 - accuracy: 0.5836 \n",
      "Epoch 00010: val_loss improved from 1.01242 to 1.00691, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 1.0223 - accuracy: 0.5836 - val_loss: 1.0069 - val_accuracy: 0.6026\n",
      "-------Iteration Head: 2\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0386 - accuracy: 0.5921\n",
      "Epoch 00001: val_loss improved from inf to 1.02111, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0386 - accuracy: 0.5921 - val_loss: 1.0211 - val_accuracy: 0.6026\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0208 - accuracy: 0.5949 \n",
      "Epoch 00002: val_loss improved from 1.02111 to 1.01375, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0208 - accuracy: 0.5949 - val_loss: 1.0138 - val_accuracy: 0.6026\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0191 - accuracy: 0.5892\n",
      "Epoch 00003: val_loss improved from 1.01375 to 1.00651, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0191 - accuracy: 0.5892 - val_loss: 1.0065 - val_accuracy: 0.6026\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0168 - accuracy: 0.5949\n",
      "Epoch 00004: val_loss improved from 1.00651 to 1.00477, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0168 - accuracy: 0.5949 - val_loss: 1.0048 - val_accuracy: 0.6026\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0003 - accuracy: 0.5779\n",
      "Epoch 00005: val_loss improved from 1.00477 to 1.00174, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0003 - accuracy: 0.5779 - val_loss: 1.0017 - val_accuracy: 0.6026\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0051 - accuracy: 0.5779\n",
      "Epoch 00006: val_loss improved from 1.00174 to 0.99466, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0051 - accuracy: 0.5779 - val_loss: 0.9947 - val_accuracy: 0.6026\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9850 - accuracy: 0.5807 \n",
      "Epoch 00007: val_loss improved from 0.99466 to 0.98962, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9850 - accuracy: 0.5807 - val_loss: 0.9896 - val_accuracy: 0.6026\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0015 - accuracy: 0.6006\n",
      "Epoch 00008: val_loss did not improve from 0.98962\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0015 - accuracy: 0.6006 - val_loss: 0.9908 - val_accuracy: 0.6026\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0021 - accuracy: 0.5892\n",
      "Epoch 00009: val_loss did not improve from 0.98962\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0021 - accuracy: 0.5892 - val_loss: 0.9935 - val_accuracy: 0.6026\n",
      "-------Iteration Head: 3\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0397 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 1.01453, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0397 - accuracy: 0.5751 - val_loss: 1.0145 - val_accuracy: 0.5897\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0356 - accuracy: 0.5779\n",
      "Epoch 00002: val_loss improved from 1.01453 to 1.00933, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0356 - accuracy: 0.5779 - val_loss: 1.0093 - val_accuracy: 0.6026\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0267 - accuracy: 0.5892 \n",
      "Epoch 00003: val_loss improved from 1.00933 to 1.00635, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.0267 - accuracy: 0.5892 - val_loss: 1.0063 - val_accuracy: 0.6026\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0062 - accuracy: 0.5779 \n",
      "Epoch 00004: val_loss improved from 1.00635 to 1.00434, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 1.0062 - accuracy: 0.5779 - val_loss: 1.0043 - val_accuracy: 0.6026\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9944 - accuracy: 0.5807 \n",
      "Epoch 00005: val_loss improved from 1.00434 to 0.99880, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9944 - accuracy: 0.5807 - val_loss: 0.9988 - val_accuracy: 0.6026\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0205 - accuracy: 0.5807\n",
      "Epoch 00006: val_loss improved from 0.99880 to 0.99707, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0205 - accuracy: 0.5807 - val_loss: 0.9971 - val_accuracy: 0.6026\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0125 - accuracy: 0.5836 \n",
      "Epoch 00007: val_loss improved from 0.99707 to 0.99531, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 1.0125 - accuracy: 0.5836 - val_loss: 0.9953 - val_accuracy: 0.5897\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0104 - accuracy: 0.5921\n",
      "Epoch 00008: val_loss improved from 0.99531 to 0.99161, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0104 - accuracy: 0.5921 - val_loss: 0.9916 - val_accuracy: 0.6026\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0144 - accuracy: 0.5722\n",
      "Epoch 00009: val_loss improved from 0.99161 to 0.99048, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0144 - accuracy: 0.5722 - val_loss: 0.9905 - val_accuracy: 0.6026\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0122 - accuracy: 0.5892\n",
      "Epoch 00010: val_loss improved from 0.99048 to 0.98796, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 1.0122 - accuracy: 0.5892 - val_loss: 0.9880 - val_accuracy: 0.6026\n",
      "-------Iteration Head: 4\n",
      "Learn the head\n",
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0211 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 1.00338, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 1.0211 - accuracy: 0.5836 - val_loss: 1.0034 - val_accuracy: 0.6026\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0002 - accuracy: 0.5779\n",
      "Epoch 00002: val_loss improved from 1.00338 to 0.99545, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0002 - accuracy: 0.5779 - val_loss: 0.9955 - val_accuracy: 0.5897\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0110 - accuracy: 0.5779\n",
      "Epoch 00003: val_loss improved from 0.99545 to 0.98964, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0110 - accuracy: 0.5779 - val_loss: 0.9896 - val_accuracy: 0.6026\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9915 - accuracy: 0.5949\n",
      "Epoch 00004: val_loss improved from 0.98964 to 0.98721, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9915 - accuracy: 0.5949 - val_loss: 0.9872 - val_accuracy: 0.6026\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.0027 - accuracy: 0.5807\n",
      "Epoch 00005: val_loss improved from 0.98721 to 0.98472, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 1.0027 - accuracy: 0.5807 - val_loss: 0.9847 - val_accuracy: 0.6026\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9952 - accuracy: 0.5921\n",
      "Epoch 00006: val_loss improved from 0.98472 to 0.98341, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-multIterBy5-0501202209.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9952 - accuracy: 0.5921 - val_loss: 0.9834 - val_accuracy: 0.6026\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9946 - accuracy: 0.5921\n",
      "Epoch 00007: val_loss did not improve from 0.98341\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9946 - accuracy: 0.5921 - val_loss: 0.9854 - val_accuracy: 0.6026\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9990 - accuracy: 0.5892\n",
      "Epoch 00008: val_loss did not improve from 0.98341\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9990 - accuracy: 0.5892 - val_loss: 0.9838 - val_accuracy: 0.6026\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9906 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 0.98283, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 126s 11s/step - loss: 0.9906 - accuracy: 0.5836 - val_loss: 0.9828 - val_accuracy: 0.6026\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9777 - accuracy: 0.5807 \n",
      "Epoch 00002: val_loss improved from 0.98283 to 0.97711, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 0.9777 - accuracy: 0.5807 - val_loss: 0.9771 - val_accuracy: 0.5897\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9662 - accuracy: 0.5864 \n",
      "Epoch 00003: val_loss improved from 0.97711 to 0.97063, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 127s 10s/step - loss: 0.9662 - accuracy: 0.5864 - val_loss: 0.9706 - val_accuracy: 0.5897\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9600 - accuracy: 0.6091\n",
      "Epoch 00004: val_loss improved from 0.97063 to 0.96550, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9600 - accuracy: 0.6091 - val_loss: 0.9655 - val_accuracy: 0.5897\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9531 - accuracy: 0.6006\n",
      "Epoch 00005: val_loss improved from 0.96550 to 0.96424, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9531 - accuracy: 0.6006 - val_loss: 0.9642 - val_accuracy: 0.5897\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9443 - accuracy: 0.6119\n",
      "Epoch 00006: val_loss improved from 0.96424 to 0.96263, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.9443 - accuracy: 0.6119 - val_loss: 0.9626 - val_accuracy: 0.5897\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9242 - accuracy: 0.6261\n",
      "Epoch 00007: val_loss improved from 0.96263 to 0.95883, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.9242 - accuracy: 0.6261 - val_loss: 0.9588 - val_accuracy: 0.6026\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9265 - accuracy: 0.6119\n",
      "Epoch 00008: val_loss improved from 0.95883 to 0.95474, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9265 - accuracy: 0.6119 - val_loss: 0.9547 - val_accuracy: 0.6026\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8944 - accuracy: 0.6204 \n",
      "Epoch 00009: val_loss improved from 0.95474 to 0.95207, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.8944 - accuracy: 0.6204 - val_loss: 0.9521 - val_accuracy: 0.6026\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.9044 - accuracy: 0.6317 \n",
      "Epoch 00010: val_loss did not improve from 0.95207\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.9044 - accuracy: 0.6317 - val_loss: 0.9523 - val_accuracy: 0.6026\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8883 - accuracy: 0.6289 \n",
      "Epoch 00011: val_loss improved from 0.95207 to 0.94779, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.8883 - accuracy: 0.6289 - val_loss: 0.9478 - val_accuracy: 0.6154\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8852 - accuracy: 0.6431\n",
      "Epoch 00012: val_loss improved from 0.94779 to 0.94439, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.8852 - accuracy: 0.6431 - val_loss: 0.9444 - val_accuracy: 0.6282\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.6657 \n",
      "Epoch 00013: val_loss did not improve from 0.94439\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.8765 - accuracy: 0.6657 - val_loss: 0.9446 - val_accuracy: 0.6282\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8630 - accuracy: 0.6459 \n",
      "Epoch 00014: val_loss improved from 0.94439 to 0.94110, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 0.8630 - accuracy: 0.6459 - val_loss: 0.9411 - val_accuracy: 0.6282\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8462 - accuracy: 0.6601\n",
      "Epoch 00015: val_loss improved from 0.94110 to 0.93192, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.8462 - accuracy: 0.6601 - val_loss: 0.9319 - val_accuracy: 0.6282\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8351 - accuracy: 0.6657 \n",
      "Epoch 00016: val_loss improved from 0.93192 to 0.92872, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.8351 - accuracy: 0.6657 - val_loss: 0.9287 - val_accuracy: 0.6282\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8412 - accuracy: 0.6686\n",
      "Epoch 00017: val_loss did not improve from 0.92872\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.8412 - accuracy: 0.6686 - val_loss: 0.9292 - val_accuracy: 0.6282\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8419 - accuracy: 0.6799 \n",
      "Epoch 00018: val_loss improved from 0.92872 to 0.92558, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.8419 - accuracy: 0.6799 - val_loss: 0.9256 - val_accuracy: 0.6282\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8069 - accuracy: 0.6799 \n",
      "Epoch 00019: val_loss improved from 0.92558 to 0.92102, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.8069 - accuracy: 0.6799 - val_loss: 0.9210 - val_accuracy: 0.6410\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8160 - accuracy: 0.6827 \n",
      "Epoch 00020: val_loss improved from 0.92102 to 0.91820, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.8160 - accuracy: 0.6827 - val_loss: 0.9182 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.8090 - accuracy: 0.6799 \n",
      "Epoch 00001: val_loss improved from inf to 0.92023, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.8090 - accuracy: 0.6799 - val_loss: 0.9202 - val_accuracy: 0.6410\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7887 - accuracy: 0.6997 \n",
      "Epoch 00002: val_loss improved from 0.92023 to 0.91196, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.7887 - accuracy: 0.6997 - val_loss: 0.9120 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7901 - accuracy: 0.6742 \n",
      "Epoch 00003: val_loss improved from 0.91196 to 0.90599, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 0.7901 - accuracy: 0.6742 - val_loss: 0.9060 - val_accuracy: 0.6410\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7637 - accuracy: 0.7082 \n",
      "Epoch 00004: val_loss improved from 0.90599 to 0.90112, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 0.7637 - accuracy: 0.7082 - val_loss: 0.9011 - val_accuracy: 0.6410\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7506 - accuracy: 0.7082 \n",
      "Epoch 00005: val_loss improved from 0.90112 to 0.89991, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 0.7506 - accuracy: 0.7082 - val_loss: 0.8999 - val_accuracy: 0.6410\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7638 - accuracy: 0.7082 \n",
      "Epoch 00006: val_loss did not improve from 0.89991\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.7638 - accuracy: 0.7082 - val_loss: 0.9003 - val_accuracy: 0.6410\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7426 - accuracy: 0.7110 \n",
      "Epoch 00007: val_loss improved from 0.89991 to 0.89147, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.7426 - accuracy: 0.7110 - val_loss: 0.8915 - val_accuracy: 0.6410\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7312 - accuracy: 0.7054 \n",
      "Epoch 00008: val_loss improved from 0.89147 to 0.88765, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.7312 - accuracy: 0.7054 - val_loss: 0.8876 - val_accuracy: 0.6410\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7279 - accuracy: 0.7139 \n",
      "Epoch 00009: val_loss improved from 0.88765 to 0.88321, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.7279 - accuracy: 0.7139 - val_loss: 0.8832 - val_accuracy: 0.6538\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7248 - accuracy: 0.6912 \n",
      "Epoch 00010: val_loss did not improve from 0.88321\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.7248 - accuracy: 0.6912 - val_loss: 0.8903 - val_accuracy: 0.6410\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.7195 \n",
      "Epoch 00011: val_loss improved from 0.88321 to 0.88230, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.7063 - accuracy: 0.7195 - val_loss: 0.8823 - val_accuracy: 0.6538\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.7082 \n",
      "Epoch 00012: val_loss improved from 0.88230 to 0.86871, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.7185 - accuracy: 0.7082 - val_loss: 0.8687 - val_accuracy: 0.6410\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.7071 - accuracy: 0.7224 \n",
      "Epoch 00013: val_loss did not improve from 0.86871\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.7071 - accuracy: 0.7224 - val_loss: 0.8858 - val_accuracy: 0.6538\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.7422 \n",
      "Epoch 00014: val_loss did not improve from 0.86871\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.6695 - accuracy: 0.7422 - val_loss: 0.8708 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.7479\n",
      "Epoch 00001: val_loss improved from inf to 0.87771, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.6938 - accuracy: 0.7479 - val_loss: 0.8777 - val_accuracy: 0.6410\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6625 - accuracy: 0.7450 \n",
      "Epoch 00002: val_loss did not improve from 0.87771\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.6625 - accuracy: 0.7450 - val_loss: 0.8825 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.7564 \n",
      "Epoch 00003: val_loss improved from 0.87771 to 0.87670, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.6776 - accuracy: 0.7564 - val_loss: 0.8767 - val_accuracy: 0.6282\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6610 - accuracy: 0.7365 \n",
      "Epoch 00004: val_loss improved from 0.87670 to 0.87247, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.6610 - accuracy: 0.7365 - val_loss: 0.8725 - val_accuracy: 0.6282\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.7535 \n",
      "Epoch 00005: val_loss did not improve from 0.87247\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.6540 - accuracy: 0.7535 - val_loss: 0.8743 - val_accuracy: 0.6538\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6548 - accuracy: 0.7337 \n",
      "Epoch 00006: val_loss improved from 0.87247 to 0.87077, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.6548 - accuracy: 0.7337 - val_loss: 0.8708 - val_accuracy: 0.6282\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6441 - accuracy: 0.7620 \n",
      "Epoch 00007: val_loss did not improve from 0.87077\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.6441 - accuracy: 0.7620 - val_loss: 0.8764 - val_accuracy: 0.6282\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6215 - accuracy: 0.7790 \n",
      "Epoch 00008: val_loss improved from 0.87077 to 0.86818, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.6215 - accuracy: 0.7790 - val_loss: 0.8682 - val_accuracy: 0.6154\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.7705 \n",
      "Epoch 00009: val_loss did not improve from 0.86818\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.6150 - accuracy: 0.7705 - val_loss: 0.8785 - val_accuracy: 0.6410\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.7762 \n",
      "Epoch 00010: val_loss improved from 0.86818 to 0.86139, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.6061 - accuracy: 0.7762 - val_loss: 0.8614 - val_accuracy: 0.6282\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.7875 \n",
      "Epoch 00011: val_loss improved from 0.86139 to 0.86057, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.6083 - accuracy: 0.7875 - val_loss: 0.8606 - val_accuracy: 0.6410\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6031 - accuracy: 0.7790 \n",
      "Epoch 00012: val_loss improved from 0.86057 to 0.85612, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.6031 - accuracy: 0.7790 - val_loss: 0.8561 - val_accuracy: 0.6538\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.7875 \n",
      "Epoch 00013: val_loss did not improve from 0.85612\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.5739 - accuracy: 0.7875 - val_loss: 0.8650 - val_accuracy: 0.6538\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.8045 \n",
      "Epoch 00014: val_loss did not improve from 0.85612\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.5673 - accuracy: 0.8045 - val_loss: 0.8595 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.7762 \n",
      "Epoch 00001: val_loss improved from inf to 0.85699, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5804 - accuracy: 0.7762 - val_loss: 0.8570 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5535 - accuracy: 0.8017 \n",
      "Epoch 00002: val_loss did not improve from 0.85699\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5535 - accuracy: 0.8017 - val_loss: 0.8720 - val_accuracy: 0.6282\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.7960 \n",
      "Epoch 00003: val_loss did not improve from 0.85699\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5437 - accuracy: 0.7960 - val_loss: 0.8651 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.7762 \n",
      "Epoch 00001: val_loss improved from inf to 0.87138, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.5731 - accuracy: 0.7762 - val_loss: 0.8714 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5565 - accuracy: 0.8045 \n",
      "Epoch 00002: val_loss did not improve from 0.87138\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5565 - accuracy: 0.8045 - val_loss: 0.8752 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.8074 \n",
      "Epoch 00003: val_loss did not improve from 0.87138\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5537 - accuracy: 0.8074 - val_loss: 0.8851 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 5\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.7847 \n",
      "Epoch 00001: val_loss improved from inf to 0.87693, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.5717 - accuracy: 0.7847 - val_loss: 0.8769 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5324 - accuracy: 0.8045 \n",
      "Epoch 00002: val_loss did not improve from 0.87693\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5324 - accuracy: 0.8045 - val_loss: 0.8779 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.8300 \n",
      "Epoch 00003: val_loss did not improve from 0.87693\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5427 - accuracy: 0.8300 - val_loss: 0.8924 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 6\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.7932 \n",
      "Epoch 00001: val_loss improved from inf to 0.90374, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5613 - accuracy: 0.7932 - val_loss: 0.9037 - val_accuracy: 0.6026\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.8414 \n",
      "Epoch 00002: val_loss improved from 0.90374 to 0.88536, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.5272 - accuracy: 0.8414 - val_loss: 0.8854 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.8159 \n",
      "Epoch 00003: val_loss did not improve from 0.88536\n",
      "12/12 [==============================] - 126s 10s/step - loss: 0.5322 - accuracy: 0.8159 - val_loss: 0.8925 - val_accuracy: 0.6154\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.8187 \n",
      "Epoch 00004: val_loss did not improve from 0.88536\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5202 - accuracy: 0.8187 - val_loss: 0.9228 - val_accuracy: 0.6795\n",
      "-------Iteration FT: 7\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.7989 \n",
      "Epoch 00001: val_loss improved from inf to 0.88833, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5386 - accuracy: 0.7989 - val_loss: 0.8883 - val_accuracy: 0.6026\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.7989 \n",
      "Epoch 00002: val_loss did not improve from 0.88833\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5050 - accuracy: 0.7989 - val_loss: 0.9093 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.8074 \n",
      "Epoch 00003: val_loss did not improve from 0.88833\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5337 - accuracy: 0.8074 - val_loss: 0.9003 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 8\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5229 - accuracy: 0.8159 \n",
      "Epoch 00001: val_loss improved from inf to 0.88957, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5229 - accuracy: 0.8159 - val_loss: 0.8896 - val_accuracy: 0.6026\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.8470 \n",
      "Epoch 00002: val_loss did not improve from 0.88957\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5141 - accuracy: 0.8470 - val_loss: 0.9268 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4983 - accuracy: 0.8414 \n",
      "Epoch 00003: val_loss did not improve from 0.88957\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.4983 - accuracy: 0.8414 - val_loss: 0.8982 - val_accuracy: 0.6154\n",
      "-------Iteration FT: 9\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.8385 \n",
      "Epoch 00001: val_loss improved from inf to 0.88983, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5159 - accuracy: 0.8385 - val_loss: 0.8898 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5115 - accuracy: 0.8159 \n",
      "Epoch 00002: val_loss did not improve from 0.88983\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5115 - accuracy: 0.8159 - val_loss: 0.9215 - val_accuracy: 0.6026\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.8244 \n",
      "Epoch 00003: val_loss did not improve from 0.88983\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.4991 - accuracy: 0.8244 - val_loss: 0.9184 - val_accuracy: 0.6282\n",
      "-------Iteration FT: 10\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4986 - accuracy: 0.8300 \n",
      "Epoch 00001: val_loss improved from inf to 0.91220, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4986 - accuracy: 0.8300 - val_loss: 0.9122 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.8244 \n",
      "Epoch 00002: val_loss did not improve from 0.91220\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.5039 - accuracy: 0.8244 - val_loss: 0.9285 - val_accuracy: 0.6154\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4876 - accuracy: 0.8612 \n",
      "Epoch 00003: val_loss did not improve from 0.91220\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4876 - accuracy: 0.8612 - val_loss: 0.9238 - val_accuracy: 0.6154\n",
      "-------Iteration FT: 11\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.8385 \n",
      "Epoch 00001: val_loss improved from inf to 0.91544, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.5082 - accuracy: 0.8385 - val_loss: 0.9154 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4731 - accuracy: 0.8385 \n",
      "Epoch 00002: val_loss did not improve from 0.91544\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4731 - accuracy: 0.8385 - val_loss: 0.9230 - val_accuracy: 0.6282\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.8300 \n",
      "Epoch 00003: val_loss did not improve from 0.91544\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4970 - accuracy: 0.8300 - val_loss: 0.9599 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 12\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8187 \n",
      "Epoch 00001: val_loss improved from inf to 0.93849, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.5023 - accuracy: 0.8187 - val_loss: 0.9385 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.8272 \n",
      "Epoch 00002: val_loss improved from 0.93849 to 0.92994, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4792 - accuracy: 0.8272 - val_loss: 0.9299 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4769 - accuracy: 0.8414 \n",
      "Epoch 00003: val_loss did not improve from 0.92994\n",
      "12/12 [==============================] - 123s 10s/step - loss: 0.4769 - accuracy: 0.8414 - val_loss: 0.9431 - val_accuracy: 0.6410\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4361 - accuracy: 0.8725 \n",
      "Epoch 00004: val_loss did not improve from 0.92994\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.4361 - accuracy: 0.8725 - val_loss: 0.9577 - val_accuracy: 0.6282\n",
      "-------Iteration FT: 13\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.8640 \n",
      "Epoch 00001: val_loss improved from inf to 0.94306, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.4660 - accuracy: 0.8640 - val_loss: 0.9431 - val_accuracy: 0.6154\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4511 - accuracy: 0.8470 \n",
      "Epoch 00002: val_loss did not improve from 0.94306\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4511 - accuracy: 0.8470 - val_loss: 0.9685 - val_accuracy: 0.6154\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.8697 \n",
      "Epoch 00003: val_loss did not improve from 0.94306\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4427 - accuracy: 0.8697 - val_loss: 0.9843 - val_accuracy: 0.6154\n",
      "-------Iteration FT: 14\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4565 - accuracy: 0.8640 \n",
      "Epoch 00001: val_loss improved from inf to 0.97743, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4565 - accuracy: 0.8640 - val_loss: 0.9774 - val_accuracy: 0.6026\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.8640 \n",
      "Epoch 00002: val_loss improved from 0.97743 to 0.96254, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.4477 - accuracy: 0.8640 - val_loss: 0.9625 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.8612 \n",
      "Epoch 00003: val_loss did not improve from 0.96254\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4343 - accuracy: 0.8612 - val_loss: 0.9645 - val_accuracy: 0.6282\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8640 \n",
      "Epoch 00004: val_loss did not improve from 0.96254\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4284 - accuracy: 0.8640 - val_loss: 0.9657 - val_accuracy: 0.6282\n",
      "-------Iteration FT: 15\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4486 - accuracy: 0.8442 \n",
      "Epoch 00001: val_loss improved from inf to 0.94304, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.4486 - accuracy: 0.8442 - val_loss: 0.9430 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.8867 \n",
      "Epoch 00002: val_loss did not improve from 0.94304\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.4221 - accuracy: 0.8867 - val_loss: 0.9702 - val_accuracy: 0.6282\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.8782 \n",
      "Epoch 00003: val_loss did not improve from 0.94304\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4211 - accuracy: 0.8782 - val_loss: 0.9672 - val_accuracy: 0.6282\n",
      "-------Iteration FT: 16\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.8555 \n",
      "Epoch 00001: val_loss improved from inf to 0.96673, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4563 - accuracy: 0.8555 - val_loss: 0.9667 - val_accuracy: 0.6026\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8640 \n",
      "Epoch 00002: val_loss did not improve from 0.96673\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.4287 - accuracy: 0.8640 - val_loss: 0.9677 - val_accuracy: 0.6026\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8867 \n",
      "Epoch 00003: val_loss did not improve from 0.96673\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.4131 - accuracy: 0.8867 - val_loss: 0.9848 - val_accuracy: 0.6282\n",
      "-------Iteration FT: 17\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.8584 \n",
      "Epoch 00001: val_loss improved from inf to 0.97295, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.4413 - accuracy: 0.8584 - val_loss: 0.9729 - val_accuracy: 0.6410\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.8924 \n",
      "Epoch 00002: val_loss improved from 0.97295 to 0.96824, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.3975 - accuracy: 0.8924 - val_loss: 0.9682 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8839 \n",
      "Epoch 00003: val_loss did not improve from 0.96824\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.4206 - accuracy: 0.8839 - val_loss: 1.0070 - val_accuracy: 0.6538\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3966 - accuracy: 0.8839 \n",
      "Epoch 00004: val_loss did not improve from 0.96824\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3966 - accuracy: 0.8839 - val_loss: 1.0079 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 18\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.8640 \n",
      "Epoch 00001: val_loss improved from inf to 1.01683, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.4400 - accuracy: 0.8640 - val_loss: 1.0168 - val_accuracy: 0.6154\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8895\n",
      "Epoch 00002: val_loss improved from 1.01683 to 1.00943, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3869 - accuracy: 0.8895 - val_loss: 1.0094 - val_accuracy: 0.6410\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.9008 \n",
      "Epoch 00003: val_loss improved from 1.00943 to 0.99556, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.3792 - accuracy: 0.9008 - val_loss: 0.9956 - val_accuracy: 0.6154\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.9065 \n",
      "Epoch 00004: val_loss did not improve from 0.99556\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.3632 - accuracy: 0.9065 - val_loss: 1.0140 - val_accuracy: 0.6282\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3755 - accuracy: 0.8867 \n",
      "Epoch 00005: val_loss did not improve from 0.99556\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.3755 - accuracy: 0.8867 - val_loss: 1.0344 - val_accuracy: 0.6282\n",
      "-------Iteration FT: 19\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3750 - accuracy: 0.8952 \n",
      "Epoch 00001: val_loss improved from inf to 1.04641, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.3750 - accuracy: 0.8952 - val_loss: 1.0464 - val_accuracy: 0.6154\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.9122 \n",
      "Epoch 00002: val_loss improved from 1.04641 to 1.02671, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.3553 - accuracy: 0.9122 - val_loss: 1.0267 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3704 - accuracy: 0.9008\n",
      "Epoch 00003: val_loss did not improve from 1.02671\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3704 - accuracy: 0.9008 - val_loss: 1.0739 - val_accuracy: 0.6282\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3748 - accuracy: 0.8867 \n",
      "Epoch 00004: val_loss did not improve from 1.02671\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.3748 - accuracy: 0.8867 - val_loss: 1.0801 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 20\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8924 \n",
      "Epoch 00001: val_loss improved from inf to 1.00338, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.3777 - accuracy: 0.8924 - val_loss: 1.0034 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8895 \n",
      "Epoch 00002: val_loss did not improve from 1.00338\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3855 - accuracy: 0.8895 - val_loss: 1.0392 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3439 - accuracy: 0.9122 \n",
      "Epoch 00003: val_loss did not improve from 1.00338\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.3439 - accuracy: 0.9122 - val_loss: 1.1412 - val_accuracy: 0.6795\n",
      "-------Iteration FT: 21\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.8952 \n",
      "Epoch 00001: val_loss improved from inf to 1.04185, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.3674 - accuracy: 0.8952 - val_loss: 1.0419 - val_accuracy: 0.6410\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.9122\n",
      "Epoch 00002: val_loss did not improve from 1.04185\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3506 - accuracy: 0.9122 - val_loss: 1.0543 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8895 \n",
      "Epoch 00003: val_loss did not improve from 1.04185\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.3387 - accuracy: 0.8895 - val_loss: 1.0982 - val_accuracy: 0.6410\n",
      "-------Iteration FT: 22\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3769 - accuracy: 0.8952\n",
      "Epoch 00001: val_loss improved from inf to 1.05048, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3769 - accuracy: 0.8952 - val_loss: 1.0505 - val_accuracy: 0.6410\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.9008 \n",
      "Epoch 00002: val_loss did not improve from 1.05048\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3523 - accuracy: 0.9008 - val_loss: 1.1026 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.9093\n",
      "Epoch 00003: val_loss did not improve from 1.05048\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3261 - accuracy: 0.9093 - val_loss: 1.0964 - val_accuracy: 0.6282\n",
      "-------Iteration FT: 23\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3591 - accuracy: 0.8980\n",
      "Epoch 00001: val_loss improved from inf to 1.07548, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3591 - accuracy: 0.8980 - val_loss: 1.0755 - val_accuracy: 0.6538\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.9263\n",
      "Epoch 00002: val_loss did not improve from 1.07548\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3305 - accuracy: 0.9263 - val_loss: 1.1394 - val_accuracy: 0.6795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.9093\n",
      "Epoch 00003: val_loss did not improve from 1.07548\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3256 - accuracy: 0.9093 - val_loss: 1.1562 - val_accuracy: 0.6538\n",
      "-------Iteration FT: 24\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3494 - accuracy: 0.8895\n",
      "Epoch 00001: val_loss improved from inf to 1.17203, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3494 - accuracy: 0.8895 - val_loss: 1.1720 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8980\n",
      "Epoch 00002: val_loss improved from 1.17203 to 1.10682, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3454 - accuracy: 0.8980 - val_loss: 1.1068 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.9093\n",
      "Epoch 00003: val_loss did not improve from 1.10682\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3259 - accuracy: 0.9093 - val_loss: 1.1094 - val_accuracy: 0.6538\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.9292\n",
      "Epoch 00004: val_loss improved from 1.10682 to 1.09885, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2993 - accuracy: 0.9292 - val_loss: 1.0989 - val_accuracy: 0.6795\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.9122\n",
      "Epoch 00005: val_loss did not improve from 1.09885\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3172 - accuracy: 0.9122 - val_loss: 1.1674 - val_accuracy: 0.6538\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.9235 \n",
      "Epoch 00006: val_loss did not improve from 1.09885\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2937 - accuracy: 0.9235 - val_loss: 1.1152 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 25\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.9207\n",
      "Epoch 00001: val_loss improved from inf to 1.12015, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3238 - accuracy: 0.9207 - val_loss: 1.1201 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.9178\n",
      "Epoch 00002: val_loss did not improve from 1.12015\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.3136 - accuracy: 0.9178 - val_loss: 1.1585 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.9292\n",
      "Epoch 00003: val_loss did not improve from 1.12015\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2944 - accuracy: 0.9292 - val_loss: 1.1402 - val_accuracy: 0.6538\n",
      "-------Iteration FT: 26\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.9207\n",
      "Epoch 00001: val_loss improved from inf to 1.12499, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3062 - accuracy: 0.9207 - val_loss: 1.1250 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.9207\n",
      "Epoch 00002: val_loss did not improve from 1.12499\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3021 - accuracy: 0.9207 - val_loss: 1.1877 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2915 - accuracy: 0.9178\n",
      "Epoch 00003: val_loss did not improve from 1.12499\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2915 - accuracy: 0.9178 - val_loss: 1.1883 - val_accuracy: 0.6538\n",
      "-------Iteration FT: 27\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.9093\n",
      "Epoch 00001: val_loss improved from inf to 1.18703, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.3146 - accuracy: 0.9093 - val_loss: 1.1870 - val_accuracy: 0.6410\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.9178\n",
      "Epoch 00002: val_loss did not improve from 1.18703\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.2943 - accuracy: 0.9178 - val_loss: 1.1872 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2814 - accuracy: 0.9292\n",
      "Epoch 00003: val_loss improved from 1.18703 to 1.16779, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2814 - accuracy: 0.9292 - val_loss: 1.1678 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9235\n",
      "Epoch 00004: val_loss did not improve from 1.16779\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2879 - accuracy: 0.9235 - val_loss: 1.1874 - val_accuracy: 0.6795\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9462\n",
      "Epoch 00005: val_loss did not improve from 1.16779\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.2649 - accuracy: 0.9462 - val_loss: 1.2432 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 28\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9320\n",
      "Epoch 00001: val_loss improved from inf to 1.22647, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2656 - accuracy: 0.9320 - val_loss: 1.2265 - val_accuracy: 0.6538\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9320\n",
      "Epoch 00002: val_loss improved from 1.22647 to 1.20605, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.2751 - accuracy: 0.9320 - val_loss: 1.2061 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9292\n",
      "Epoch 00003: val_loss did not improve from 1.20605\n",
      "12/12 [==============================] - 110s 9s/step - loss: 0.2712 - accuracy: 0.9292 - val_loss: 1.2699 - val_accuracy: 0.6795\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9263\n",
      "Epoch 00004: val_loss did not improve from 1.20605\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2657 - accuracy: 0.9263 - val_loss: 1.3052 - val_accuracy: 0.6538\n",
      "-------Iteration FT: 29\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9292\n",
      "Epoch 00001: val_loss improved from inf to 1.21679, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.2686 - accuracy: 0.9292 - val_loss: 1.2168 - val_accuracy: 0.6410\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.9320\n",
      "Epoch 00002: val_loss did not improve from 1.21679\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2670 - accuracy: 0.9320 - val_loss: 1.2468 - val_accuracy: 0.6795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9377\n",
      "Epoch 00003: val_loss did not improve from 1.21679\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2606 - accuracy: 0.9377 - val_loss: 1.2963 - val_accuracy: 0.6795\n",
      "-------Iteration FT: 30\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.9178\n",
      "Epoch 00001: val_loss improved from inf to 1.23346, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.2746 - accuracy: 0.9178 - val_loss: 1.2335 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9292\n",
      "Epoch 00002: val_loss improved from 1.23346 to 1.23011, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.2641 - accuracy: 0.9292 - val_loss: 1.2301 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.9263\n",
      "Epoch 00003: val_loss did not improve from 1.23011\n",
      "12/12 [==============================] - 113s 9s/step - loss: 0.2660 - accuracy: 0.9263 - val_loss: 1.3447 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9547\n",
      "Epoch 00004: val_loss did not improve from 1.23011\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.2349 - accuracy: 0.9547 - val_loss: 1.2901 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 31\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.9377\n",
      "Epoch 00001: val_loss improved from inf to 1.30253, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.2507 - accuracy: 0.9377 - val_loss: 1.3025 - val_accuracy: 0.6538\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.9405\n",
      "Epoch 00002: val_loss did not improve from 1.30253\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.2352 - accuracy: 0.9405 - val_loss: 1.3530 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9490\n",
      "Epoch 00003: val_loss did not improve from 1.30253\n",
      "12/12 [==============================] - 114s 10s/step - loss: 0.2173 - accuracy: 0.9490 - val_loss: 1.3625 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 32\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.9263\n",
      "Epoch 00001: val_loss improved from inf to 1.30011, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2561 - accuracy: 0.9263 - val_loss: 1.3001 - val_accuracy: 0.6538\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9377\n",
      "Epoch 00002: val_loss did not improve from 1.30011\n",
      "12/12 [==============================] - 114s 9s/step - loss: 0.2571 - accuracy: 0.9377 - val_loss: 1.3783 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9462\n",
      "Epoch 00003: val_loss did not improve from 1.30011\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.2300 - accuracy: 0.9462 - val_loss: 1.3932 - val_accuracy: 0.6923\n",
      "-------Iteration FT: 33\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9377\n",
      "Epoch 00001: val_loss improved from inf to 1.34109, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2600 - accuracy: 0.9377 - val_loss: 1.3411 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9433\n",
      "Epoch 00002: val_loss did not improve from 1.34109\n",
      "12/12 [==============================] - 115s 10s/step - loss: 0.2363 - accuracy: 0.9433 - val_loss: 1.4780 - val_accuracy: 0.6795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9348\n",
      "Epoch 00003: val_loss improved from 1.34109 to 1.29979, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.2450 - accuracy: 0.9348 - val_loss: 1.2998 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9518\n",
      "Epoch 00004: val_loss did not improve from 1.29979\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.2339 - accuracy: 0.9518 - val_loss: 1.3794 - val_accuracy: 0.6795\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9688\n",
      "Epoch 00005: val_loss did not improve from 1.29979\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.2053 - accuracy: 0.9688 - val_loss: 1.4705 - val_accuracy: 0.6795\n",
      "-------Iteration FT: 34\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9263\n",
      "Epoch 00001: val_loss improved from inf to 1.38681, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2543 - accuracy: 0.9263 - val_loss: 1.3868 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9433\n",
      "Epoch 00002: val_loss did not improve from 1.38681\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.2415 - accuracy: 0.9433 - val_loss: 1.3910 - val_accuracy: 0.6795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9688\n",
      "Epoch 00003: val_loss did not improve from 1.38681\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.2029 - accuracy: 0.9688 - val_loss: 1.4443 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 35\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9433\n",
      "Epoch 00001: val_loss improved from inf to 1.39250, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2312 - accuracy: 0.9433 - val_loss: 1.3925 - val_accuracy: 0.6282\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9745\n",
      "Epoch 00002: val_loss did not improve from 1.39250\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.1933 - accuracy: 0.9745 - val_loss: 1.4738 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9575\n",
      "Epoch 00003: val_loss did not improve from 1.39250\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.2147 - accuracy: 0.9575 - val_loss: 1.4389 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 36\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9433\n",
      "Epoch 00001: val_loss improved from inf to 1.41133, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2333 - accuracy: 0.9433 - val_loss: 1.4113 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9518\n",
      "Epoch 00002: val_loss did not improve from 1.41133\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2091 - accuracy: 0.9518 - val_loss: 1.4630 - val_accuracy: 0.6795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9462\n",
      "Epoch 00003: val_loss did not improve from 1.41133\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.2041 - accuracy: 0.9462 - val_loss: 1.4418 - val_accuracy: 0.6923\n",
      "-------Iteration FT: 37\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9575\n",
      "Epoch 00001: val_loss improved from inf to 1.43033, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2167 - accuracy: 0.9575 - val_loss: 1.4303 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9575\n",
      "Epoch 00002: val_loss did not improve from 1.43033\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.2024 - accuracy: 0.9575 - val_loss: 1.5053 - val_accuracy: 0.7051\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1993 - accuracy: 0.9688\n",
      "Epoch 00003: val_loss improved from 1.43033 to 1.42666, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.1993 - accuracy: 0.9688 - val_loss: 1.4267 - val_accuracy: 0.6923\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9688\n",
      "Epoch 00004: val_loss did not improve from 1.42666\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1854 - accuracy: 0.9688 - val_loss: 1.4777 - val_accuracy: 0.6795\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9688\n",
      "Epoch 00005: val_loss did not improve from 1.42666\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1820 - accuracy: 0.9688 - val_loss: 1.4951 - val_accuracy: 0.7051\n",
      "-------Iteration FT: 38\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2129 - accuracy: 0.9377\n",
      "Epoch 00001: val_loss improved from inf to 1.52385, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.2129 - accuracy: 0.9377 - val_loss: 1.5239 - val_accuracy: 0.7051\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9603\n",
      "Epoch 00002: val_loss improved from 1.52385 to 1.48968, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1981 - accuracy: 0.9603 - val_loss: 1.4897 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9575\n",
      "Epoch 00003: val_loss did not improve from 1.48968\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1905 - accuracy: 0.9575 - val_loss: 1.5156 - val_accuracy: 0.6923\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9717\n",
      "Epoch 00004: val_loss did not improve from 1.48968\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1896 - accuracy: 0.9717 - val_loss: 1.6073 - val_accuracy: 0.6795\n",
      "-------Iteration FT: 39\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9660\n",
      "Epoch 00001: val_loss improved from inf to 1.60952, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.1889 - accuracy: 0.9660 - val_loss: 1.6095 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9660\n",
      "Epoch 00002: val_loss did not improve from 1.60952\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1771 - accuracy: 0.9660 - val_loss: 1.6623 - val_accuracy: 0.6795\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9688\n",
      "Epoch 00003: val_loss improved from 1.60952 to 1.54323, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.1754 - accuracy: 0.9688 - val_loss: 1.5432 - val_accuracy: 0.6795\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1693 - accuracy: 0.9745\n",
      "Epoch 00004: val_loss did not improve from 1.54323\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1693 - accuracy: 0.9745 - val_loss: 1.5790 - val_accuracy: 0.6923\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9745\n",
      "Epoch 00005: val_loss did not improve from 1.54323\n",
      "12/12 [==============================] - 111s 9s/step - loss: 0.1627 - accuracy: 0.9745 - val_loss: 1.6138 - val_accuracy: 0.6795\n",
      "-------Iteration FT: 40\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9660 \n",
      "Epoch 00001: val_loss improved from inf to 1.75489, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 0.1774 - accuracy: 0.9660 - val_loss: 1.7549 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9462\n",
      "Epoch 00002: val_loss improved from 1.75489 to 1.57243, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.1897 - accuracy: 0.9462 - val_loss: 1.5724 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9717\n",
      "Epoch 00003: val_loss did not improve from 1.57243\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1608 - accuracy: 0.9717 - val_loss: 1.6297 - val_accuracy: 0.7179\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9773\n",
      "Epoch 00004: val_loss did not improve from 1.57243\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1499 - accuracy: 0.9773 - val_loss: 1.6423 - val_accuracy: 0.6795\n",
      "-------Iteration FT: 41\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9632\n",
      "Epoch 00001: val_loss improved from inf to 1.59824, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.1647 - accuracy: 0.9632 - val_loss: 1.5982 - val_accuracy: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9830 \n",
      "Epoch 00002: val_loss did not improve from 1.59824\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.1525 - accuracy: 0.9830 - val_loss: 1.7278 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9830 \n",
      "Epoch 00003: val_loss did not improve from 1.59824\n",
      "12/12 [==============================] - 118s 10s/step - loss: 0.1480 - accuracy: 0.9830 - val_loss: 1.6899 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 42\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9660 \n",
      "Epoch 00001: val_loss improved from inf to 1.73354, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.1684 - accuracy: 0.9660 - val_loss: 1.7335 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9518\n",
      "Epoch 00002: val_loss improved from 1.73354 to 1.59672, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1862 - accuracy: 0.9518 - val_loss: 1.5967 - val_accuracy: 0.6667\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9773\n",
      "Epoch 00003: val_loss did not improve from 1.59672\n",
      "12/12 [==============================] - 112s 9s/step - loss: 0.1538 - accuracy: 0.9773 - val_loss: 1.6528 - val_accuracy: 0.6795\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9858\n",
      "Epoch 00004: val_loss did not improve from 1.59672\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.1466 - accuracy: 0.9858 - val_loss: 1.6782 - val_accuracy: 0.6923\n",
      "-------Iteration FT: 43\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9688\n",
      "Epoch 00001: val_loss improved from inf to 1.61018, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1755 - accuracy: 0.9688 - val_loss: 1.6102 - val_accuracy: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9490\n",
      "Epoch 00002: val_loss did not improve from 1.61018\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.1972 - accuracy: 0.9490 - val_loss: 1.6603 - val_accuracy: 0.6538\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1612 - accuracy: 0.9773\n",
      "Epoch 00003: val_loss did not improve from 1.61018\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1612 - accuracy: 0.9773 - val_loss: 1.7886 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 44\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9660 \n",
      "Epoch 00001: val_loss improved from inf to 1.79534, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.1788 - accuracy: 0.9660 - val_loss: 1.7953 - val_accuracy: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.9717 \n",
      "Epoch 00002: val_loss improved from 1.79534 to 1.60550, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 0.1709 - accuracy: 0.9717 - val_loss: 1.6055 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9830\n",
      "Epoch 00003: val_loss did not improve from 1.60550\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.1455 - accuracy: 0.9830 - val_loss: 1.7296 - val_accuracy: 0.6923\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9802\n",
      "Epoch 00004: val_loss did not improve from 1.60550\n",
      "12/12 [==============================] - 116s 10s/step - loss: 0.1390 - accuracy: 0.9802 - val_loss: 1.8213 - val_accuracy: 0.6923\n",
      "-------Iteration FT: 45\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9802\n",
      "Epoch 00001: val_loss improved from inf to 1.76145, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1354 - accuracy: 0.9802 - val_loss: 1.7615 - val_accuracy: 0.6795\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1493 - accuracy: 0.9773\n",
      "Epoch 00002: val_loss improved from 1.76145 to 1.73901, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 0.1493 - accuracy: 0.9773 - val_loss: 1.7390 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9830 \n",
      "Epoch 00003: val_loss did not improve from 1.73901\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.1370 - accuracy: 0.9830 - val_loss: 1.8362 - val_accuracy: 0.7051\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9887 \n",
      "Epoch 00004: val_loss improved from 1.73901 to 1.69411, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.1446 - accuracy: 0.9887 - val_loss: 1.6941 - val_accuracy: 0.6923\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9887 \n",
      "Epoch 00005: val_loss did not improve from 1.69411\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.1354 - accuracy: 0.9887 - val_loss: 1.8011 - val_accuracy: 0.6795\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9858 \n",
      "Epoch 00006: val_loss did not improve from 1.69411\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.1289 - accuracy: 0.9858 - val_loss: 1.8019 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 46\n",
      "Fine-tuning whole network\n",
      "Epoch 1/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1588 - accuracy: 0.9717 \n",
      "Epoch 00001: val_loss improved from inf to 1.80581, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 0.1588 - accuracy: 0.9717 - val_loss: 1.8058 - val_accuracy: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9915 \n",
      "Epoch 00002: val_loss improved from 1.80581 to 1.69111, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-multIterBy5-0501202209_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 0.1245 - accuracy: 0.9915 - val_loss: 1.6911 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      " 9/12 [=====================>........] - ETA: 30s - loss: 0.1259 - accuracy: 0.9896"
     ]
    }
   ],
   "source": [
    "#multiply no of runs by 5\n",
    "%run 3classBaseExperiment --headIter 5 --ftIter 50 --expName multIterBy5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(noOfHeadIter=1, noOfFtIter=5, noOfHeadEpoch=1, noOfFtEpoch=1, outp='output', expName='base1epoch')\n",
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 224, 224, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 224, 224, 3)  7           ['rescaling[0][0]']              \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',     \n",
      "                                                                  'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4035 - accuracy: 0.4816\n",
      "Epoch 00001: val_loss improved from inf to 2.12785, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-head-base1epoch-0801202214.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 2.4035 - accuracy: 0.4816 - val_loss: 2.1278 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2343 - accuracy: 0.5184\n",
      "Epoch 00001: val_loss improved from inf to 2.12802, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 2.2343 - accuracy: 0.5184 - val_loss: 2.1280 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1664 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.12347, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1664 - accuracy: 0.5722 - val_loss: 2.1235 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1963 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.11180, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1963 - accuracy: 0.5496 - val_loss: 2.1118 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1627 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.09965, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 130s 10s/step - loss: 2.1627 - accuracy: 0.5411 - val_loss: 2.0997 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1386 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.09904, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 140s 11s/step - loss: 2.1386 - accuracy: 0.5552 - val_loss: 2.0990 - val_accuracy: 0.6235\n",
      "Evaluating samm-010\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3607 - accuracy: 0.5099 \n",
      "Epoch 00001: val_loss improved from inf to 2.22352, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 132s 11s/step - loss: 2.3607 - accuracy: 0.5099 - val_loss: 2.2235 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2078 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.17606, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 2.2078 - accuracy: 0.5382 - val_loss: 2.1761 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1593 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.16335, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 127s 10s/step - loss: 2.1593 - accuracy: 0.5552 - val_loss: 2.1634 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1504 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.15155, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 145s 12s/step - loss: 2.1504 - accuracy: 0.5722 - val_loss: 2.1515 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0934 - accuracy: 0.6006 \n",
      "Epoch 00001: val_loss improved from inf to 2.13588, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 158s 13s/step - loss: 2.0934 - accuracy: 0.6006 - val_loss: 2.1359 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0941 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.13620, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 145s 12s/step - loss: 2.0941 - accuracy: 0.5807 - val_loss: 2.1362 - val_accuracy: 0.6000\n",
      "Evaluating smic-s15\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4054 - accuracy: 0.4986\n",
      "Epoch 00001: val_loss improved from inf to 2.16941, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.4054 - accuracy: 0.4986 - val_loss: 2.1694 - val_accuracy: 0.5595\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1850 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.15291, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1850 - accuracy: 0.5694 - val_loss: 2.1529 - val_accuracy: 0.5595\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2138 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.13181, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.2138 - accuracy: 0.5439 - val_loss: 2.1318 - val_accuracy: 0.5595\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1660 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.11397, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.1660 - accuracy: 0.5637 - val_loss: 2.1140 - val_accuracy: 0.5595\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1604 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.09911, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1604 - accuracy: 0.5892 - val_loss: 2.0991 - val_accuracy: 0.5833\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1449 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.08874, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 106s 9s/step - loss: 2.1449 - accuracy: 0.5609 - val_loss: 2.0887 - val_accuracy: 0.5595\n",
      "Evaluating samm-022\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3594 - accuracy: 0.5042\n",
      "Epoch 00001: val_loss improved from inf to 2.09070, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 107s 9s/step - loss: 2.3594 - accuracy: 0.5042 - val_loss: 2.0907 - val_accuracy: 0.6420\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2345 - accuracy: 0.5127\n",
      "Epoch 00001: val_loss improved from inf to 2.09856, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 2.2345 - accuracy: 0.5127 - val_loss: 2.0986 - val_accuracy: 0.6420\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1732 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.07361, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.1732 - accuracy: 0.5439 - val_loss: 2.0736 - val_accuracy: 0.6420\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1613 - accuracy: 0.5212\n",
      "Epoch 00001: val_loss improved from inf to 2.07235, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.1613 - accuracy: 0.5212 - val_loss: 2.0724 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1449 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.05244, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.1449 - accuracy: 0.5581 - val_loss: 2.0524 - val_accuracy: 0.6543\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0996 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.04037, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 106s 9s/step - loss: 2.0996 - accuracy: 0.5722 - val_loss: 2.0404 - val_accuracy: 0.6667\n",
      "Evaluating samm-035\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3901 - accuracy: 0.5071\n",
      "Epoch 00001: val_loss improved from inf to 2.08256, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 105s 9s/step - loss: 2.3901 - accuracy: 0.5071 - val_loss: 2.0826 - val_accuracy: 0.6296\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2074 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.06587, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.2074 - accuracy: 0.5609 - val_loss: 2.0659 - val_accuracy: 0.6296\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1871 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.04643, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.1871 - accuracy: 0.5269 - val_loss: 2.0464 - val_accuracy: 0.6296\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1533 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.02993, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.1533 - accuracy: 0.5609 - val_loss: 2.0299 - val_accuracy: 0.6296\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1529 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.01834, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 107s 9s/step - loss: 2.1529 - accuracy: 0.5609 - val_loss: 2.0183 - val_accuracy: 0.6667\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1166 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.00224, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 108s 9s/step - loss: 2.1166 - accuracy: 0.5694 - val_loss: 2.0022 - val_accuracy: 0.6420\n",
      "Evaluating samm-007\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3641 - accuracy: 0.4844\n",
      "Epoch 00001: val_loss improved from inf to 2.15227, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 107s 9s/step - loss: 2.3641 - accuracy: 0.4844 - val_loss: 2.1523 - val_accuracy: 0.5795\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1971 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.12472, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1971 - accuracy: 0.5581 - val_loss: 2.1247 - val_accuracy: 0.5795\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1603 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.10936, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1603 - accuracy: 0.5666 - val_loss: 2.1094 - val_accuracy: 0.5795\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1521 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.10386, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1521 - accuracy: 0.5552 - val_loss: 2.1039 - val_accuracy: 0.5795\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1130 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.09858, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1130 - accuracy: 0.5666 - val_loss: 2.0986 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0944 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.08766, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0944 - accuracy: 0.5751 - val_loss: 2.0877 - val_accuracy: 0.5795\n",
      "Evaluating samm-024\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3986 - accuracy: 0.4731\n",
      "Epoch 00001: val_loss improved from inf to 2.00109, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 110s 9s/step - loss: 2.3986 - accuracy: 0.4731 - val_loss: 2.0011 - val_accuracy: 0.6962\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1979 - accuracy: 0.5297\n",
      "Epoch 00001: val_loss improved from inf to 2.03060, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1979 - accuracy: 0.5297 - val_loss: 2.0306 - val_accuracy: 0.6962\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1727 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.00082, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1727 - accuracy: 0.5524 - val_loss: 2.0008 - val_accuracy: 0.6962\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1444 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 1.99919, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1444 - accuracy: 0.5581 - val_loss: 1.9992 - val_accuracy: 0.7089\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1477 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 1.99526, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1477 - accuracy: 0.5411 - val_loss: 1.9953 - val_accuracy: 0.7089\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1333 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 1.98232, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1333 - accuracy: 0.5779 - val_loss: 1.9823 - val_accuracy: 0.7089\n",
      "Evaluating smic-s14\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4018 - accuracy: 0.4816\n",
      "Epoch 00001: val_loss improved from inf to 2.14500, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.4018 - accuracy: 0.4816 - val_loss: 2.1450 - val_accuracy: 0.5875\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1933 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.12147, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1933 - accuracy: 0.5552 - val_loss: 2.1215 - val_accuracy: 0.5875\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2056 - accuracy: 0.5326\n",
      "Epoch 00001: val_loss improved from inf to 2.11091, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.2056 - accuracy: 0.5326 - val_loss: 2.1109 - val_accuracy: 0.5875\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1660 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.10138, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1660 - accuracy: 0.5666 - val_loss: 2.1014 - val_accuracy: 0.5875\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1293 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.09364, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1293 - accuracy: 0.5892 - val_loss: 2.0936 - val_accuracy: 0.5875\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1257 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.08422, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1257 - accuracy: 0.5892 - val_loss: 2.0842 - val_accuracy: 0.5875\n",
      "Evaluating smic-s12\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3940 - accuracy: 0.5042\n",
      "Epoch 00001: val_loss improved from inf to 2.29600, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3940 - accuracy: 0.5042 - val_loss: 2.2960 - val_accuracy: 0.5294\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1910 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.23969, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1910 - accuracy: 0.5609 - val_loss: 2.2397 - val_accuracy: 0.5294\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1460 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.20986, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1460 - accuracy: 0.5694 - val_loss: 2.2099 - val_accuracy: 0.5412\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1392 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.19826, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1392 - accuracy: 0.5694 - val_loss: 2.1983 - val_accuracy: 0.5412\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1273 - accuracy: 0.5921\n",
      "Epoch 00001: val_loss improved from inf to 2.18511, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1273 - accuracy: 0.5921 - val_loss: 2.1851 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1049 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.17319, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1049 - accuracy: 0.5864 - val_loss: 2.1732 - val_accuracy: 0.5529\n",
      "Evaluating casme2-sub06\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3736 - accuracy: 0.5071\n",
      "Epoch 00001: val_loss improved from inf to 2.04962, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3736 - accuracy: 0.5071 - val_loss: 2.0496 - val_accuracy: 0.6322\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2005 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.04733, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.2005 - accuracy: 0.5269 - val_loss: 2.0473 - val_accuracy: 0.6322\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1951 - accuracy: 0.5354\n",
      "Epoch 00001: val_loss improved from inf to 2.02560, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1951 - accuracy: 0.5354 - val_loss: 2.0256 - val_accuracy: 0.6437\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1549 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.01565, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1549 - accuracy: 0.5694 - val_loss: 2.0157 - val_accuracy: 0.6552\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1307 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.00639, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1307 - accuracy: 0.5609 - val_loss: 2.0064 - val_accuracy: 0.6437\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1393 - accuracy: 0.5467\n",
      "Epoch 00001: val_loss improved from inf to 1.99503, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1393 - accuracy: 0.5467 - val_loss: 1.9950 - val_accuracy: 0.6667\n",
      "Evaluating casme2-sub20\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3928 - accuracy: 0.4674\n",
      "Epoch 00001: val_loss improved from inf to 2.13828, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3928 - accuracy: 0.4674 - val_loss: 2.1383 - val_accuracy: 0.5977\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2462 - accuracy: 0.5382\n",
      "Epoch 00001: val_loss improved from inf to 2.12032, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.2462 - accuracy: 0.5382 - val_loss: 2.1203 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1681 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.10188, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1681 - accuracy: 0.5411 - val_loss: 2.1019 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1619 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.09375, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1619 - accuracy: 0.5269 - val_loss: 2.0938 - val_accuracy: 0.5977\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1360 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.08421, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1360 - accuracy: 0.5439 - val_loss: 2.0842 - val_accuracy: 0.5977\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1161 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.06737, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1161 - accuracy: 0.5609 - val_loss: 2.0674 - val_accuracy: 0.6322\n",
      "Evaluating smic-s19\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3776 - accuracy: 0.4986\n",
      "Epoch 00001: val_loss improved from inf to 2.18205, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3776 - accuracy: 0.4986 - val_loss: 2.1821 - val_accuracy: 0.5465\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1804 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.11426, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1804 - accuracy: 0.5439 - val_loss: 2.1143 - val_accuracy: 0.5465\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1491 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.10320, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1491 - accuracy: 0.5722 - val_loss: 2.1032 - val_accuracy: 0.5465\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1127 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.08381, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1127 - accuracy: 0.5609 - val_loss: 2.0838 - val_accuracy: 0.5465\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1083 - accuracy: 0.6062\n",
      "Epoch 00001: val_loss improved from inf to 2.06890, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1083 - accuracy: 0.6062 - val_loss: 2.0689 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0905 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 2.05652, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0905 - accuracy: 0.5807 - val_loss: 2.0565 - val_accuracy: 0.5581\n",
      "Evaluating samm-028\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3823 - accuracy: 0.4533\n",
      "Epoch 00001: val_loss improved from inf to 2.02429, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3823 - accuracy: 0.4533 - val_loss: 2.0243 - val_accuracy: 0.6744\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1996 - accuracy: 0.5326\n",
      "Epoch 00001: val_loss improved from inf to 2.03783, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1996 - accuracy: 0.5326 - val_loss: 2.0378 - val_accuracy: 0.6744\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1910 - accuracy: 0.5184\n",
      "Epoch 00001: val_loss improved from inf to 2.01352, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1910 - accuracy: 0.5184 - val_loss: 2.0135 - val_accuracy: 0.6860\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1661 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.00143, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1661 - accuracy: 0.5269 - val_loss: 2.0014 - val_accuracy: 0.6977\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1259 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 1.98276, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1259 - accuracy: 0.5751 - val_loss: 1.9828 - val_accuracy: 0.6977\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1062 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 1.97804, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1062 - accuracy: 0.5694 - val_loss: 1.9780 - val_accuracy: 0.6977\n",
      "Evaluating casme2-sub14\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3455 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.12375, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3455 - accuracy: 0.5269 - val_loss: 2.1237 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1849 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.08889, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1849 - accuracy: 0.5524 - val_loss: 2.0889 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1316 - accuracy: 0.5467\n",
      "Epoch 00001: val_loss improved from inf to 2.08421, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1316 - accuracy: 0.5467 - val_loss: 2.0842 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1238 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.07840, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1238 - accuracy: 0.5666 - val_loss: 2.0784 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0833 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 2.07278, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0833 - accuracy: 0.5807 - val_loss: 2.0728 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0971 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.06915, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.0971 - accuracy: 0.5722 - val_loss: 2.0692 - val_accuracy: 0.6023\n",
      "Evaluating casme2-sub21\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3799 - accuracy: 0.4844\n",
      "Epoch 00001: val_loss improved from inf to 2.26015, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3799 - accuracy: 0.4844 - val_loss: 2.2601 - val_accuracy: 0.5125\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2032 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.19099, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.2032 - accuracy: 0.5439 - val_loss: 2.1910 - val_accuracy: 0.5125\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1579 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.16689, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1579 - accuracy: 0.5722 - val_loss: 2.1669 - val_accuracy: 0.5125\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1505 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.15226, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1505 - accuracy: 0.5581 - val_loss: 2.1523 - val_accuracy: 0.5250\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1245 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.13613, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1245 - accuracy: 0.5637 - val_loss: 2.1361 - val_accuracy: 0.5375\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1170 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.12579, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1170 - accuracy: 0.5836 - val_loss: 2.1258 - val_accuracy: 0.5375\n",
      "Evaluating samm-026\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3347 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.37159, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3347 - accuracy: 0.5524 - val_loss: 2.3716 - val_accuracy: 0.4773\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2161 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.21595, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.2161 - accuracy: 0.5722 - val_loss: 2.2159 - val_accuracy: 0.4773\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1400 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.20018, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1400 - accuracy: 0.5524 - val_loss: 2.2002 - val_accuracy: 0.4773\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1283 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.21902, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1283 - accuracy: 0.5722 - val_loss: 2.2190 - val_accuracy: 0.4773\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0769 - accuracy: 0.6204\n",
      "Epoch 00001: val_loss improved from inf to 2.21042, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0769 - accuracy: 0.6204 - val_loss: 2.2104 - val_accuracy: 0.4773\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0829 - accuracy: 0.6147\n",
      "Epoch 00001: val_loss improved from inf to 2.18885, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0829 - accuracy: 0.6147 - val_loss: 2.1888 - val_accuracy: 0.4886\n",
      "Evaluating samm-031\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3924 - accuracy: 0.4674\n",
      "Epoch 00001: val_loss improved from inf to 2.16527, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3924 - accuracy: 0.4674 - val_loss: 2.1653 - val_accuracy: 0.5522\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2057 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.12324, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 2.2057 - accuracy: 0.5354 - val_loss: 2.1232 - val_accuracy: 0.5522\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1802 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.09888, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.1802 - accuracy: 0.5524 - val_loss: 2.0989 - val_accuracy: 0.5522\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1130 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.08442, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1130 - accuracy: 0.5666 - val_loss: 2.0844 - val_accuracy: 0.5672\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1280 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.07060, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1280 - accuracy: 0.5722 - val_loss: 2.0706 - val_accuracy: 0.5821\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0976 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.05606, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.0976 - accuracy: 0.5722 - val_loss: 2.0561 - val_accuracy: 0.5821\n",
      "Evaluating smic-s20\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3475 - accuracy: 0.5212\n",
      "Epoch 00001: val_loss improved from inf to 2.21735, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3475 - accuracy: 0.5212 - val_loss: 2.2174 - val_accuracy: 0.5455\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1904 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.16831, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1904 - accuracy: 0.5751 - val_loss: 2.1683 - val_accuracy: 0.5455\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1436 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.15386, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1436 - accuracy: 0.5666 - val_loss: 2.1539 - val_accuracy: 0.5455\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1600 - accuracy: 0.5467\n",
      "Epoch 00001: val_loss improved from inf to 2.13957, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1600 - accuracy: 0.5467 - val_loss: 2.1396 - val_accuracy: 0.5455\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1246 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.14320, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1246 - accuracy: 0.5694 - val_loss: 2.1432 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1206 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.11700, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1206 - accuracy: 0.5581 - val_loss: 2.1170 - val_accuracy: 0.5795\n",
      "Evaluating samm-036\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3736 - accuracy: 0.4929\n",
      "Epoch 00001: val_loss improved from inf to 2.20290, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3736 - accuracy: 0.4929 - val_loss: 2.2029 - val_accuracy: 0.5641\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1670 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.16980, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1670 - accuracy: 0.5722 - val_loss: 2.1698 - val_accuracy: 0.5513\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1854 - accuracy: 0.5382\n",
      "Epoch 00001: val_loss improved from inf to 2.15953, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1854 - accuracy: 0.5382 - val_loss: 2.1595 - val_accuracy: 0.5641\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1467 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.15187, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1467 - accuracy: 0.5581 - val_loss: 2.1519 - val_accuracy: 0.5641\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1293 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.14338, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1293 - accuracy: 0.5524 - val_loss: 2.1434 - val_accuracy: 0.5769\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0695 - accuracy: 0.6147\n",
      "Epoch 00001: val_loss improved from inf to 2.13816, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0695 - accuracy: 0.6147 - val_loss: 2.1382 - val_accuracy: 0.5769\n",
      "Evaluating casme2-sub12\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4283 - accuracy: 0.4844\n",
      "Epoch 00001: val_loss improved from inf to 2.20569, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.4283 - accuracy: 0.4844 - val_loss: 2.2057 - val_accuracy: 0.5443\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2178 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.17298, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.2178 - accuracy: 0.5666 - val_loss: 2.1730 - val_accuracy: 0.5443\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1832 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.16337, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1832 - accuracy: 0.5581 - val_loss: 2.1634 - val_accuracy: 0.5443\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1452 - accuracy: 0.5921\n",
      "Epoch 00001: val_loss improved from inf to 2.15172, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1452 - accuracy: 0.5921 - val_loss: 2.1517 - val_accuracy: 0.5443\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1159 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.14104, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1159 - accuracy: 0.5864 - val_loss: 2.1410 - val_accuracy: 0.5443\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0967 - accuracy: 0.5949\n",
      "Epoch 00001: val_loss improved from inf to 2.12741, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.0967 - accuracy: 0.5949 - val_loss: 2.1274 - val_accuracy: 0.5443\n",
      "Evaluating smic-s13\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4164 - accuracy: 0.4731\n",
      "Epoch 00001: val_loss improved from inf to 2.16796, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 113s 9s/step - loss: 2.4164 - accuracy: 0.4731 - val_loss: 2.1680 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2207 - accuracy: 0.5354\n",
      "Epoch 00001: val_loss improved from inf to 2.14788, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2207 - accuracy: 0.5354 - val_loss: 2.1479 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2116 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.13292, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2116 - accuracy: 0.5411 - val_loss: 2.1329 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1792 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.11572, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1792 - accuracy: 0.5694 - val_loss: 2.1157 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1688 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.10225, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1688 - accuracy: 0.5552 - val_loss: 2.1023 - val_accuracy: 0.5977\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1373 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.09527, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1373 - accuracy: 0.5751 - val_loss: 2.0953 - val_accuracy: 0.6207\n",
      "Evaluating casme2-sub22\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3238 - accuracy: 0.4901\n",
      "Epoch 00001: val_loss improved from inf to 2.13252, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.3238 - accuracy: 0.4901 - val_loss: 2.1325 - val_accuracy: 0.6163\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1730 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.11068, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1730 - accuracy: 0.5411 - val_loss: 2.1107 - val_accuracy: 0.6163\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1482 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.08650, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1482 - accuracy: 0.5524 - val_loss: 2.0865 - val_accuracy: 0.6163\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1301 - accuracy: 0.5467\n",
      "Epoch 00001: val_loss improved from inf to 2.08101, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1301 - accuracy: 0.5467 - val_loss: 2.0810 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0973 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.06665, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0973 - accuracy: 0.5864 - val_loss: 2.0667 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0950 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.06082, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0950 - accuracy: 0.5552 - val_loss: 2.0608 - val_accuracy: 0.6279\n",
      "Evaluating samm-018\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4131 - accuracy: 0.4674\n",
      "Epoch 00001: val_loss improved from inf to 2.09790, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.4131 - accuracy: 0.4674 - val_loss: 2.0979 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2294 - accuracy: 0.5071\n",
      "Epoch 00001: val_loss improved from inf to 2.09607, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2294 - accuracy: 0.5071 - val_loss: 2.0961 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1834 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.08310, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1834 - accuracy: 0.5269 - val_loss: 2.0831 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1503 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.06472, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1503 - accuracy: 0.5439 - val_loss: 2.0647 - val_accuracy: 0.6353\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1262 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.05536, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1262 - accuracy: 0.5694 - val_loss: 2.0554 - val_accuracy: 0.6353\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1080 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.04924, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1080 - accuracy: 0.5609 - val_loss: 2.0492 - val_accuracy: 0.6353\n",
      "Evaluating samm-017\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3454 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.14802, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 113s 9s/step - loss: 2.3454 - accuracy: 0.5269 - val_loss: 2.1480 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2120 - accuracy: 0.5326\n",
      "Epoch 00001: val_loss improved from inf to 2.11649, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2120 - accuracy: 0.5326 - val_loss: 2.1165 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1714 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.09632, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1714 - accuracy: 0.5439 - val_loss: 2.0963 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1361 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.08346, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1361 - accuracy: 0.5779 - val_loss: 2.0835 - val_accuracy: 0.5814\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1508 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 2.07087, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1508 - accuracy: 0.5807 - val_loss: 2.0709 - val_accuracy: 0.5814\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0866 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.06103, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0866 - accuracy: 0.5864 - val_loss: 2.0610 - val_accuracy: 0.5814\n",
      "Evaluating samm-034\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3660 - accuracy: 0.5241\n",
      "Epoch 00001: val_loss improved from inf to 2.23348, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 113s 9s/step - loss: 2.3660 - accuracy: 0.5241 - val_loss: 2.2335 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2011 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.15120, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.2011 - accuracy: 0.5779 - val_loss: 2.1512 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1754 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.13927, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 9s/step - loss: 2.1754 - accuracy: 0.5892 - val_loss: 2.1393 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1419 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.12096, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1419 - accuracy: 0.5836 - val_loss: 2.1210 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1238 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 2.11655, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1238 - accuracy: 0.5807 - val_loss: 2.1165 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0987 - accuracy: 0.6006\n",
      "Epoch 00001: val_loss improved from inf to 2.09036, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0987 - accuracy: 0.6006 - val_loss: 2.0904 - val_accuracy: 0.5256\n",
      "Evaluating casme2-sub19\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3725 - accuracy: 0.4929\n",
      "Epoch 00001: val_loss improved from inf to 2.18822, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 113s 9s/step - loss: 2.3725 - accuracy: 0.4929 - val_loss: 2.1882 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1896 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.15993, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1896 - accuracy: 0.5269 - val_loss: 2.1599 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1414 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.14467, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1414 - accuracy: 0.5751 - val_loss: 2.1447 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1295 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.13248, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1295 - accuracy: 0.5722 - val_loss: 2.1325 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1067 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.12037, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1067 - accuracy: 0.5694 - val_loss: 2.1204 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0949 - accuracy: 0.5949\n",
      "Epoch 00001: val_loss improved from inf to 2.11010, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.0949 - accuracy: 0.5949 - val_loss: 2.1101 - val_accuracy: 0.5814\n",
      "Evaluating casme2-sub01\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3593 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.33787, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 113s 9s/step - loss: 2.3593 - accuracy: 0.5269 - val_loss: 2.3379 - val_accuracy: 0.4762\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2149 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.26310, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2149 - accuracy: 0.5836 - val_loss: 2.2631 - val_accuracy: 0.4762\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1723 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.26461, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1723 - accuracy: 0.5581 - val_loss: 2.2646 - val_accuracy: 0.4881\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1595 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.25953, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1595 - accuracy: 0.5836 - val_loss: 2.2595 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1505 - accuracy: 0.6062\n",
      "Epoch 00001: val_loss improved from inf to 2.24880, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1505 - accuracy: 0.6062 - val_loss: 2.2488 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1164 - accuracy: 0.6147\n",
      "Epoch 00001: val_loss improved from inf to 2.23407, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1164 - accuracy: 0.6147 - val_loss: 2.2341 - val_accuracy: 0.5000\n",
      "Evaluating casme2-sub07\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3841 - accuracy: 0.4618\n",
      "Epoch 00001: val_loss improved from inf to 2.11897, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.3841 - accuracy: 0.4618 - val_loss: 2.1190 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1791 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.09465, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1791 - accuracy: 0.5637 - val_loss: 2.0946 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1662 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.07363, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1662 - accuracy: 0.5552 - val_loss: 2.0736 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1550 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.06188, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1550 - accuracy: 0.5722 - val_loss: 2.0619 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1332 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.05903, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1332 - accuracy: 0.5666 - val_loss: 2.0590 - val_accuracy: 0.5882\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1084 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.04620, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1084 - accuracy: 0.5864 - val_loss: 2.0462 - val_accuracy: 0.6000\n",
      "Evaluating smic-s9\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3495 - accuracy: 0.4958\n",
      "Epoch 00001: val_loss improved from inf to 2.32415, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 115s 10s/step - loss: 2.3495 - accuracy: 0.4958 - val_loss: 2.3241 - val_accuracy: 0.4881\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1797 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.26034, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.1797 - accuracy: 0.5524 - val_loss: 2.2603 - val_accuracy: 0.4881\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1399 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.25207, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1399 - accuracy: 0.5609 - val_loss: 2.2521 - val_accuracy: 0.4881\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1135 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.23820, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1135 - accuracy: 0.5864 - val_loss: 2.2382 - val_accuracy: 0.4881\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1036 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.23308, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1036 - accuracy: 0.5836 - val_loss: 2.2331 - val_accuracy: 0.4881\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0824 - accuracy: 0.5921\n",
      "Epoch 00001: val_loss improved from inf to 2.21462, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.0824 - accuracy: 0.5921 - val_loss: 2.2146 - val_accuracy: 0.5357\n",
      "Evaluating casme2-sub03\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4241 - accuracy: 0.5042\n",
      "Epoch 00001: val_loss improved from inf to 2.11719, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 10s/step - loss: 2.4241 - accuracy: 0.5042 - val_loss: 2.1172 - val_accuracy: 0.6429\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1950 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.10796, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1950 - accuracy: 0.5411 - val_loss: 2.1080 - val_accuracy: 0.6429\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2154 - accuracy: 0.5269\n",
      "Epoch 00001: val_loss improved from inf to 2.08681, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2154 - accuracy: 0.5269 - val_loss: 2.0868 - val_accuracy: 0.6429\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1769 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.09635, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1769 - accuracy: 0.5552 - val_loss: 2.0963 - val_accuracy: 0.6548\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1651 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.08042, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 142s 12s/step - loss: 2.1651 - accuracy: 0.5467 - val_loss: 2.0804 - val_accuracy: 0.6548\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1675 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.08550, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1675 - accuracy: 0.5581 - val_loss: 2.0855 - val_accuracy: 0.6548\n",
      "Evaluating samm-016\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4166 - accuracy: 0.5212 \n",
      "Epoch 00001: val_loss improved from inf to 2.11336, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 125s 10s/step - loss: 2.4166 - accuracy: 0.5212 - val_loss: 2.1134 - val_accuracy: 0.6092\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1826 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.10110, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1826 - accuracy: 0.5751 - val_loss: 2.1011 - val_accuracy: 0.6092\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1748 - accuracy: 0.5241\n",
      "Epoch 00001: val_loss improved from inf to 2.08779, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1748 - accuracy: 0.5241 - val_loss: 2.0878 - val_accuracy: 0.6092\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1502 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.08020, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1502 - accuracy: 0.5666 - val_loss: 2.0802 - val_accuracy: 0.6092\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1548 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.06974, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1548 - accuracy: 0.5779 - val_loss: 2.0697 - val_accuracy: 0.6092\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1096 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.06346, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1096 - accuracy: 0.5864 - val_loss: 2.0635 - val_accuracy: 0.6092\n",
      "Evaluating casme2-sub13\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3996 - accuracy: 0.4646\n",
      "Epoch 00001: val_loss improved from inf to 2.27539, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 110s 9s/step - loss: 2.3996 - accuracy: 0.4646 - val_loss: 2.2754 - val_accuracy: 0.5309\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2656 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.23689, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.2656 - accuracy: 0.5552 - val_loss: 2.2369 - val_accuracy: 0.5309\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2322 - accuracy: 0.5467\n",
      "Epoch 00001: val_loss improved from inf to 2.22179, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.2322 - accuracy: 0.5467 - val_loss: 2.2218 - val_accuracy: 0.5309\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1951 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.20741, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 2.1951 - accuracy: 0.5694 - val_loss: 2.2074 - val_accuracy: 0.5309\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1713 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.19652, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1713 - accuracy: 0.5666 - val_loss: 2.1965 - val_accuracy: 0.5432\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1613 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.18529, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1613 - accuracy: 0.5524 - val_loss: 2.1853 - val_accuracy: 0.5556\n",
      "Evaluating casme2-sub23\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3673 - accuracy: 0.4731\n",
      "Epoch 00001: val_loss improved from inf to 2.17360, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 109s 9s/step - loss: 2.3673 - accuracy: 0.4731 - val_loss: 2.1736 - val_accuracy: 0.5542\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1952 - accuracy: 0.5156\n",
      "Epoch 00001: val_loss improved from inf to 2.13836, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1952 - accuracy: 0.5156 - val_loss: 2.1384 - val_accuracy: 0.5542\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1833 - accuracy: 0.5241\n",
      "Epoch 00001: val_loss improved from inf to 2.12800, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1833 - accuracy: 0.5241 - val_loss: 2.1280 - val_accuracy: 0.5542\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1266 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.11569, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1266 - accuracy: 0.5637 - val_loss: 2.1157 - val_accuracy: 0.5542\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1563 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.10106, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1563 - accuracy: 0.5496 - val_loss: 2.1011 - val_accuracy: 0.5663\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1264 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.09756, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1264 - accuracy: 0.5552 - val_loss: 2.0976 - val_accuracy: 0.5663\n",
      "Evaluating samm-013\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3821 - accuracy: 0.5014\n",
      "Epoch 00001: val_loss improved from inf to 2.18251, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 108s 9s/step - loss: 2.3821 - accuracy: 0.5014 - val_loss: 2.1825 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1863 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.13405, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1863 - accuracy: 0.5609 - val_loss: 2.1340 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1573 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.12441, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 109s 9s/step - loss: 2.1573 - accuracy: 0.5637 - val_loss: 2.1244 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1530 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.11223, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1530 - accuracy: 0.5666 - val_loss: 2.1122 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0838 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.10907, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.0838 - accuracy: 0.5666 - val_loss: 2.1091 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1144 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.09631, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1144 - accuracy: 0.5722 - val_loss: 2.0963 - val_accuracy: 0.5930\n",
      "Evaluating casme2-sub15\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3795 - accuracy: 0.5127\n",
      "Epoch 00001: val_loss improved from inf to 2.28929, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 110s 9s/step - loss: 2.3795 - accuracy: 0.5127 - val_loss: 2.2893 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1969 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.22902, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1969 - accuracy: 0.5779 - val_loss: 2.2290 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1856 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.21930, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1856 - accuracy: 0.5637 - val_loss: 2.2193 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1498 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.20761, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1498 - accuracy: 0.5722 - val_loss: 2.2076 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1087 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.20617, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1087 - accuracy: 0.5836 - val_loss: 2.2062 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1117 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.19311, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1117 - accuracy: 0.5581 - val_loss: 2.1931 - val_accuracy: 0.5256\n",
      "Evaluating samm-006\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3853 - accuracy: 0.4759\n",
      "Epoch 00001: val_loss improved from inf to 2.18038, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 109s 9s/step - loss: 2.3853 - accuracy: 0.4759 - val_loss: 2.1804 - val_accuracy: 0.5395\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1797 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.13710, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1797 - accuracy: 0.5581 - val_loss: 2.1371 - val_accuracy: 0.5395\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1842 - accuracy: 0.5297\n",
      "Epoch 00001: val_loss improved from inf to 2.11574, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1842 - accuracy: 0.5297 - val_loss: 2.1157 - val_accuracy: 0.5526\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1164 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.10361, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1164 - accuracy: 0.5836 - val_loss: 2.1036 - val_accuracy: 0.5395\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1140 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.09778, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.1140 - accuracy: 0.5637 - val_loss: 2.0978 - val_accuracy: 0.5395\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0882 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.07374, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.0882 - accuracy: 0.5779 - val_loss: 2.0737 - val_accuracy: 0.5921\n",
      "Evaluating smic-s8\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3840 - accuracy: 0.4703\n",
      "Epoch 00001: val_loss improved from inf to 2.13489, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3840 - accuracy: 0.4703 - val_loss: 2.1349 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1964 - accuracy: 0.5382\n",
      "Epoch 00001: val_loss improved from inf to 2.11421, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1964 - accuracy: 0.5382 - val_loss: 2.1142 - val_accuracy: 0.5795\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1644 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.10125, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1644 - accuracy: 0.5666 - val_loss: 2.1013 - val_accuracy: 0.5795\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1597 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.09561, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1597 - accuracy: 0.5524 - val_loss: 2.0956 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1232 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.08903, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1232 - accuracy: 0.5552 - val_loss: 2.0890 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1071 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.08329, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1071 - accuracy: 0.5496 - val_loss: 2.0833 - val_accuracy: 0.5682\n",
      "Evaluating samm-023\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3444 - accuracy: 0.4986\n",
      "Epoch 00001: val_loss improved from inf to 2.23858, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 110s 9s/step - loss: 2.3444 - accuracy: 0.4986 - val_loss: 2.2386 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1998 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.19240, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1998 - accuracy: 0.5666 - val_loss: 2.1924 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1818 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.17407, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1818 - accuracy: 0.5694 - val_loss: 2.1741 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1400 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.16278, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1400 - accuracy: 0.5722 - val_loss: 2.1628 - val_accuracy: 0.5316\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1065 - accuracy: 0.5921\n",
      "Epoch 00001: val_loss improved from inf to 2.15843, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1065 - accuracy: 0.5921 - val_loss: 2.1584 - val_accuracy: 0.5443\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1157 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.15178, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1157 - accuracy: 0.5892 - val_loss: 2.1518 - val_accuracy: 0.5570\n",
      "Evaluating casme2-sub09\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3878 - accuracy: 0.4844\n",
      "Epoch 00001: val_loss improved from inf to 2.24404, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3878 - accuracy: 0.4844 - val_loss: 2.2440 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2018 - accuracy: 0.5354\n",
      "Epoch 00001: val_loss improved from inf to 2.17437, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.2018 - accuracy: 0.5354 - val_loss: 2.1744 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1690 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.16795, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1690 - accuracy: 0.5581 - val_loss: 2.1679 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1401 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.15717, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1401 - accuracy: 0.5892 - val_loss: 2.1572 - val_accuracy: 0.5747\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1282 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.14971, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1282 - accuracy: 0.5694 - val_loss: 2.1497 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1039 - accuracy: 0.6006\n",
      "Epoch 00001: val_loss improved from inf to 2.15568, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1039 - accuracy: 0.6006 - val_loss: 2.1557 - val_accuracy: 0.5747\n",
      "Evaluating casme2-sub04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3785 - accuracy: 0.4816\n",
      "Epoch 00001: val_loss improved from inf to 2.14360, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3785 - accuracy: 0.4816 - val_loss: 2.1436 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1666 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.12593, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1666 - accuracy: 0.5496 - val_loss: 2.1259 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1175 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.10229, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1175 - accuracy: 0.5779 - val_loss: 2.1023 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1261 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.09171, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1261 - accuracy: 0.5751 - val_loss: 2.0917 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1422 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.08006, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1422 - accuracy: 0.5496 - val_loss: 2.0801 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0873 - accuracy: 0.6062\n",
      "Epoch 00001: val_loss improved from inf to 2.07080, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0873 - accuracy: 0.6062 - val_loss: 2.0708 - val_accuracy: 0.6279\n",
      "Evaluating casme2-sub16\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3158 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.48083, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 110s 9s/step - loss: 2.3158 - accuracy: 0.5496 - val_loss: 2.4808 - val_accuracy: 0.3966\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1622 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.31895, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.1622 - accuracy: 0.5694 - val_loss: 2.3190 - val_accuracy: 0.3966\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1197 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.32068, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1197 - accuracy: 0.5892 - val_loss: 2.3207 - val_accuracy: 0.3966\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0920 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.30559, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 110s 9s/step - loss: 2.0920 - accuracy: 0.5836 - val_loss: 2.3056 - val_accuracy: 0.3966\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0952 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.29273, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 111s 9s/step - loss: 2.0952 - accuracy: 0.5892 - val_loss: 2.2927 - val_accuracy: 0.4138\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1010 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.28716, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1010 - accuracy: 0.5892 - val_loss: 2.2872 - val_accuracy: 0.4138\n",
      "Evaluating casme2-sub17\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3517 - accuracy: 0.4958\n",
      "Epoch 00001: val_loss improved from inf to 2.15811, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3517 - accuracy: 0.4958 - val_loss: 2.1581 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2099 - accuracy: 0.5382\n",
      "Epoch 00001: val_loss improved from inf to 2.13722, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.2099 - accuracy: 0.5382 - val_loss: 2.1372 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1514 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.12613, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1514 - accuracy: 0.5439 - val_loss: 2.1261 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1123 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.11486, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1123 - accuracy: 0.5751 - val_loss: 2.1149 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1430 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.11125, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1430 - accuracy: 0.5524 - val_loss: 2.1112 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1081 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.10408, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1081 - accuracy: 0.5751 - val_loss: 2.1041 - val_accuracy: 0.6235\n",
      "Evaluating samm-020\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3391 - accuracy: 0.4986\n",
      "Epoch 00001: val_loss improved from inf to 2.07398, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3391 - accuracy: 0.4986 - val_loss: 2.0740 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1601 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.07594, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1601 - accuracy: 0.5411 - val_loss: 2.0759 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2078 - accuracy: 0.5099\n",
      "Epoch 00001: val_loss improved from inf to 2.05319, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.2078 - accuracy: 0.5099 - val_loss: 2.0532 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1462 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.04705, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1462 - accuracy: 0.5496 - val_loss: 2.0471 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1510 - accuracy: 0.5212\n",
      "Epoch 00001: val_loss improved from inf to 2.03178, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1510 - accuracy: 0.5212 - val_loss: 2.0318 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1160 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 2.01583, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1160 - accuracy: 0.5807 - val_loss: 2.0158 - val_accuracy: 0.6667\n",
      "Evaluating samm-021\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4166 - accuracy: 0.4873\n",
      "Epoch 00001: val_loss improved from inf to 2.23593, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.4166 - accuracy: 0.4873 - val_loss: 2.2359 - val_accuracy: 0.5227\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1727 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.18388, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1727 - accuracy: 0.5496 - val_loss: 2.1839 - val_accuracy: 0.5227\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1698 - accuracy: 0.6062\n",
      "Epoch 00001: val_loss improved from inf to 2.16490, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1698 - accuracy: 0.6062 - val_loss: 2.1649 - val_accuracy: 0.5227\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1606 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.14600, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1606 - accuracy: 0.5637 - val_loss: 2.1460 - val_accuracy: 0.5568\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1357 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.14348, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1357 - accuracy: 0.5722 - val_loss: 2.1435 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1222 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 2.12617, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1222 - accuracy: 0.5807 - val_loss: 2.1262 - val_accuracy: 0.5682\n",
      "Evaluating samm-019\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3954 - accuracy: 0.4731\n",
      "Epoch 00001: val_loss improved from inf to 2.01552, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3954 - accuracy: 0.4731 - val_loss: 2.0155 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2204 - accuracy: 0.5127\n",
      "Epoch 00001: val_loss improved from inf to 2.01604, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.2204 - accuracy: 0.5127 - val_loss: 2.0160 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1764 - accuracy: 0.5326\n",
      "Epoch 00001: val_loss improved from inf to 1.99722, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1764 - accuracy: 0.5326 - val_loss: 1.9972 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1580 - accuracy: 0.5467\n",
      "Epoch 00001: val_loss improved from inf to 1.99814, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1580 - accuracy: 0.5467 - val_loss: 1.9981 - val_accuracy: 0.7429\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1346 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 1.98694, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1346 - accuracy: 0.5751 - val_loss: 1.9869 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1308 - accuracy: 0.5354\n",
      "Epoch 00001: val_loss improved from inf to 1.98220, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.1308 - accuracy: 0.5354 - val_loss: 1.9822 - val_accuracy: 0.7143\n",
      "Evaluating smic-s4\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3510 - accuracy: 0.5212\n",
      "Epoch 00001: val_loss improved from inf to 2.25128, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3510 - accuracy: 0.5212 - val_loss: 2.2513 - val_accuracy: 0.5366\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1603 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.21167, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1603 - accuracy: 0.5694 - val_loss: 2.2117 - val_accuracy: 0.5366\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1541 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.19567, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 2.1541 - accuracy: 0.5524 - val_loss: 2.1957 - val_accuracy: 0.5366\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1180 - accuracy: 0.6119\n",
      "Epoch 00001: val_loss improved from inf to 2.19362, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 2.1180 - accuracy: 0.6119 - val_loss: 2.1936 - val_accuracy: 0.5366\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0976 - accuracy: 0.6034\n",
      "Epoch 00001: val_loss improved from inf to 2.18381, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0976 - accuracy: 0.6034 - val_loss: 2.1838 - val_accuracy: 0.5366\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0760 - accuracy: 0.6176\n",
      "Epoch 00001: val_loss improved from inf to 2.17653, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.0760 - accuracy: 0.6176 - val_loss: 2.1765 - val_accuracy: 0.5366\n",
      "Evaluating smic-s18\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3946 - accuracy: 0.4646\n",
      "Epoch 00001: val_loss improved from inf to 2.04838, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 10s/step - loss: 2.3946 - accuracy: 0.4646 - val_loss: 2.0484 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1915 - accuracy: 0.5326\n",
      "Epoch 00001: val_loss improved from inf to 2.04116, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1915 - accuracy: 0.5326 - val_loss: 2.0412 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1760 - accuracy: 0.5326\n",
      "Epoch 00001: val_loss improved from inf to 2.02073, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1760 - accuracy: 0.5326 - val_loss: 2.0207 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1550 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.00822, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1550 - accuracy: 0.5411 - val_loss: 2.0082 - val_accuracy: 0.6235\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1102 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 1.99906, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1102 - accuracy: 0.5637 - val_loss: 1.9991 - val_accuracy: 0.6471\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1177 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 1.99214, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1177 - accuracy: 0.5524 - val_loss: 1.9921 - val_accuracy: 0.6471\n",
      "Evaluating casme2-sub11\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3641 - accuracy: 0.4674\n",
      "Epoch 00001: val_loss improved from inf to 2.14774, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 112s 9s/step - loss: 2.3641 - accuracy: 0.4674 - val_loss: 2.1477 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2153 - accuracy: 0.5382\n",
      "Epoch 00001: val_loss improved from inf to 2.10825, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.2153 - accuracy: 0.5382 - val_loss: 2.1082 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1497 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.08175, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1497 - accuracy: 0.5694 - val_loss: 2.0817 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1551 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.07254, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1551 - accuracy: 0.5694 - val_loss: 2.0725 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1362 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.06048, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1362 - accuracy: 0.5609 - val_loss: 2.0605 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1215 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.05496, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1215 - accuracy: 0.5694 - val_loss: 2.0550 - val_accuracy: 0.6375\n",
      "Evaluating casme2-sub02\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4025 - accuracy: 0.4816\n",
      "Epoch 00001: val_loss improved from inf to 2.13174, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 113s 9s/step - loss: 2.4025 - accuracy: 0.4816 - val_loss: 2.1317 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1860 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.10440, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1860 - accuracy: 0.5694 - val_loss: 2.1044 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1664 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.09187, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1664 - accuracy: 0.5637 - val_loss: 2.0919 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1468 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.07638, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1468 - accuracy: 0.5524 - val_loss: 2.0764 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1644 - accuracy: 0.5524\n",
      "Epoch 00001: val_loss improved from inf to 2.06512, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1644 - accuracy: 0.5524 - val_loss: 2.0651 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1049 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.05701, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1049 - accuracy: 0.5694 - val_loss: 2.0570 - val_accuracy: 0.6322\n",
      "Evaluating smic-s5\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3734 - accuracy: 0.5297\n",
      "Epoch 00001: val_loss improved from inf to 2.28281, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 10s/step - loss: 2.3734 - accuracy: 0.5297 - val_loss: 2.2828 - val_accuracy: 0.4941\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1728 - accuracy: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 2.19865, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1728 - accuracy: 0.5864 - val_loss: 2.1987 - val_accuracy: 0.4941\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1826 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.18770, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1826 - accuracy: 0.5694 - val_loss: 2.1877 - val_accuracy: 0.4941\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1590 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.19144, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 2.1590 - accuracy: 0.5779 - val_loss: 2.1914 - val_accuracy: 0.4941\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1059 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.17350, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1059 - accuracy: 0.5694 - val_loss: 2.1735 - val_accuracy: 0.5059\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0894 - accuracy: 0.6006\n",
      "Epoch 00001: val_loss improved from inf to 2.17351, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 10s/step - loss: 2.0894 - accuracy: 0.6006 - val_loss: 2.1735 - val_accuracy: 0.5294\n",
      "Evaluating smic-s6\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3506 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.42046, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 111s 9s/step - loss: 2.3506 - accuracy: 0.5439 - val_loss: 2.4205 - val_accuracy: 0.4262\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1827 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.26304, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1827 - accuracy: 0.5666 - val_loss: 2.2630 - val_accuracy: 0.4262\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1287 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.26979, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1287 - accuracy: 0.5836 - val_loss: 2.2698 - val_accuracy: 0.4426\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0993 - accuracy: 0.6091\n",
      "Epoch 00001: val_loss improved from inf to 2.27426, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0993 - accuracy: 0.6091 - val_loss: 2.2743 - val_accuracy: 0.4590\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1073 - accuracy: 0.6034\n",
      "Epoch 00001: val_loss improved from inf to 2.27310, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1073 - accuracy: 0.6034 - val_loss: 2.2731 - val_accuracy: 0.4590\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0513 - accuracy: 0.6119\n",
      "Epoch 00001: val_loss improved from inf to 2.25992, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.0513 - accuracy: 0.6119 - val_loss: 2.2599 - val_accuracy: 0.4590\n",
      "Evaluating smic-s2\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3372 - accuracy: 0.4929 \n",
      "Epoch 00001: val_loss improved from inf to 2.14047, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 118s 10s/step - loss: 2.3372 - accuracy: 0.4929 - val_loss: 2.1405 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1538 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.10710, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1538 - accuracy: 0.5609 - val_loss: 2.1071 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1524 - accuracy: 0.5666\n",
      "Epoch 00001: val_loss improved from inf to 2.08968, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1524 - accuracy: 0.5666 - val_loss: 2.0897 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1326 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.07406, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1326 - accuracy: 0.5411 - val_loss: 2.0741 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1202 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.05928, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.1202 - accuracy: 0.5637 - val_loss: 2.0593 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0722 - accuracy: 0.5807\n",
      "Epoch 00001: val_loss improved from inf to 2.04751, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.0722 - accuracy: 0.5807 - val_loss: 2.0475 - val_accuracy: 0.6512\n",
      "Evaluating samm-015\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3781 - accuracy: 0.4958\n",
      "Epoch 00001: val_loss improved from inf to 2.16098, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.3781 - accuracy: 0.4958 - val_loss: 2.1610 - val_accuracy: 0.5800\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1907 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.12601, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1907 - accuracy: 0.5751 - val_loss: 2.1260 - val_accuracy: 0.5800\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1629 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.10405, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1629 - accuracy: 0.5439 - val_loss: 2.1040 - val_accuracy: 0.5800\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1770 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.09478, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1770 - accuracy: 0.5751 - val_loss: 2.0948 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1371 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.08590, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1371 - accuracy: 0.5552 - val_loss: 2.0859 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0873 - accuracy: 0.6062\n",
      "Epoch 00001: val_loss improved from inf to 2.07409, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 112s 9s/step - loss: 2.0873 - accuracy: 0.6062 - val_loss: 2.0741 - val_accuracy: 0.6000\n",
      "Evaluating smic-s3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4053 - accuracy: 0.4788\n",
      "Epoch 00001: val_loss improved from inf to 2.13220, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 10s/step - loss: 2.4053 - accuracy: 0.4788 - val_loss: 2.1322 - val_accuracy: 0.5833\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2197 - accuracy: 0.5326\n",
      "Epoch 00001: val_loss improved from inf to 2.11003, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2197 - accuracy: 0.5326 - val_loss: 2.1100 - val_accuracy: 0.5833\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1574 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.10093, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1574 - accuracy: 0.5722 - val_loss: 2.1009 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1938 - accuracy: 0.5467\n",
      "Epoch 00001: val_loss improved from inf to 2.08699, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1938 - accuracy: 0.5467 - val_loss: 2.0870 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1273 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.07770, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1273 - accuracy: 0.5836 - val_loss: 2.0777 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1153 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.07127, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1153 - accuracy: 0.5722 - val_loss: 2.0713 - val_accuracy: 0.5952\n",
      "Evaluating samm-033\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3857 - accuracy: 0.4958\n",
      "Epoch 00001: val_loss improved from inf to 2.13222, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.3857 - accuracy: 0.4958 - val_loss: 2.1322 - val_accuracy: 0.5823\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1760 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.11441, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1760 - accuracy: 0.5609 - val_loss: 2.1144 - val_accuracy: 0.5696\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1516 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.10354, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1516 - accuracy: 0.5694 - val_loss: 2.1035 - val_accuracy: 0.5696\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1618 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.09553, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 114s 9s/step - loss: 2.1618 - accuracy: 0.5496 - val_loss: 2.0955 - val_accuracy: 0.6076\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0972 - accuracy: 0.6006\n",
      "Epoch 00001: val_loss improved from inf to 2.09028, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0972 - accuracy: 0.6006 - val_loss: 2.0903 - val_accuracy: 0.6076\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1052 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.08390, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1052 - accuracy: 0.5722 - val_loss: 2.0839 - val_accuracy: 0.6203\n",
      "Evaluating samm-014\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3663 - accuracy: 0.5014\n",
      "Epoch 00001: val_loss improved from inf to 2.28340, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 10s/step - loss: 2.3663 - accuracy: 0.5014 - val_loss: 2.2834 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1629 - accuracy: 0.5751\n",
      "Epoch 00001: val_loss improved from inf to 2.21638, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1629 - accuracy: 0.5751 - val_loss: 2.2164 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1247 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.19245, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1247 - accuracy: 0.5581 - val_loss: 2.1924 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1266 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.17806, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1266 - accuracy: 0.5722 - val_loss: 2.1781 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0856 - accuracy: 0.5949\n",
      "Epoch 00001: val_loss improved from inf to 2.16212, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0856 - accuracy: 0.5949 - val_loss: 2.1621 - val_accuracy: 0.4941\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0763 - accuracy: 0.6006\n",
      "Epoch 00001: val_loss improved from inf to 2.14723, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.0763 - accuracy: 0.6006 - val_loss: 2.1472 - val_accuracy: 0.5529\n",
      "Evaluating samm-009\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3819 - accuracy: 0.4759\n",
      "Epoch 00001: val_loss improved from inf to 2.13743, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 115s 10s/step - loss: 2.3819 - accuracy: 0.4759 - val_loss: 2.1374 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1830 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.09924, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1830 - accuracy: 0.5552 - val_loss: 2.0992 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1803 - accuracy: 0.5354\n",
      "Epoch 00001: val_loss improved from inf to 2.07700, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1803 - accuracy: 0.5354 - val_loss: 2.0770 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1630 - accuracy: 0.5439\n",
      "Epoch 00001: val_loss improved from inf to 2.05970, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1630 - accuracy: 0.5439 - val_loss: 2.0597 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1329 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.05015, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1329 - accuracy: 0.5722 - val_loss: 2.0502 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1228 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.03643, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1228 - accuracy: 0.5552 - val_loss: 2.0364 - val_accuracy: 0.6744\n",
      "Evaluating samm-012\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3105 - accuracy: 0.5297\n",
      "Epoch 00001: val_loss improved from inf to 2.34713, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 114s 9s/step - loss: 2.3105 - accuracy: 0.5297 - val_loss: 2.3471 - val_accuracy: 0.5060\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1626 - accuracy: 0.5921\n",
      "Epoch 00001: val_loss improved from inf to 2.24282, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1626 - accuracy: 0.5921 - val_loss: 2.2428 - val_accuracy: 0.5060\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1022 - accuracy: 0.6062\n",
      "Epoch 00001: val_loss improved from inf to 2.22393, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1022 - accuracy: 0.6062 - val_loss: 2.2239 - val_accuracy: 0.5060\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0840 - accuracy: 0.6006\n",
      "Epoch 00001: val_loss improved from inf to 2.22186, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0840 - accuracy: 0.6006 - val_loss: 2.2219 - val_accuracy: 0.5060\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0701 - accuracy: 0.6147\n",
      "Epoch 00001: val_loss improved from inf to 2.19522, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.0701 - accuracy: 0.6147 - val_loss: 2.1952 - val_accuracy: 0.5181\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0471 - accuracy: 0.6261\n",
      "Epoch 00001: val_loss improved from inf to 2.18100, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.0471 - accuracy: 0.6261 - val_loss: 2.1810 - val_accuracy: 0.5301\n",
      "Evaluating casme2-sub05\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3711 - accuracy: 0.5099\n",
      "Epoch 00001: val_loss improved from inf to 2.28507, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 115s 10s/step - loss: 2.3711 - accuracy: 0.5099 - val_loss: 2.2851 - val_accuracy: 0.4884\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1728 - accuracy: 0.6006 \n",
      "Epoch 00001: val_loss improved from inf to 2.21310, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1728 - accuracy: 0.6006 - val_loss: 2.2131 - val_accuracy: 0.4884\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1243 - accuracy: 0.5949\n",
      "Epoch 00001: val_loss improved from inf to 2.20411, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1243 - accuracy: 0.5949 - val_loss: 2.2041 - val_accuracy: 0.4884\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1272 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.20435, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1272 - accuracy: 0.5722 - val_loss: 2.2044 - val_accuracy: 0.4884\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0707 - accuracy: 0.6034\n",
      "Epoch 00001: val_loss improved from inf to 2.17621, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.0707 - accuracy: 0.6034 - val_loss: 2.1762 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0777 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.18331, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.0777 - accuracy: 0.5779 - val_loss: 2.1833 - val_accuracy: 0.5000\n",
      "Evaluating casme2-sub24\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3172 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.19752, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 113s 9s/step - loss: 2.3172 - accuracy: 0.5609 - val_loss: 2.1975 - val_accuracy: 0.6176\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1757 - accuracy: 0.6119 \n",
      "Epoch 00001: val_loss improved from inf to 2.16543, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1757 - accuracy: 0.6119 - val_loss: 2.1654 - val_accuracy: 0.6176\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1326 - accuracy: 0.6091\n",
      "Epoch 00001: val_loss improved from inf to 2.17905, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1326 - accuracy: 0.6091 - val_loss: 2.1790 - val_accuracy: 0.6176\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1280 - accuracy: 0.5921\n",
      "Epoch 00001: val_loss improved from inf to 2.16390, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.1280 - accuracy: 0.5921 - val_loss: 2.1639 - val_accuracy: 0.6176\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1177 - accuracy: 0.6062 \n",
      "Epoch 00001: val_loss improved from inf to 2.17305, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 115s 10s/step - loss: 2.1177 - accuracy: 0.6062 - val_loss: 2.1731 - val_accuracy: 0.6176\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0747 - accuracy: 0.6147\n",
      "Epoch 00001: val_loss improved from inf to 2.16220, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 113s 9s/step - loss: 2.0747 - accuracy: 0.6147 - val_loss: 2.1622 - val_accuracy: 0.6176\n",
      "Evaluating smic-s1\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4241 - accuracy: 0.4674\n",
      "Epoch 00001: val_loss improved from inf to 2.09979, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 115s 10s/step - loss: 2.4241 - accuracy: 0.4674 - val_loss: 2.0998 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2312 - accuracy: 0.5241\n",
      "Epoch 00001: val_loss improved from inf to 2.08452, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2312 - accuracy: 0.5241 - val_loss: 2.0845 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1876 - accuracy: 0.5382\n",
      "Epoch 00001: val_loss improved from inf to 2.07054, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1876 - accuracy: 0.5382 - val_loss: 2.0705 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1812 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.06473, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.1812 - accuracy: 0.5581 - val_loss: 2.0647 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1609 - accuracy: 0.5496\n",
      "Epoch 00001: val_loss improved from inf to 2.05470, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1609 - accuracy: 0.5496 - val_loss: 2.0547 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1611 - accuracy: 0.5637\n",
      "Epoch 00001: val_loss improved from inf to 2.04545, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1611 - accuracy: 0.5637 - val_loss: 2.0454 - val_accuracy: 0.6235\n",
      "Evaluating samm-032\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3811 - accuracy: 0.5014\n",
      "Epoch 00001: val_loss improved from inf to 2.13521, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 117s 10s/step - loss: 2.3811 - accuracy: 0.5014 - val_loss: 2.1352 - val_accuracy: 0.6220\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2071 - accuracy: 0.5552\n",
      "Epoch 00001: val_loss improved from inf to 2.12362, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.2071 - accuracy: 0.5552 - val_loss: 2.1236 - val_accuracy: 0.6220\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1568 - accuracy: 0.5297\n",
      "Epoch 00001: val_loss improved from inf to 2.10626, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1568 - accuracy: 0.5297 - val_loss: 2.1063 - val_accuracy: 0.6220\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1598 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.10367, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 116s 10s/step - loss: 2.1598 - accuracy: 0.5694 - val_loss: 2.1037 - val_accuracy: 0.6220\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1476 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.10072, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 2.1476 - accuracy: 0.5694 - val_loss: 2.1007 - val_accuracy: 0.6220\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1192 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.08715, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1192 - accuracy: 0.5807 - val_loss: 2.0871 - val_accuracy: 0.6220\n",
      "Evaluating smic-s11\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3308 - accuracy: 0.5212\n",
      "Epoch 00001: val_loss improved from inf to 2.10971, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 116s 10s/step - loss: 2.3308 - accuracy: 0.5212 - val_loss: 2.1097 - val_accuracy: 0.6071\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1819 - accuracy: 0.5297\n",
      "Epoch 00001: val_loss improved from inf to 2.11248, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1819 - accuracy: 0.5297 - val_loss: 2.1125 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1792 - accuracy: 0.5156\n",
      "Epoch 00001: val_loss improved from inf to 2.09919, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1792 - accuracy: 0.5156 - val_loss: 2.0992 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1565 - accuracy: 0.5297\n",
      "Epoch 00001: val_loss improved from inf to 2.09428, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1565 - accuracy: 0.5297 - val_loss: 2.0943 - val_accuracy: 0.6190\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1295 - accuracy: 0.5779\n",
      "Epoch 00001: val_loss improved from inf to 2.08275, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1295 - accuracy: 0.5779 - val_loss: 2.0828 - val_accuracy: 0.6310\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1267 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.08141, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1267 - accuracy: 0.5722 - val_loss: 2.0814 - val_accuracy: 0.6071\n",
      "Evaluating casme2-sub25\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4011 - accuracy: 0.4816\n",
      "Epoch 00001: val_loss improved from inf to 2.12811, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 117s 10s/step - loss: 2.4011 - accuracy: 0.4816 - val_loss: 2.1281 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2151 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.10862, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.2151 - accuracy: 0.5411 - val_loss: 2.1086 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1602 - accuracy: 0.5609\n",
      "Epoch 00001: val_loss improved from inf to 2.09807, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1602 - accuracy: 0.5609 - val_loss: 2.0981 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1792 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.08624, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 2.1792 - accuracy: 0.5552 - val_loss: 2.0862 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1310 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.07891, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1310 - accuracy: 0.5722 - val_loss: 2.0789 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1309 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.07078, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1309 - accuracy: 0.5722 - val_loss: 2.0708 - val_accuracy: 0.6364\n",
      "Evaluating samm-037\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4211 - accuracy: 0.4929\n",
      "Epoch 00001: val_loss improved from inf to 2.27658, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 117s 10s/step - loss: 2.4211 - accuracy: 0.4929 - val_loss: 2.2766 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1953 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.22571, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1953 - accuracy: 0.5581 - val_loss: 2.2257 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2006 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.21282, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.2006 - accuracy: 0.5439 - val_loss: 2.2128 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1534 - accuracy: 0.5836\n",
      "Epoch 00001: val_loss improved from inf to 2.19293, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1534 - accuracy: 0.5836 - val_loss: 2.1929 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1577 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.17642, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1577 - accuracy: 0.5722 - val_loss: 2.1764 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1560 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.16281, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1560 - accuracy: 0.5722 - val_loss: 2.1628 - val_accuracy: 0.5128\n",
      "Evaluating casme2-sub26\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4235 - accuracy: 0.4873\n",
      "Epoch 00001: val_loss improved from inf to 2.28984, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 116s 10s/step - loss: 2.4235 - accuracy: 0.4873 - val_loss: 2.2898 - val_accuracy: 0.5116\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1957 - accuracy: 0.5722\n",
      "Epoch 00001: val_loss improved from inf to 2.23440, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1957 - accuracy: 0.5722 - val_loss: 2.2344 - val_accuracy: 0.5116\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2291 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.21021, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.2291 - accuracy: 0.5581 - val_loss: 2.2102 - val_accuracy: 0.5116\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1901 - accuracy: 0.5694\n",
      "Epoch 00001: val_loss improved from inf to 2.20303, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1901 - accuracy: 0.5694 - val_loss: 2.2030 - val_accuracy: 0.5116\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1630 - accuracy: 0.5892\n",
      "Epoch 00001: val_loss improved from inf to 2.20804, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1630 - accuracy: 0.5892 - val_loss: 2.2080 - val_accuracy: 0.5116\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1292 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.18950, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1292 - accuracy: 0.5751 - val_loss: 2.1895 - val_accuracy: 0.5465\n",
      "Evaluating samm-030\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4333 - accuracy: 0.4448\n",
      "Epoch 00001: val_loss improved from inf to 2.14008, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 116s 10s/step - loss: 2.4333 - accuracy: 0.4448 - val_loss: 2.1401 - val_accuracy: 0.5942\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2469 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.12680, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.2469 - accuracy: 0.5411 - val_loss: 2.1268 - val_accuracy: 0.5942\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2085 - accuracy: 0.5411\n",
      "Epoch 00001: val_loss improved from inf to 2.12552, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.2085 - accuracy: 0.5411 - val_loss: 2.1255 - val_accuracy: 0.5942\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1965 - accuracy: 0.5581\n",
      "Epoch 00001: val_loss improved from inf to 2.12096, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 117s 10s/step - loss: 2.1965 - accuracy: 0.5581 - val_loss: 2.1210 - val_accuracy: 0.5942\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1697 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.11554, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1697 - accuracy: 0.5552 - val_loss: 2.1155 - val_accuracy: 0.6087\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1496 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.11337, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1496 - accuracy: 0.5751 - val_loss: 2.1134 - val_accuracy: 0.6087\n",
      "Evaluating samm-011\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3358 - accuracy: 0.4873\n",
      "Epoch 00001: val_loss improved from inf to 2.08111, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-head-base1epoch-0801202214.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 117s 10s/step - loss: 2.3358 - accuracy: 0.4873 - val_loss: 2.0811 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2311 - accuracy: 0.5042 \n",
      "Epoch 00001: val_loss improved from inf to 2.07507, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.2311 - accuracy: 0.5042 - val_loss: 2.0751 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1947 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.05229, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1947 - accuracy: 0.5269 - val_loss: 2.0523 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1795 - accuracy: 0.5042\n",
      "Epoch 00001: val_loss improved from inf to 2.04375, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 118s 10s/step - loss: 2.1795 - accuracy: 0.5042 - val_loss: 2.0438 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1530 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.03793, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.1530 - accuracy: 0.5326 - val_loss: 2.0379 - val_accuracy: 0.6477\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1034 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.01906, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0801202214_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.1034 - accuracy: 0.5524 - val_loss: 2.0191 - val_accuracy: 0.6477\n",
      "Evaluating casme2-sub08\n",
      "'Figure' object has no attribute 'saveFig'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAQhCAYAAAAj0UQDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAB+qElEQVR4nO3deVyU9f4+/us9wy6LoIKy7wzDpuKSuaWWWmaRWamdbPl0PGV2StTMzsls8ZSdVj3ZdrTSk2ZZaZZmi0supYI/V0BARVkEUVBAEBh4//5g+YKpbDM3M/dcz8fDR87MPXO/7/t8Pte8vJm5EFJKEBGRemk6ewFERGRaDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIlI5m85ewJW6d+8uAwMDO3sZREQWJTk5+ZyUssfVHjO7oA8MDERSUlJnL4OIyKIIIU5d6zFeuiEiUjkGPRGRyjHoiYhUzuyu0RNRx1RXVyMnJweXL1/u7KWQCTg4OMDX1xe2tratfg6DnkhlcnJy4OLigsDAQAghOns5ZERSSpw/fx45OTkICgpq9fN46YZIZS5fvoxu3box5FVICIFu3bq1+V9rDHoiFWLIq1d7/rdl0BNZAa1Wi969eyMuLg59+/bF7t272/U677zzDsrLy6/5+Llz52Bra4sPPvigvUs1maSkJPz9738HAGzbtq3d5+BqsrKysGrVqqvuyxww6ImsgKOjIw4cOICDBw/i1Vdfxbx589r1Oi0F/VdffYUbbrgBq1evbu9SjcZgMDS73a9fPyxevBhA+4L+ytdr6sqgb7ovc8CgJ7IyJSUlcHd3b7z973//G/3790dsbCxeeOEFAMClS5cwbtw4xMXFITo6GmvWrMHixYuRl5eHESNGYMSIEVd97dWrV+PNN99Ebm4ucnJyGu9fsWIFYmNjERcXhwceeAAAUFBQgLvuugtxcXGIi4u7avA6Oztj5syZiIqKwqhRo1BYWAgAOH78OMaOHYv4+HgMHToUaWlpAICHHnoIjz32GAYOHIhnnnmm2Wtt27YNt99+O7KysvDBBx/g7bffRu/evbFjxw4UFhbi7rvvRv/+/dG/f3/s2rULALBgwQI88MADGDx4MB544AFkZWVh6NCh6Nu3b7N/GT377LPYsWMHevfujbfffrtxXwBQVFSEhIQExMbG4oYbbsChQ4caX/uRRx7BTTfdhODg4MY3hqud+w6TUprVn/j4eElE7ZeSkvKn+zQajYyLi5MRERHS1dVVJiUlSSml3Lx5s/zrX/8qa2trZU1NjRw3bpzcvn27XLt2rXz00Ucbn3/hwgUppZQBAQGysLDwqvs9ffq0DA0NlVJKOW/ePPnGG29IKaU8cuSIDAsLa3ze+fPnpZRS3nvvvfLtt9+WUkppMBga99EUAPm///1PSinliy++KJ944gkppZQjR46U6enpUkop//jjDzlixAgppZQPPvigHDdunDQYDH96ra1bt8px48ZJKaV84YUX5L///e/GxyZPnix37NghpZTy1KlTUqfTNW7Xt29fWV5eLqWU8tKlS7KiokJKKWV6erpsyKumr33l7RkzZsgFCxZIKaX89ddfZVxcXONrDxo0SF6+fFkWFhZKDw8PWVVVdc1z39TV/jcGkCSvkav8eCWRFWi4dAMAv//+O6ZOnYojR47gp59+wk8//YQ+ffoAAMrKypCRkYGhQ4di1qxZmDt3Lm6//XYMHTq0xX2sWbMG9957LwBg0qRJeOSRRzBr1ixs2bIF99xzD7p37w4A8PDwAABs2bIFK1asAFD3MwQ3N7c/vaZGo8F9990HAPjLX/6CCRMmoKysDLt378Y999zTuF1lZWXj3++55x5otdo2nZ9ffvkFKSkpjbdLSkpQVlYGALjjjjvg6OgIoO47CjNmzMCBAweg1WqRnp7e4mvv3LkTX3/9NQBg5MiROH/+PEpKSgAA48aNg729Pezt7eHp6YmCggLExMS0+dy3hEFPZGUGDRqEc+fOobCwEFJKzJs3D3/729/+tN3+/fuxceNG/POf/8SoUaMwf/78677u6tWrkZ+fj88//xwAkJeXh4yMDKOuXQiB2tpadO3atfGN60pdunRp8+vW1tbijz/+gIODw3Vf7+2334aXlxcOHjyI2traq27fFvb29o1/12q1MBgMCA8Pb/O5b0mrrtELIcYKIY4JITKFEM9e5fG3hRAH6v+kCyEuNHnsQSFERv2fBzu0WiLqsLS0NNTU1KBbt24YM2YMli9f3ji95ubm4uzZs8jLy4OTkxP+8pe/YM6cOdi/fz8AwMXFBaWlpX96zfT0dJSVlSE3NxdZWVnIysrCvHnzsHr1aowcORJfffUVzp8/D6DumjUAjBo1Cu+//z4AoKamBhcvXvzT69bW1mLt2rUAgFWrVmHIkCFwdXVFUFAQvvrqKwB1l58PHjzYpnNw5XGMHj0aS5Ysabx9rTeRixcvolevXtBoNFi5ciVqamque14AYOjQoY1vftu2bUP37t3h6up6zbVd69x3RItBL4TQAngPwK0A9AAmCyH0TbeRUs6UUvaWUvYGsATAN/XP9QDwAoCBAAYAeEEI4Q4iUlRFRQV69+6N3r1747777sNnn30GrVaL0aNHY8qUKRg0aBBiYmIwceJElJaW4vDhwxgwYAB69+6NF198Ef/85z8BANOmTcPYsWP/9MPY1atX46677mp23913343Vq1cjKioK//jHPzB8+HDExcUhMTERAPDuu+9i69atiImJQXx8fLNLJw26dOmCvXv3Ijo6Glu2bGmcbD///HMsW7YMcXFxiIqKwvr169t0PsaPH49vv/228YexixcvRlJSEmJjY6HX66/58dDp06fjs88+Q1xcHNLS0hqn/djYWGi1WsTFxeHtt99u9pwFCxYgOTkZsbGxePbZZ/HZZ59dd23XOvcdIequ4V9nAyEGAVggpRxTf3seAEgpX73G9rsBvCCl/FkIMRnATVLKv9U/9iGAbVLKa372ql+/fpJ99ETtl5qaisjIyM5ehlE4Ozs3/muD/p+r/W8shEiWUva72vatuXTjAyC7ye2c+vv+RAgRACAIwJa2PpeIiEzD2J+jnwRgrZSypi1PEkJME0IkCSGSGj4nS0TEad44WhP0uQD8mtz2rb/vaiYBaHpZplXPlVJ+JKXsJ6Xs16PHVX/lIRERtVNrgn4fgDAhRJAQwg51Yf7dlRsJIXQA3AH83uTuzQBGCyHc638IO7r+PiIiUkiLn6OXUhqEEDNQF9BaAMullEeFEC+h7ptYDaE/CcAXsslPd6WURUKIl1H3ZgEAL0kpi4x7CEREdD2t+sKUlHIjgI1X3Df/itsLrvHc5QCWt3N9RETUQSw1IyKr8fnnnyM2NhYxMTG48cYb2/xFq2sJDAzEuXPn/nR/ZWUl7rvvPoSGhmLgwIHIysoCAJw/fx4jRoyAs7MzZsyYYZQ1XA+DnoisRlBQELZv347Dhw/j+eefx7Rp00y6v2XLlsHd3R2ZmZmYOXMm5s6dC6Du976+/PLLeOONN0y6/wYMeiJS1JWVxRs2bMDAgQPRp08f3HzzzSgoKAAAbN++vfHbvH369EFpaSm2bduG4cOH484770RwcDCeffZZfP755xgwYABiYmJw/PhxALhm7fCNN97YWNF8ww03NKtSbupa+26oHgaAGTNm4NNPP228/frrryMmJgYDBgxAZmYmAGD9+vV48MG65peJEyfi119/hZQSXbp0wZAhQzrcldNaLDUjskIvbjiKlLwSo76m3tsVL4yPuu42R48exSuvvILdu3eje/fuKCoqghACf/zxB4QQ+O9//4vXX38db775Jt544w289957GDx4MMrKyhpD8eDBg0hNTYWHhweCg4Px6KOPYu/evXj33XexZMkSvPPOO3jqqacwc+ZMDBkyBKdPn8aYMWOQmprabC3Lli3DrbfeetV1Xmvf1+Pm5obDhw9jxYoVePrpp/H9998jNzcXfn51nzC3sbGBm5sbzp8/39jkqRQGPREp5mqVxYcPH8Z9992HM2fOoKqqCkFBQQCAwYMHIzExEffffz8mTJgAX19fAED//v3Rq1cvAEBISAhGjx4NAIiJicHWrVsBXLt22NnZGQCwdetWLFu2DDt37rzqOq+17+uZPHly439nzpzZ5nNjSgx6IivU0uStpCeffBKJiYm44447sG3bNixYsABA3W9tGjduHDZu3IjBgwdj8+a6r+A0rfbVaDSNtzUaTeOv+7te7fChQ4fw6KOPYtOmTejWrRsA4L333sPHH38MANi4ceNV921jY4Pa2trG17l8+XKz1236S7sb/u7j44Ps7Gz4+vrCYDDg4sWLjftUEq/RE5FirlZZfPHiRfj41FVgNW12PH78OGJiYjB37lz079+/8dcFtsa1aodPnz6NCRMmYOXKlQgPD298/IknnsCBAwdw4MABeHt7X3XfAQEBSElJQWVlJS5cuIBff/212T4bfuXfmjVrMGjQIAB1v7Sk4ZjWrl2LkSNHNntDUAoneiJSTNPKYq1Wiz59+mDBggW455574O7ujpEjR+LkyZMA6n4R+datW6HRaBAVFYVbb70Vv//+ewt7qLN48WI88cQTiI2NhcFgwLBhw/DBBx/gpZdewvnz5zF9+nQAddfNr9aWe7V929vb495770V0dDSCgoIafytXg+LiYsTGxsLe3r7xl6P/3//9Hx544AGEhobCw8MDX3zxReP2gYGBKCkpQVVVFdatW4effvoJen2zBnijabGmWGmsKSbqGDXVFNPVmaKmmIiILBiDnohI5Rj0REQqx6AnIlI5Bj0Rkcox6ImIVI5BT0RWw1xqin/++WfEx8cjJiYG8fHx2LJli1HWcS38whQRWY2GmmJ3d3ds2rQJ06ZNw549e0y2v6Y1xV988QXmzp2LNWvWoHv37tiwYQO8vb1x5MgRjBkzBrm51/pV3B3HiZ6IFMWaYok+ffrA29sbQN23hSsqKlBZWWnEs9wcJ3oia7TpWSD/sHFfs2cMcOtr192ENcV/rin++uuv0bdv32ZlbcbGoCcixbCmuLmjR49i7ty5+Omnn1q1fXsx6ImsUQuTt5KstaY4JycHd911F1asWIGQkJD2n8BW4DV6IlIMa4rraoovXLiAcePG4bXXXsPgwYNbfVzt1aqJXggxFsC7ALQA/iul/NM4IIS4F8ACABLAQSnllPr7awA0XAw8LaW8wwjrJiILxJriupri//znP8jMzMRLL72El156CQDw008/wdPTs30ntgUt1hQLIbQA0gHcAiAHwD4Ak6WUKU22CQPwJYCRUspiIYSnlPJs/WNlUkrn1i6INcVEHcOaYvUzRU3xAACZUsoTUsoqAF8AuPOKbf4K4D0pZTEANIQ8ERF1vtYEvQ+A7Ca3c+rvayocQLgQYpcQ4o/6Sz0NHIQQSfX3J3RsuURE1FbG+tSNDYAwADcB8AXwmxAiRkp5AUCAlDJXCBEMYIsQ4rCU8njTJwshpgGYBgD+/v5GWhIREQGtm+hzAfg1ue1bf19TOQC+k1JWSylPou6afhgASClz6/97AsA2AH2ueC6klB9JKftJKfv16NGjzQdBRETX1pqg3wcgTAgRJISwAzAJwHdXbLMOddM8hBDdUXcp54QQwl0IYd/k/sEAUkBERIpp8dKNlNIghJgBYDPqPl65XEp5VAjxEoAkKeV39Y+NFkKkAKgBMEdKeV4IcSOAD4UQtah7U3mt6ad1iIjI9Fr1hSkp5UYpZbiUMkRKubD+vvn1IQ9ZJ1FKqZdSxkgpv6i/f3f97bj6/y4z3aEQEV2fudQU7927t7E0LS4uDt9++61R1nEtrEAgIqthLjXF0dHRSEpKgo2NDc6cOYO4uDiMHz8eNjamiWRWIBCRolhTLOHk5NQY6pcvX27Wk2MKnOiJrNCivYuQVtT67pjW0HnoMHfA3Otuw5ri/1dTvGfPHjzyyCM4deoUVq5cabJpHmDQE5GCWFP8/wwcOBBHjx5FamoqHnzwQdx6662tekNpDwY9kRVqafJWkrXWFDeIjIyEs7Mzjhw5gn79rlpV02G8Rk9EimFNcV1N8cmTJxvflE6dOoW0tDQEBga2+vjaihM9ESmGNcV1NcU7d+7Ea6+9BltbW2g0GixdurTZrxc0thZripXGmmKijmFNsfqZoqaYiIgsGIOeiEjlGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRFbDXGqKG5w+fRrOzs544403jLKOa2HQE5HVaKgpPnz4MJ5//nlMmzbNpPtrWlM8c+ZMzJ3bvHoiMTHxmsVqxsSgJyJFsaa47kuq69atQ1BQEKKioox4dq+OFQhEVij/X/9CZapxa4rtI3Xo+dxz192GNcV1NcUODg5YtGgRfv75Z5NftgEY9ESkINYU11mwYAFmzpzZuB5TY9ATWaGWJm8lWWNN8Z49e7B27Vo888wzuHDhAjQaDRwcHDBjxowOnctr4TV6IlIMa4rraop37NiBrKwsZGVl4emnn8Zzzz1nspAHONETkYJYU/xFR05fu7WqplgIMRbAuwC0AP4rpXztKtvcC2ABAAngoJRySv39DwL4Z/1mr0gpP7vyuU2xppioY1hTrH5trSlucaIXQmgBvAfgFgA5APYJIb6TUqY02SYMwDwAg6WUxUIIz/r7PQC8AKAf6t4AkuufW9yuoyMiojZrzTX6AQAypZQnpJRVAL4AcOcV2/wVwHsNAS6lPFt//xgAP0spi+of+xnAWOMsnYiIWqM1Qe8DILvJ7Zz6+5oKBxAuhNglhPij/lJPa59LREQmZKwfxtoACANwEwBfAL8JIWJa+2QhxDQA0wDA39/fSEsiIiKgdRN9LgC/Jrd96+9rKgfAd1LKainlSQDpqAv+1jwXUsqPpJT9pJT9evTo0Zb1ExFRC1oT9PsAhAkhgoQQdgAmAfjuim3WoW6ahxCiO+ou5ZwAsBnAaCGEuxDCHcDo+vuIiEghLQa9lNIAYAbqAjoVwJdSyqNCiJeEEHfUb7YZwHkhRAqArQDmSCnPSymLALyMujeLfQBeqr+PiEhx5lJTnJWVBUdHx8bitMcee8wo67iWVl2jl1JuBLDxivvmN/m7BJBY/+fK5y4HsLxjyyQi6riGmmJ3d3ds2rQJ06ZNw549e0y2v6Y1xV988QXmzp3b+A3akJCQxm/smhorEIhIUawpbvlLqsbGCgQiK7Tjy3Scyy4z6mt293PG0HvDr7sNa4rdGnt+Tp48iT59+sDV1RWvvPIKhg4d2uI+2otBT0SKYU1xnV69euH06dPo1q0bkpOTkZCQgKNHj8LV1bXF/bQHg57ICrU0eSvJGmuKhRCN646Pj0dISAjS09PRr99Vq2o6jNfoiUgxrCmuqykuLCxETU0NAODEiRPIyMhAcHBwq4+vrTjRE5FiWFNcV1P822+/Yf78+bC1tYVGo8EHH3wADw+Pdp/XlrSqplhJrCkm6hjWFKtfW2uKeemGiEjlGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRKRyDHoishrmUlMM1H1Dd9CgQYiKikJMTMyfvmlrTPzCFBFZDXOpKTYYDPjLX/6ClStXIi4uDufPn4etra3J1sGJnogUxZpiiZ9++qnxHABAt27doNVqjXiWm+NET2SFtn76Ec6eOmHU1/QMCMaIh6ZddxvWFNfVFKenp0MIgTFjxqCwsBCTJk3CM88805rT3C4MeiJSDGuK6xgMBuzcuRP79u2Dk5MTRo0ahfj4eIwaNarF/bQHg57ICrU0eSvJGmuKfX19MWzYsMY3vNtuuw379+83WdDzGj0RKYY1xSMbL9kcPnwY5eXlMBgM2L59O/R6fauPr6040RORYlhTXFdT7O7ujsTERPTv3x9CCNx2220YN25cu89rS1hTTKQyrClWP5PUFAshxgohjgkhMoUQz17l8YeEEIVCiAP1fx5t8lhNk/u/a+PxEBFRB7V46UYIoQXwHoBbAOQA2CeE+E5KmXLFpmuklDOu8hIVUsreHV4pERG1S2sm+gEAMqWUJ6SUVQC+AHCnaZdFRETG0pqg9wGQ3eR2Tv19V7pbCHFICLFWCOHX5H4HIUSSEOIPIURCB9ZKRETtYKyPV24AECiljAXwM4DPmjwWUP8DgikA3hFChFz5ZCHEtPo3g6TCwkIjLYmIiIDWBX0ugKYTum/9fY2klOellJX1N/8LIL7JY7n1/z0BYBuA5p9JqnvsIyllPyllvx49erTpAIiI6PpaE/T7AIQJIYKEEHYAJgFo9ukZIUSvJjfvAJBaf7+7EMK+/u/dAQwGcOUPcYmIFGEuNcWff/55Y2la7969odFoGr/UZQotfupGSmkQQswAsBmAFsByKeVRIcRLAJKklN8B+LsQ4g4ABgBFAB6qf3okgA+FELWoe1N57Sqf1iEiUoS51BTff//9uP/++wEAhw8fRkJCAnr37m2ydbTqGr2UcqOUMlxKGSKlXFh/3/z6kIeUcp6UMkpKGSelHCGlTKu/f7eUMqb+/hgp5TKTHQkRWQTWFDf/kurq1asxadIkI5zZa2MFApEVurDhOKryLhn1Ne28u6Dr+D991qIZ1hTX1RQ3lJkBdd0469evb/H1O4JBT0SKYU1xc3v27IGTkxOio6NbtX17MeiJrFBLk7eSrLGmuMEXX3zR+AZhSqwpJiLFsKZ4ZOObQG1tLb788kuTX58HONETkYJYU/xF4/a//fYb/Pz8EBwc3K5z2RasKSZSGdYUq59JaoqJiMhyMeiJiFSOQU9EpHIMeiIilWPQExGpHIOeiEjlGPREZDXMpaa4uroaDz74IGJiYhAZGYlXX33VKOu4FgY9EVmNhpriw4cP4/nnn8e0adNMur+mNcUzZ87E3LlzAQBfffUVKisrcfjwYSQnJ+PDDz9sfBMwBQY9ESmKNcUSQghcunQJBoMBFRUVsLOzg6urq3FPdBOsQCCyQps2bUJ+fr5RX7Nnz57XrP1twJriupriiRMnYv369ejVqxfKy8vx9ttvw8PDozWnuV0Y9ESkGNYU19m7dy+0Wi3y8vJQXFyMoUOH4uabbzZZ7w2DnsgKtTR5K8kaa4pXrVqFsWPHwtbWFp6enhg8eDCSkpJMFvS8Rk9EimFNcV1Nsb+/P7Zs2QIAuHTpEv744w/odLpWH19bcaInIsWwpriupviJJ57Aww8/jKioKEgp8fDDDyM2Nrbd57UlrCkmUhnWFKsfa4qJiKiZVgW9EGKsEOKYECJTCPHsVR5/SAhRKIQ4UP/n0SaPPSiEyKj/86AxF09ERC1r8Rq9EEIL4D0AtwDIAbBPCPGdlDLlik3XSClnXPFcDwAvAOgHQAJIrn9usVFWT0RELWrNRD8AQKaU8oSUsgrAFwDubOXrjwHws5SyqD7cfwYwtn1LJSKi9mhN0PsAyG5yO6f+vivdLYQ4JIRYK4Twa+NziYjIRIz1w9gNAAKllLGom9o/a2H7ZoQQ04QQSUKIpMLCQiMtiYiIgNYFfS4Avya3fevvaySlPC+lrKy/+V8A8a19bv3zP5JS9pNS9uvRo0dr105E1CbmUlNcVVWFhx9+GDExMYiLi8O2bduMso5raU3Q7wMQJoQIEkLYAZgE4LumGwghejW5eQeAhvagzQBGCyHchRDuAEbX30dEpDhzqSluqFs4fPgwfv75Z8yaNatZvYKxtRj0UkoDgBmoC+hUAF9KKY8KIV4SQtxRv9nfhRBHhRAHAfwdwEP1zy0C8DLq3iz2AXip/j4islKsKZZISUnByJEjAQCenp7o2rXrVb+hayytqkCQUm4EsPGK++Y3+fs8APOu8dzlAJZ3YI1EZGTp6S+jtCy15Q3bwMU5EuHhz193G9YU19UUx8XF4bvvvsPkyZORnZ2N5ORkZGdnY8CAAa051W3GrhsiUgxrius88sgjSE1NRb9+/RAQEIAbb7wRWq22xX20F4OeyAq1NHkryRprioUQePvttxu3v/HGG5u1aRobu26ISDGsKa6rKS4vL8elS5cAAD///DNsbGyg1+tbfXxtxYmeiBTDmuK6muKzZ89izJgx0Gg08PHxwcqVK9t9TluDNcVEKsOaYvVjTTERETXDoCciUjkGPRGRyjHoiYhUjkFPRKRyDHoiIpVj0BOR1VC6pvi3335D3759YWNjg7Vr1zZ7bOzYsejatWuzojRTYdATkdVQuqbY398fn376KaZMmfKnx+bMmWPyL0o1YNATkaKsqaY4MDAQsbGx0Gj+HLWjRo2Ci4tLx09oK7ACgcgKPZ+RgyNlFUZ9zWhnR7wcdv2WR2urKTYXDHoiUgxrijsHg57ICrU0eStJzTXF5oLX6IlIMdZWU2wuONETkWKsraZ43759uOuuu1BcXIwNGzbghRdewNGjRwEAQ4cORVpaGsrKyuDr64tly5ZhzJgx7T6318OaYiKVYU2x+rGmmIiImmHQExGpHIOeiEjlWhX0QoixQohjQohMIcSz19nubiGEFEL0q78dKISoEEIcqP/zgbEWTkRErdPip26EEFoA7wG4BUAOgH1CiO+klClXbOcC4CkAe654ieNSyt7GWS4REbVVayb6AQAypZQnpJRVAL4AcOdVtnsZwCIAl6/yGBERdZLWBL0PgOwmt3Pq72skhOgLwE9K+cNVnh8khPj/hBDbhRBDr7YDIcQ0IUSSECKpsLCwtWsnImoTc6kpPnDgAAYNGoSoqCjExsY2ftnKVDr8hSkhhAbAWwAeusrDZwD4SynPCyHiAawTQkRJKUuabiSl/AjAR0Dd5+g7uiYioqtpqCl2d3fHpk2bMG3aNOzZc+XVZuNpqCl+4403mt3v5OSEFStWICwsDHl5eYiPj8eYMWPQtWtXk6yjNRN9LgC/Jrd96+9r4AIgGsA2IUQWgBsAfCeE6CelrJRSngcAKWUygOMAwkFEVos1xUB4eDjCwsIAAN7e3vD09IQpr2a0ZqLfByBMCBGEuoCfBKCxRV9KeRFA94bbQohtAGZLKZOEED0AFEkpa4QQwQDCAJww4vqJqB1e3HAUKXklLW/YBnpvV7wwPuq627Cm+M/27t2LqqoqhISEtGr79mgx6KWUBiHEDACbAWgBLJdSHhVCvAQgSUr53XWePgzAS0KIagC1AB6TUhYZY+FEZHlYU9zcmTNn8MADD+Czzz676i8nMZZWXaOXUm4EsPGK++ZfY9ubmvz9awBfd2B9RGQCLU3eSrLWmuKSkhKMGzcOCxcuxA033NDi9h3Bb8YSkWJYU1ynqqoKd911F6ZOnYqJEye2+rjaizXFRKQY1hTX1RR/+eWX+O2333D+/PnGH+h++umn6N27dzvP7PWxpphIZVhTrH6sKSYiomYY9EREKsegJyJSOQY9EZHKMeiJiFSOQU9EpHIMeiKyGuZSU3zq1Cn07dsXvXv3RlRUFD74wLS/fI9fmCIiq2EuNcW9evXC77//Dnt7e5SVlSE6Ohp33HEHvL29TbIOTvREpCjWFAN2dnaNHT2VlZXNOnRMgRM9kTXa9CyQf9i4r9kzBrj1tetuwpri/yc7Oxvjxo1DZmYm/v3vf5tsmgcY9ESkINYU/z9+fn44dOgQ8vLykJCQgIkTJ8LLy6vF57UHg57IGrUweSvJWmuKG3h7eyM6Oho7duwwWZMlr9ETkWJYU1wnJycHFRUVAOpaL3fu3ImIiIhWH19bcaInIsWwpriupjg1NRWzZs2CEAJSSsyePRsxMTEdObXXxZpiIpVhTbH6saaYiIiaYdATEakcg56ISOUY9EREKteqoBdCjBVCHBNCZAohnr3OdncLIaQQol+T++bVP++YEGKMMRZNRESt1+LHK4UQWgDvAbgFQA6AfUKI76SUKVds5wLgKQB7mtynBzAJQBQAbwC/CCHCpZQ1xjsEIiK6ntZM9AMAZEopT0gpqwB8AeDOq2z3MoBFAJp+XexOAF9IKSullCcBZNa/HhGR4sylprhBSUkJfH19MWPGDKOs41paE/Q+ALKb3M6pv6+REKIvAD8p5Q9tfW7986cJIZKEEEmFhYWtWjgRUVs11BQfPnwYzz//PKZNm2bS/TXUFE+ZMuWqjz///PMYNmyYSdcAGOGHsUIIDYC3AMxq72tIKT+SUvaTUvbr0aNHR5dERGaMNcV1kpOTUVBQ0FjKZkqtqUDIBeDX5LZv/X0NXABEA9hWX+TTE8B3Qog7WvFcIuoEi/YuQlpR67tjWkPnocPcAXOvuw1riuvU1tZi1qxZ+N///odffvmlxdfuqNYE/T4AYUKIINSF9CQAjf8OkVJeBNC94bYQYhuA2VLKJCFEBYBVQoi3UPfD2DAAe423fCKyJKwprrN06VLcdtttrXpdY2gx6KWUBiHEDACbAWgBLJdSHhVCvAQgSUr53XWee1QI8SWAFAAGAE/wEzdEna+lyVtJ1lhT/Pvvv2PHjh1YunQpysrKUFVVBWdnZ7z2mmnqo1t1jV5KuVFKGS6lDJFSLqy/b/7VQl5KeZOUMqnJ7YX1z4uQUm4y3tKJyNKwprjO559/jtOnTyMrKwtvvPEGpk6darKQB1hTTEQKYk1xXU2x0lhTTKQyrClWP9YUExFRMwx6IiKVY9ATEakcg56ISOXM7lM3ycnJ54QQpzp7He3QHcCfW43Ujcdshn7++eeYmpoag7Fer6amxkar1Rrt9SyBuR9zfn6+jV6vP3zF3QHX2t7sgl5KaZFlN0KIpGv9xFuteMzm6eDBg1nR0dFGezM6cuRIZHR0dGrLW6qHuR9zTU1N97b83yEv3RCR1Xj//fc9wsPD9eHh4fo+ffrofv/9d0djvK6Pj0/MmTNn/jQ4b9q0yVmv10fa2NjEf/LJJ+5NH9NqtfE6nU6v0+n0I0eODDXGOq7F7CZ6IiJTCQ0Nrdy1a9exHj161Hz55Zeuf/vb3wIOHTpk3Ha3JoKDg6s++eSTrNdee83rysfs7e1r09LSUq72PGPjRG88H3X2AjoBj9kKdO/e3ai/JOI///lPt/DwcH1ERIQ+ISEhaNWqVW6xsbG6yMhI/Y033hienZ1tAwA//PCDc8PEGxkZqS8uLtZ8//33Lv37948YNWpUiK+vb8z06dN93n//fY+YmJjI8PBw/dGjR+0BIC8vz2bMmDEh0dHRkdHR0ZE//fRTFwC45ZZbLvXo0aMGAEaMGHEpPz/f7mprPHr0aMnV9j1ixIjGyXvq1Kn+ixcv7tZw+8UXX+wZHh6uj4mJiTxy5Ig9AERERFQNHDiw4mo1xUriRG8kUkqrCwAes+XKe+4ffpUZGU6t3f4k0K2lbezDwsq9/7Uw+3rbJCUlObzxxhu9fv/997RevXoZCgoKtBqNBpMmTUrTaDR46623ur/00ks9P/7445w333yz5+LFi0+NHj360sWLFzVOTk61AJCWluZ45MiRo56enoaAgIAYe3v7c4cPH059+eWXPd98803P5cuXZ//tb3/zS0xMLBgzZkxZRkaG3ZgxY8JOnDjRrHtgyZIl3UeMGHHxauv88MMPHa+27+txc3MzpKenp/znP//p9uSTT/pt3bo183rbV1VVaaKjoyO1Wq2cPXt2/gMPPHChpX20F4OeiBSzefNm1/Hjxxf36tXLAABeXl41e/fudUxISPAtLCy0raqq0vj5+VUCwA033FA2e/Zsv3vvvbdo8uTJxSEhIbUAEBMTcykgIKAaAPz9/StvvfXWiwAQFxdXsX37dhcA2LVrl2tGRkbj9feysjLtxYsXNW5ubrUAsGHDBpf//e9/3Xfv3n3VyzbX2vf1PPjgg0UA8Ne//rXon//8p19L22dkZBwKCgqqTklJsbvlllsi+vbtWxEVFVXZ0vPag0FPZIVamryVNGPGDP+nnnoq//7777/4/fffu7z00kveAPCvf/0rPyEh4eL69evdhg4dqvvhhx8yAMDe3r6xoEuj0cDBwUE2/L2mpkYAgJQS+/fvT3VycvpTmdeePXscp0+fHvDDDz9k9OzZswYAXn311R6fffZZDwD48ccfM662b1tbW9m0priysrJZF3HTyzNCiBZLxIKCgqoBQK/XV91www2le/fudTJV0PMafRsIITyEED8LITLq/+t+je0erN8mQwjx4FUe/04IccT0K+64jhyzEMJJCPGDECJNCHFUCGG6HtYOEkKMFUIcE0JkCiGevcrj9kKINfWP7xFCBDZ5bF79/ceEEGMUXXgHFBUVuR46dCj60KFD0Tk5OT2vfLy2tlZkZGQEHzp0KPro0aO6y5cv2wFAcXGx65EjRyIPHz6sP3LkSOSFCxdcWrvPMWPGlGzYsME9Pz9fCwAFBQXa0tJSrb+/fzUAfPrpp42XiI4ePWo/YMCAioULF+bHxsZeOnLkSMu/5qnekCFDSl599VXPhtu7d+92BID9+/d7TJw4MfKVV14RHh4ejf+3PG/evMK0tLSUlJSU1Orqar/vv/8+pkuXLgHPP/98UcO+Q0JCKjMyMpz++OOPPkePHu25c+dO16b7XLFihQcALFu2zL1Pnz6Xrre+wsJCbUVFhQCAM2fO2CQlJTnHxsZWtPb42opB3zbPAvhVShkG4Nf6280IITwAvABgIIABAF5oGo5CiAkAypRZrlF09JjfkFLqAPQBMFgIcfXf3daJhBBaAO8BuBWAHsBkIYT+is3+D0CxlDIUwNsAFtU/V4+637oWBWAsgKX1r2fWpJTIycnxDwsLS4+Ojj564cIFj0uXLjUL0oKCgu5ardYQGxt7xNPTsyA7O9sXAGxtbavDwsIyY2JiUoKCgk5mZWUFtXa//fr1uzxr1qwzQ4cO1UVEROinT5/u949//CNv8uTJIVFRUZHdunVr/JLS66+/7hkWFhYVHh6ut7W1lRMnTrzq9fSr+eijj7L379/fJTw8XB8SEhL1n//8p4eUEi+//LL/xYsXa1999dWaUaNGeUdFRUVd7Zi/+uqrC3feeadtTExM475DQ0Orx4wZU3Pvvffir3/9a/eoqKjyps8tLi7WhoeH65cuXeq1ePHibADYvn27k5eXV+zGjRvdZ86cGRAaGhoFAAcOHHCIi4uLjIiI0A8fPjz86aefzo+Pj2/+m0yMyOxqis2ZEOIYgJuklGeEEL0AbJNSRlyxzeT6bf5Wf/vD+u1WCyGcAfwIYBqAL6WU0QofQpt19Jiv2O5dAEeklB8rtPxWEUIMArBASjmm/vY8AJBSvtpkm8312/wuhLABkA+gB+rf+Bq2bbqdskfx/xw8eDArLi7uul+YKikp6ZKXl+et0+kyAKBhovf19c1v2CYtLS3M29s7z9XV9VJtbS0OHjwY17t374NNf3uSlBIHDhzoHRcXd1Cj0Zh1mHT0mM+fP9+1tLTUWavV1mo0mhofH5+CzjqWgwcPdo+Liwts7fac6NvGS0p5pv7v+QD+9NlYAD4Aml7/zKm/DwBeBvAmgPIrn2TGOnrMAAAhRFcA41H3rwJz0+L6m24jpTQAuIi6T6K05rlmp6qqys7W1raq4badnV1VdXV1s48aVldX29nb21cBddeftVptjcFgaPZzvfPnz7s7ODiUm3vIAx07ZoPBoMnPz+/p6+ubp/S6jYE/jL2CEOIXAH+6XgngH01vSClla37g0uR1ewMIkVLObHp91xyY6pibvL4NgNUAFkspT7RvlWRuLl265JCXl+cTFhaW0dlrMbWcnBxvT0/PAhsbmxY/fWOOGPRXkFLefK3HhBAFQoheTS5jnL3KZrkAbmpy2xfANgCDAPQTQmSh7rx7CiG2SSlvQicz4TE3+AhAhpTynY6v1iRyATT9OJxv/X1X2yan/o3LDcD5Vj7X7Fw5zV457QKAra1tVWVlpZ29vX11bW0tampqtDY2NgYAqKystD1+/HhoQEDASUdHR5N8UsTYOnLM5eXlXS5evOiel5fnW1NToxVCQKPR1Pbq1cuoXyYzFV66aZvvADR8iuZBAOuvss1mAKOFEO71P5AcDWCzlPJ9KaW3lDIQwBAA6eYQ8q3Q7mMGACHEK6gLxadNv9R22wcgTAgRJISwQ90PV6/8xfdNz8NEAFtk3Q+4vgMwqf5TOUEAwgDsVWjd7ebs7HypsrLSoaKiwq62tlZcuHDBw93d/ULTbdzc3C6cO3euG1B3icbZ2blUCAGDwaDNyMgI8/b2znFzc7vup0vMSUeOWa/XH4uLizscFxd3uEePHmc9PT3PWErIA5zo2+o1AF8KIf4PwCkA9wKAEKIfgMeklI9KKYuEEC+jLjwA4CUpZVHnLNco2n3MQghf1F3+SQOwv/6HeP+RUv5X8aO4DimlQQgxA3VvTloAy6WUR4UQLwFIklJ+B2AZgJVCiEwARah7M0D9dl8CSAFgAPCElLKmUw6kDTQaDfz8/E5nZGSEA4CHh8e5Ll26XD59+rR3ly5dLnXr1u2ip6fnuePHjwcdOnQoWqvV1gQHBx8HgDNnznhWVlba5+fne+fn53sDQHh4eLqdnZ3Z1voCHTtmS8dP3RCpTGs+dUOWjZ+6ISK6BnOqKc7IyLAbPHhwWHBwcFRISEjUsWPHrlqwZgwMeiKyGg01xenp6Snz5s3L+9vf/nbN38pkDA01xePHjz9/5WP3339/0OzZswtOnDhxdP/+/ane3t4mu/TFoCciRVlCTfG19m2smuLk5GSHmpoa3HXXXSUA4ObmVuvi4mKyj27yh7FEVujXFal+Rbllra4pbg0PH+fyUVMjVVFTfK19X09baopTUlIcXF1da0aPHh2SnZ1tP2zYsJL33nsvx8bGNJHMoCcixbCmuI7BYBBJSUnOe/bsSQkLC6u6/fbbQ5YsWdJ95syZJvkhOoOeVE0IUSaldK7/NvKNUspVRnzt56SU/2pye7eU8kZjvb4ptTR5K8kaa4r9/f2rdDpdhV6vrwKAO+64o/iPP/5wbtuZaz1eoydrEQhgSlueUP8N2Ot5rukNSwn5ztTZNcUZGRl299xzT8jy5ctPxsbGNn6jt6GmOC0tLSUwMLD6avsOCQmpzMzMdKyoqBDnzp3TdqSmePjw4ZdKSkq0eXl5NgCwdetWV71eb7KaYk70ZC1eAxAphDgA4DMAi+vvuwmAPYD3pJQfCiFuQl35XDEAHYBwIcQ61NUcOAB4V0r5UX23vmP96x2VUt7f5F8PAsDrqKs9lgBekVKuqX/tBQDOAYgGkAzgL9KKvszStKZYo9HI6Ojo8oaaYjc3N8OQIUNKT58+bQ/U1RTv3r3bVQghIyIiKiZOnHjx119/bdXU+9FHH2U/+uij/uHh4fqamhoxcODA0htvvPH0P//5z14XLlywefLJJwMAwMbGRh45ciT1yudfbd+Ojo5y/PjxxTqdLsrX17fyWjXFdnZ28osvvjgB1NUU33vvvaElJSXaX3/9tevChQu9MzMzj9rY2OC1117Luemmm8IBICYmptxUl20AfmGKVK5J+N4EYLaU8vb6+6cB8JRSviKEsAewC8A9AAIA/AAgWkp5sn5bj/pv+jqi7tu/w6WU5xte+yr7uhvAY6jrp+9e/5yBACJQVyERBSCvfp9zpJQ7jXnM/MKU+vELU0StMxrA1PqJfA/qKofD6h/b2xDy9f4uhDgI4A/UTfZhuL4hAFZLKWuklAUAtgPo3+S1c6SUtQAOoO6SEpFJ8dINWSsB4Ekp5eZmd9ZN/peuuH0zgEFSynIhxDbUXcJpr6ZNjzXg/w+SAjjRk7UoBdD0d5tuBvC4EMIWAIQQ4UKILld5nhvqfoVguRBCB+CGJo9VNzz/CjsA3CeE0AohegAYBgtotCT14jRB1uIQgJr6SzCfAngXdZdN9tf/8LQQQMJVnvcjgMeEEKkAjqHu8k2DjwAcEkLsl1Le3+T+b1H3+wcOou6Hsc9IKfPr3yiIFMcfxhKpDH8Yq378YSwRETXDoCciq2EuNcUbNmxwaShN0+l0ent7+74rV67saoy1XA2DnoishrnUFI8fP7604Zu427dvP+bg4FCbkJBQYqp1MOiJSFGsKW5u5cqV7sOHD7/ImmIiMqrN77/jdy77lFFrirv7BZSPefxp1hS3oqa4qbVr13o89dRTBa3Ztr0Y9ESkGNYUN3fq1CnbY8eOOU6YMMFkl20ABj2RVWpp8laSNdYUN1ixYoX72LFjLzQ9JlPgNXoiUgxriptbu3atx5QpU4pae1ztxYmeiBTDmuK6mmIAOHbsmN2ZM2fsbrvtttKOnteW8JuxRCrDb8aqH78ZS0REzTDoiYhUjkFPRKRyDHoiIpVj0BMRqRyDnohI5Rj0RGQ1zKWmGAAef/xxn7CwsKiwsLCojz/+2P3K5xoTg56IrIa51BR/8cUXbgcPHnRKSUk5mpycnPruu+/2LCoqMlkeM+iJSFGsKQaOHj3qMHjw4DJbW1u4urrW6vX68m+++cbNyKe6ESsQiKxQ0dp0v+r8S0atKbbt2aXcY2I4a4pbUVPcp0+fildeecW7tLS0oKysTLN7927XyMjIyy3to70Y9ESkGNYU15kwYULJnj17nPr376/z8PCo7tu3b5lWqzVZHw2DnsgKtTR5K8laa4oXLVqUv2jRonwAGD9+fFBERERlS89pL16jJyLFsKa4jsFgQMM52LNnj2NaWprThAkTrnoZyRg40RORYlhTXFdTXFVVJQYPHqwDAGdn55rPPvvshK2tbUdP7zWxpphIZVhTrH6sKSYiomYY9EREKsegJyJSOQY9EZHKMeiJiFSOQU9EpHIMeiKyGkrXFC9YsMArJCQkKjw8XD9o0KDw9PT0xhK1JUuWdAsICIgOCAiIXrJkSbcrn2tMDHoishpK1xTHx8eXHzhwIDU9PT0lISGheObMmb5A3TeCFy1a5L13797UpKSk1EWLFnkXFhZqTbUOBj0RKcqaaorHjx9f6uLiUgsAQ4YMKTtz5owdAKxbt85t2LBhJV5eXjU9evSoGTZsWAlrionIqNatW+d39uxZo9YUe3p6lickJLCm+Bo1xR9++GGPm2+++SIA5Obm2vr6+lY1PObj41OVm5trsg4EBj0RKcZaa4qXLl3qcfDgQacPP/zwWNvPWscx6ImsUEuTt5LUXlO8bt06lzfeeKPXjh07jjk6OkoA8PHxqW54UwKA3Nxcu+HDh5d25DxeD6/RE5FirK2meNeuXY5PPvlkwPr16zN9fHwMDdsmJCRc3L59u2thYaG2sLBQu337dteEhATWFBOR5bO2muI5c+b4lZeXa++5554QAPD29q7asmVLppeXV82cOXPy4uPjIwHgmWeeyfPy8qrp2Nm9NtYUE6kMa4rVjzXFRETUDIOeiEjlGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRFbDnGqKhw4dGubi4tK7aVGaqTDoichqmEtNMQDMnj07/8MPPzxpyv03YNATkaJYU1znzjvvLHV1dW2xLM0YWIFAZIVSUuf6XSpLN2pNcRfn8HJ95CLWFLeiplhpDHoiUgxrillTTEQKaWnyVpI11hQrjdfoiUgxrCnuHJzoiUgxrCmuqykGgPj4+IgTJ044VFRUaL28vGKXLl2adffdd5d07AxfHWuKiVSGNcXqx5piIiJqhkFPRKRyDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIqthLjXFu3fvduzdu7cuNDQ0Kjw8XP/xxx+7G2Md18KgJyKrYS41xc7OzrUrV648mZmZefSnn37KeO655/zOnTunNdU6GPREpCjWFAOxsbGVMTExlQAQGBhY7eHhYbjavwiMhRUIRFbo6dTTfmmXLhu1pljXxaH8nUh/1hS3saZ469atTtXV1UKv11de+ZixMOiJSDGsKW5eU3zq1Cnbhx9+OHjZsmUntVqTXblh0BNZo5YmbyVZa01xUVGR5tZbbw194YUXckeNGnWpY2fx+niNnogUw5riOpcvXxbjxo0LnTRp0vmHH364uK3nsa040RORYlhTXFdTvHz5cvd9+/Y5FxcX26xatao7ACxfvvzkjTfeWNGxM3x1rCkmUhnWFKsfa4qJiKgZBj0Rkcox6ImIVI5BT0Skcgx6IiKVY9ATEakcg56IrIa51BSnp6fb6fX6SJ1Opw8NDY16/fXXexhjHdfCoCciq2EuNcX+/v7VycnJaWlpaSnJycmp7777bs+srCxbU62DQU9EimJNMeDg4CAbem8qKipE0w4dU2AFApEVmrP2oF96fqlRa4rDe7qU/3tiHGuKW1lTnJmZaXvbbbeFZWdn28+fPz8nMDCwuqV9tBeDnogUw5ri/1dTHBoaWp2enp6SlZVlO378+NC//OUvxX5+foYrX9cYGPREVqilyVtJ1lpT3CAwMLBap9NV/PLLLy6marLkNXoiUgxriuscP37ctqysTABAYWGhdt++fc5RUVGX23Y2W48TPREphjXFdTXFhw4dcpw7d66vEAJSSsyYMSN/wIABJqkoBlhTTKQ6rClWP9YUExFRMwx6IiKVY9ATEakcg56ISOUY9EREKsegJyJSOQY9EVkNc6kpblBUVKTx8vKKnTp1qr8x1nEtDHoishrmUlPcYNasWT4DBgwoNeUaAAY9ESmMNcV1duzY4VRYWGh7yy23lBj/LDfHCgQia7TuCT+cTTFqTTE89eVIeI81xa2oKa6pqcGsWbP8Vq9efeKHH35wvforGg+DnogUw5riupriRYsW9Rg9evSFkJAQk3XQN8WgJ7JGLUzeSrLGmuI//vjDed++fc6ffPKJZ3l5uaa6ulrj7Oxcs3Tp0tyOn9E/4zV6IlIMa4rrfPfddyfPnDlzODc39/CLL76YM2HChPOmCnmAEz0RKYg1xXU1xR09j23FmmIilWFNsfqxppiIiJph0BMRqRyDnohI5Rj0REQqx6AnIlI5Bj0Rkcox6InIaphTTbFWq41vKE4bOXJk6JXPNSYGPRFZDXOqKba3t69t+Dauqb9ExaAnIkWxplh5rEAgskLP73reL7M406g1xaHuoeUvD36ZNcWtqCkGgKqqKk10dHSkVquVs2fPzn/ggQcutLSP9mLQE5FiWFNcV1MMABkZGYeCgoKqU1JS7G655ZaIvn37VkRFRVVe+brGwKAnskItTd5KssaaYgAICgqqBgC9Xl91ww03lO7du9fJVEHPa/REpBjWFNcpLCzUVlRUCAA4c+aMTVJSknNsbGxF285m63GiJyLFsKa4rqb4wIEDDk888USAEAJSSjz99NP58fHxlzt6fq+FNcVEKsOaYvVjTTERETXDoCciUjkGPRGRyjHoiYhUjkFPRKRyDHoiIpVj0BOR1TCnmuKMjAy7wYMHhwUHB0eFhIREHTt2zGSFZwx6IrIa5lRTfP/99wfNnj274MSJE0f379+f6u3tbbjea3UEg56IFMWaYiA5OdmhpqYGd911VwkAuLm51TZsZwqsQCCyQnnP/cOvMiPDqDXF9mFh5d7/Wsia4lbUFKekpDi4urrWjB49OiQ7O9t+2LBhJe+9916OjY1pIplBT0SKYU1xXU2xwWAQSUlJznv27EkJCwuruv3220OWLFnSfebMmSaprmDQE1mhliZvJVljTbG/v3+VTqer0Ov1VQBwxx13FP/xxx+tKmxrD16jJyLFsKa4zvDhwy+VlJRo8/LybABg69atrnq9njXFRGT5WFNcV1NsY2OD1157Leemm24KB4CYmJhyU122AVhTTKQ6rClWP9YUExFRMwx6IiKVY9ATEakcg56ISOUY9EREKsegJyJSOQY9EVmdtLQ0u9jYWJ2/v3/0uHHjgi9fviwAoKKiQowbNy7Y398/OjY2Vte0OnjPnj2OvXv31oWGhkaFh4fry8vLxbX3YF4Y9ERkdRITE31nzJhRcPr06SNubm6Gd999tzsAvPvuu93d3NwMp0+fPjJjxoyCxMREXwCorq7GAw88EPT++++fyszMPPrbb78ds7Ozs5gvITHoiUgxJSUlmptuuik0IiJCHxYWFvXxxx+7z549u1d0dHRkWFhY1OTJkwMa+mQGDBgQ8X//939+0dHRkcHBwVHbt293Gj16dEhAQED03//+d28AOHbsmF1QUFDU3XffHRgYGBh9xx13BK1bt86lb9++uoCAgOitW7f+qaGztrYWv//+u8vDDz9cDACPPPLI+Q0bNnQFgO+//77rI488ch4AHn744eLdu3e71NbW4ptvvnGLjIysGDRoUAUA9OzZs8ZUTZOmYDkrJSKj+XVFql9RbplRa4o9fJzLR02NvG5Z2jfffOPas2fP6m3btmUCwPnz57UGg6HkjTfeOAMACQkJQV988YXblClTLgKAnZ1d7ZEjR1Jffvllz3vuuSd03759qZ6enobAwMCY5557rgAAsrOzHdasWXMiPj4+KzY2NvLzzz/vlpSUlLZq1aquCxcu7DVixIjjTddQUFBg4+LiUmNrawsACAwMrCooKLCrf8wuKCioCgBsbW3h7OxcU1BQYHPs2DF7IQSGDBkSVlRUZDNhwoSiV155pcCY58+UONETkWL69u1bsWPHDtfHH3/c58cff3Tu1q1bzaZNm1xiY2N14eHh+t27d7scOXKksV74rrvuugDUVRCHhoZWBAQEVDs6Oko/P7/KEydO2AGAj49P5YABAyq0Wi3Cw8MrRo4cWaLRaNC3b9/ynJwce2Os22AwiH379jl/9dVXJ/fs2XPs+++/d1+/fr2LMV5bCZzoiaxQS5O3qcTGxlbu378/5euvv3Z7/vnnfX755ZeSTz75xHPPnj0poaGh1YmJid6XL19uHECbVhBfWU9sMBgEADS9Vt60tlir1TbWFg8ZMiTs3LlztnFxcZdWr159qrS0VFtdXQ1bW1tkZWXZeXl5VQGAl5dX1cmTJ+1CQkKqq6urUVZWpvXy8jL4+vpWDRw4sLShR/+WW265mJSU5HTnnXeWKnHeOooTPREpJisry9bFxaV2+vTpRYmJifkHDhxwAoCePXsaLl68qNmwYYO7Kfa7c+fOjLS0tJQ1a9ac0mg0uOGGG0o/+eQTdwBYvnx5t9tvv/0CAIwbN+7C8uXLuwHAJ5984j5o0KBSjUaDu+66qyQtLc2xtLRUU11djV27drlERUVdNsVaTYETPREpJjk52XHevHm+Go0GNjY2cunSpafWrl3bNTIyMqpHjx6GuLi4S0qs480338y57777Ql555RWfqKio8qeeeuocADz11FPn7r777iB/f/9oNze3mjVr1hwHgB49etTMmDGjoE+fPpFCCIwaNeripEmTrvprCM0Ra4qJVIY1xerHmmIiImqGQU9EpHIMeiIilWPQExGpHIOeiEjlGPRERCrHoCciq9PWmuL333/fQ6fT6Rv+aDSa+N27dztefy/mg0FPRFanrTXFjz/+eFFaWlpKWlpayooVK076+PhU3njjjRWdexStx6AnIsVYak1xUytWrPBISEgoNumJMjJWIBBZoc3vv+N3LvuUUWuKu/sFlI95/GlV1hQ3lJkBwPr1692/+eabTKOdOAVwoicixVhqTXGDLVu2dHF0dKzt37+/xRSaAZzoiaxSS5O3qVhqTXHD63/++eceEyZMKDL1eTI2TvREpBhLrSkGgJqaGmzYsMF96tSpFhf0nOiJSDGWWlMMAJs2bXLp1atXlV6vr1JijcbEmmIilWFNsfqxppiIiJph0BMRqRyDnohI5Rj0REQqx6AnIlI5Bj0Rkcox6InI6rS1pvjy5cti4sSJgeHh4fqIiAj9999/79K5R9A2DHoisjptrSl+++23uwNAenp6ypYtW9Lnzp3rW1NT05mH0CYMeiJSjKXWFKekpDiOGDGiBAB8fHwMrq6uNb/99ptR2z9NiRUIRFaoaG26X3X+JaMGlW3PLuUeE8NVWVMcFxdX/v3333edNm1a0fHjx+2OHDnidOrUKTsA5cY8h6bCiZ6IFGOpNcVPPfXUOW9v7+qYmBj9E0884de3b98yrVZrjJdWBCd6IivU0uRtKpZaU6zRaLBs2bLGc9anTx+dXq+3mE56TvREpBhLrSkuLS3VlJSUaADg22+/ddVqtTI+Pt5igp4TPREpxlJrivPy8mzGjBkTrtFoZM+ePatXrVp1Uol1GgtriolUhjXF6seaYiIiaoZBT0Skcgx6IiKVY9ATEakcg56ISOUY9EREKsegJyKrc62a4k2bNjnr9fpIGxub+IYvVDVYsmRJt4CAgOiAgIDoJUuWdOuclbcPg56IrM61aoqDg4OrPvnkk6zx48efb7p9QUGBdtGiRd579+5NTUpKSl20aJF3YWGhxZTdMOiJSDHmXlMcERFRNXDgwAqNpnk0rlu3zm3YsGElXl5eNT169KgZNmxYyTfffONm4tNlNKxAILJC69at8zt79qxRa4o9PT3LExISLLqm+Fpyc3NtfX19qxpu+/j4VOXm5tq24zR1Ck70RKQYS60ptnSc6ImsUEuTt6mYe03xtfj4+FRv37698ffE5ubm2g0fPrzUWOfF1DjRE5FizL2m+FoSEhIubt++3bWwsFBbWFio3b59u2tCQsJFU6zVFDjRE5FizL2mePv27U733ntvaElJifbXX3/tunDhQu/MzMyjXl5eNXPmzMmLj4+PBIBnnnkmz8vLy2J+OzhriolUhjXF6seaYiIiaoZBT0Skcgx6IiKVY9ATEakcg56ISOUY9EREKsegJyKr056a4qFDh4a5uLj0HjFiRGjnrLr9GPREZHXaWlMMALNnz87/8MMPTyq/2o5j0BORYiy1phgA7rzzzlJXV9daE54ek2EFApEVSkmd63epLN2oNcVdnMPL9ZGLVFlTbOk40RORYlhT3Dk40RNZoZYmb1Ox1JpiS8egJyLFZGVl2Xp6ehqmT59e5O7uXrNs2bLuQPOa4vHjxxcbe787d+7MaHq7oaZ42rRpxa2pKbZ0DHoiUoyl1hQDQHx8fMSJEyccKioqtF5eXrFLly7Nuvvuu0uUWG9HsaaYSGVYU6x+rCkmIqJmGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRKRyDHoisjptrSnevXu3Y+/evXWhoaFR4eHh+o8//tj92q9ufhj0RGR12lpT7OzsXLty5cqTmZmZR3/66aeM5557zu/cuXPazll92zHoiUgxllpTHBsbWxkTE1MJAIGBgdUeHh6GM2fOWEyzgMUslIiM5+nU035ply4btaZY18Wh/J1If9XXFG/dutWpurpa6PX6yjadoE7EiZ6IFGPpNcWnTp2yffjhh4M//vjjLK3WYq7ccKInskYtTd6mYsk1xUVFRZpbb7019IUXXsgdNWqUIuVrxsKgJyLFWGpN8eXLl8W4ceNCJ02adL7h2r4l4aUbIlJMcnKyY+/evSN1Op1+4cKF3vPnzz9z//33F0ZGRkaNGDEiXMma4iVLlvT09/ePLi4utmlaU+zl5RW7ceNG95kzZwaEhoZGAcDy5cvd9+3b57xq1aruOp1Or9Pp9Lt373a8/l7MB2uKiVSGNcXqx5piIiJqhkFPRKRyDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIqvT1pri9PR0O71eH6nT6fShoaFRr7/+eo/OW33bMeiJyOq0tabY39+/Ojk5OS0tLS0lOTk59d133+2ZlZVl2zmrbzsGPREpxlJrih0cHKSjo6MEgIqKCtGwRkvBrhsiKzRn7UG/9PxSo9YUh/d0Kf/3xDjV1hRnZmba3nbbbWHZ2dn28+fPzwkMDKxu14nqBJzoiUgxllxTHBoaWp2enp6Smpp6ZNWqVd2zs7MtZlC2mIUSkfG0NHmbiiXXFDcIDAys1ul0Fb/88ouLpTRZcqInIsVkZWXZuri41E6fPr0oMTEx/8CBA05A85piU+x3586dGWlpaSlr1qw5pdFoGmuKAaA1NcXHjx+3LSsrEwBQWFio3bdvn3NUVNRlU6zVFDjRE5FikpOTHefNm+er0WhgY2Mjly5demrt2rVdIyMjo3r06GFQsqb4vvvuC3nllVd8oqKiypvWFN97772hJSUl2l9//bXrwoULvTMzM48eOnTIce7cub5CCEgpMWPGjPwBAwZUKLFWY2BNMZHKsKZY/VhTTEREzTDoiYhUjkFPRKRyDHoiIpVj0BMRqRyDnohI5Rj0RGR12lpT3KCoqEjj5eUVO3XqVH/lV91+DHoisjptrSluMGvWLJ8BAwaUKrvajmPQE5FiLLWmGAB27NjhVFhYaHvLLbeUmPAUmQQrEIis0bon/HA2xag1xfDUlyPhPVXWFNfU1GDWrFl+q1evPvHDDz+4tvscdRJO9ESkGEutKV60aFGP0aNHXwgJCbGYDvqmONETWaMWJm9TsdSa4j/++MN53759zp988olneXm5prq6WuPs7FyzdOnSXOOeIdNg0BORYrKysmw9PT0N06dPL3J3d69ZtmxZd6B5TfH48eON3vG+c+fOjKa3G2qKp02bVtyamuLvvvvuZMPfFy9e3C0pKamLpYQ8wKAnIgVZak2xEmsyJdYUE6kMa4rVjzXFRETUDIOeiEjlGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRFanPTXFWq02XqfT6XU6nX7kyJGhnbPy9mHQE5HVaU9Nsb29fW1aWlpKWlpaypYtWzKVX3X7MeiJSDGWXFNsyViBQGSFnt/1vF9mcaZRa4pD3UPLXx78siprigGgqqpKEx0dHanVauXs2bPzH3jggQvtOU+dQV1vW0Rk1iy1phgAMjIyDh05ciR19erVJ5599lm/o0ePGu21TY0TPZEVamnyNhVLrSkGgKCgoGoA0Ov1VTfccEPp3r17naKioiqNdW5MiRM9ESkmKyvL1sXFpXb69OlFiYmJ+QcOHHACmtcUm2K/O3fuzEhLS0tZs2bNKY1G01hTDACtqSkuLCzUVlRUCAA4c+aMTVJSknNsbGyFKdZqCpzoiUgxllpTfODAAYcnnngiQAgBKSWefvrp/Pj4+MtKrNUYWFNMpDKsKVY/1hQTEVEzDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIlI5Bj0RWZ321BRnZGTYDR48OCw4ODgqJCQk6tixYy3245gLBj0RWZ321BTff//9QbNnzy44ceLE0f3796d6e3sblF95+zDoiUgxllpTnJyc7FBTU4O77rqrBADc3NxqXVxcak16soyIFQhEVijvuX/4VWZkGLWm2D4srNz7XwtVWVOckpLi4OrqWjN69OiQ7Oxs+2HDhpW89957OTY2lhGhnOiJSDGWWlNsMBhEUlKS8zvvvJN96NChlKysLPslS5Z0N8ZrK8Ey3o6IyKhamrxNxVJriv39/at0Ol2FXq+vAoA77rij+I8//nA25rkxJU70RKQYS60pHj58+KWSkhJtXl6eDQBs3brVVa/Xs6aYiOhKllpTbGNjg9deey3npptuCgeAmJiY8pkzZ1pMQyhriolUhjXF6seaYiIiaoZBT0Skcgx6IiKVY9ATEakcg56ISOUY9EREKsegJyKr09aa4g0bNrjodDp9wx97e/u+K1eu7NppB9BGDHoisjptrSkeP358aVpaWkpaWlrK9u3bjzk4ONQmJCSUdM7q245BT0SKsdSa4qZWrlzpPnz48IusKSYis/brilS/otwyo9YUe/g4l4+aGqnKmuKm1q5d6/HUU08VtOnkdDJO9ESkGEutKW5w6tQp22PHjjlOmDDBYi7bAJzoiaxSS5O3qVhqTXGDFStWuI8dO/ZC07VYAk70RKQYS60pbrB27VqPKVOmFJlijabEiZ6IFGOpNcVA3Q9+z5w5Y3fbbbeVKrFGY2JNMZHKsKZY/VhTTEREzTDoiYhUjkFPRKRyDHoiIpVj0BMRqRyDnohI5Rj0RGR12lpTDACPPfaYb2hoaFRwcHDUQw895NdQvmYJGPREZHXaWlP8888/d9m7d69zWlra0fT09KMHDhzosnHjRpfOWX3bMeiJSDGWWlMshEBlZaW4fPmyqKio0BgMBuHt7V1t4tNlNKxAILJCm99/x+9c9imj1hR39wsoH/P406qsKb755psvDR48uLRXr15xAPDQQw8V9u3b93I7T5XiONETkWIstab4yJEj9unp6Q45OTmHcnJyDu3YscPlxx9/dDbGayuBEz2RFWpp8jYVS60pXrNmTdf+/ftfcnNzqwWAm2+++eLOnTu7jB07tsyY58dUONETkWIstabY39+/ateuXS7V1dWorKwUu3btctHr9RZz6YYTPREpxlJrih9++OHirVu3ukZEREQJITBixIiLDT9HsASsKSZSGdYUqx9riomIqBkGPRGRyjHoiYhUjkFPRKRyDHoiIpVj0BMRqRyDnoisTntqih9//HGfsLCwqIYyts5Zefsw6InI6rS1pviLL75wO3jwoFNKSsrR5OTk1HfffbdnUVGRxeSnxSyUiCyfpdYUHz161GHw4MFltra2cHV1rdXr9eXffPONm4lPl9GwAoHIChWtTferzr9k1Jpi255dyj0mhquyprhPnz4Vr7zyindpaWlBWVmZZvfu3a6RkZEW03XDiZ6IFGOpNcUTJkwoueWWWy70799fd/fddwf17du3TKvVWkx/DCd6IivU0uRtKpZaUwwAixYtyl+0aFE+AIwfPz4oIiKi0ljnxdQ40RORYiy1pthgMCA/P18LAHv27HFMS0tzmjBhgsW0V3KiJyLFWGpNcVVVlRg8eLAOAJydnWs+++yzEw3X+C0Ba4qJVIY1xerHmmIiImqGQU9EpHIMeiIilWPQExGpHIOeiEjlGPRERCrHoCciq3OtmuIFCxZ4hYSERIWHh+sHDRoUnp6e3tiBs2TJkm4BAQHRAQEB0UuWLOnWeatvOwY9EVmda9UUx8fHlx84cCA1PT09JSEhoXjmzJm+AFBQUKBdtGiR9969e1OTkpJSFy1a5F1YWKjt3KNoPQY9ESnG3GuKx48fX+ri4lILAEOGDCk7c+aMHQCsW7fObdiwYSVeXl41PXr0qBk2bFgJa4qJyKytW7fO7+zZs0atKfb09CxPSEhQTU3xhx9+2OPmm2++CAC5ubm2vr6+jcVnPj4+Vbm5uRbTgcCJnogUYyk1xUuXLvU4ePCg04svvphvlAPvZJzoiaxQS5O3qVhCTfG6detc3njjjV47duw45ujoKAHAx8enevv27S4N2+Tm5toNHz681HRnyrg40RORYsy9pnjXrl2OTz75ZMD69eszfXx8DA3PT0hIuLh9+3bXwsJCbWFhoXb79u2uCQkJrCkmIrqSudcUz5kzx6+8vFx7zz33hACAt7d31ZYtWzK9vLxq5syZkxcfHx8JAM8880yel5dXjRJrNQbWFBOpDGuK1Y81xURE1AyDnohI5Rj0REQqx6AnIlI5Bj0Rkcox6ImIVI5BT0RWpz01xUOHDg1zcXHpPWLEiNDOW3n7MOiJyOq0taYYAGbPnp3/4Ycfnuy8Vbcfg56IFGOpNcUAcOedd5a6urrWmv4sGR8rEIisUErqXL9LZelGrSnu4hxero9cpMqaYkvHiZ6IFMOa4s7BiZ7ICrU0eZuKpdYUWzoGPREpJisry9bT09Mwffr0Ind395ply5Z1B5rXFI8fP77Y2PvduXNnRtPbDTXF06ZNK75aTfHGjRszmtYUWzoGPREpxlJrigEgPj4+4sSJEw4VFRVaLy+v2KVLl2bdfffdJUqst6NYU0ykMqwpVj/WFBMRUTMMeiIilWPQExGpHIOeiEjlGPRERCrHoCciUjkGPRFZnbbWFO/evduxd+/eutDQ0Kjw8HD9xx9/7N65R9A2DHoisjptrSl2dnauXbly5cnMzMyjP/30U8Zzzz3nd+7cOW3nHkXrMeiJSDGWWlMcGxtbGRMTUwkAgYGB1R4eHoYzZ85YTLOAxSyUiIzn6dTTfmmXLhu1pljXxaH8nUh/1dcUb9261am6ulro9frKdp4qxXGiJyLFWHpN8alTp2wffvjh4I8//jhLq7WYKzec6ImsUUuTt6lYck1xUVGR5tZbbw194YUXckeNGqVI+ZqxMOiJSDGWWlN8+fJlMW7cuNBJkyadb7i2b0kY9ESkGEutKV6+fLn7vn37nIuLi21WrVrVHQCWL19+8sYbb6xQYr0dxZpiIpVhTbH6saaYiIiaYdATEakcg56ISOUY9EREKsegJyJSOQY9EZHKMeiJyOq0taY4PT3dTq/XR+p0On1oaGjU66+/3qNzj6BtGPREZHXaWlPs7+9fnZycnJaWlpaSnJyc+u677/bMysqy7dyjaD0GPREpxlJrih0cHGRD701FRYVoWKOlYAUCkRWas/agX3p+qVFrisN7upT/e2KcamuKMzMzbW+77baw7Oxs+/nz5+cEBgZWd/CUKYYTPREpxpJrikNDQ6vT09NTUlNTj6xatap7dna2xQzKFrNQIjKeliZvU7HkmuIGgYGB1TqdruKXX35xsZQmS070RKSYrKwsWxcXl9rp06cXJSYm5h84cMAJaF5TbIr97ty5MyMtLS1lzZo1pzQaTWNNMQBcraZ4/fr1mU1rio8fP25bVlYmAKCwsFC7b98+56ioqMumWKspcKInIsVYak3xoUOHHOfOnesrhICUEjNmzMgfMGCARVQUA6wpJlId1hSrH2uKiYioGQY9EZHKMeiJiFSOQU9EpHIMeiIilWPQExGpHIOeiKxOW2uKGxQVFWm8vLxip06d6t85K28fBj0RWZ221hQ3mDVrls+AAQNKO2fV7cegJyLFWGpNMQDs2LHDqbCw0PaWW24pMf2ZMi5WIBBZo3VP+OFsilFriuGpL0fCe6qsKa6pqcGsWbP8Vq9efeKHH35wNcLZUhQneiJSjKXWFC9atKjH6NGjL4SEhFhMB31TnOiJrFELk7epWGpN8R9//OG8b98+508++cSzvLxcU11drXF2dq5ZunRprqnPmTEw6IlIMVlZWbaenp6G6dOnF7m7u9csW7asO9C8pnj8+PFG73jfuXNnRtPbDTXF06ZNK75aTfHGjRszmtYUf/fddycb/r548eJuSUlJXSwl5AEGPREpyFJripVYkymxpphIZVhTrH6sKSYiomYY9EREKsegJyJSOQY9EZHKMeiJiFSOQU9EpHIMeiKyOu2pKc7IyLAbPHhwWHBwcFRISEjUsWPH/tSPY64Y9ERkddpTU3z//fcHzZ49u+DEiRNH9+/fn+rt7W249h7MC4OeiBRjqTXFycnJDjU1NbjrrrtKAMDNza22YTtLwAoEIiv0/K7n/TKLM41aUxzqHlr+8uCXVVlTnJKS4uDq6lozevTokOzsbPthw4aVvPfeezk2NpYRoZzoiUgxllpTbDAYRFJSkvM777yTfejQoZSsrCz7JUuWdDfCKVGEZbwdEZFRtTR5m4ql1hT7+/tX6XS6Cr1eXwUAd9xxR/Eff/zhbNqzZTyc6IlIMVlZWbYuLi6106dPL0pMTMw/cOCAE9C8ptgU+925c2dGWlpaypo1a05pNJrGmmIAuFpN8fr16zOb1hQPHz78UklJiTYvL88GALZu3eqq1+srTLFWU+BET0SKsdSaYhsbG7z22ms5N910UzgAxMTElM+cOdNiGkJZU0ykMqwpVj/WFBMRUTMMeiIilWPQExGpHIOeiEjlGPRERCrHoCciUjkGPRFZnfbUFD/22GO+oaGhUcHBwVEPPfSQX0P5miVg0BOR1WlrTfHPP//cZe/evc5paWlH09PTjx44cKDLxo0bXTr3KFqPQU9EirHUmmIhBCorK8Xly5dFRUWFxmAwCG9v72plzlrHsQKByArlPfcPv8qMDKPWFNuHhZV7/2uhKmuKb7755kuDBw8u7dWrVxwAPPTQQ4V9+/a93OGTphBO9ESkGEutKT5y5Ih9enq6Q05OzqGcnJxDO3bscPnxxx8tpr2SEz2RFWpp8jYVS60pXrNmTdf+/ftfcnNzqwWAm2+++eLOnTu7jB07tsy0Z8w4ONETkWIstabY39+/ateuXS7V1dWorKwUu3btctHr9RZz6YYTPREpxlJrih9++OHirVu3ukZEREQJITBixIiLDT9HsASsKSZSGdYUqx9riomIqBkGPRGRyjHoiYhUjkFPRKRyDHoiIpVj0BMRqRyDnoiszrVqil9//fUe4eHhep1Op4+Pj49ITk52aHjOvHnzevr7+0cHBgZGf/31166dt/q2Y9ATkdW5Vk3xo48+ej49PT0lLS0tJTExMf/pp5/2A4Dk5GSHb775xuPYsWNHf/zxx/Snn37a32AwXH8nZoRBT0SKMfeaYg8Pj8bfJlJWVqYVQgAA1q5d23XChAlFjo6OUqfTVQUEBFRu27ati8lPmJGwAoHICv26ItWvKLfMqDXFHj7O5aOmRlp8TfGrr77aY+nSpV7V1dWan3/++RgA5Obm2t1www2NBWbe3t5V2dnZdgAUqWzoKE70RKQYS6gpnjdvXmF2dvaRBQsW5Lzwwgu9jHbwnYgTPZEVamnyNhVLqClu8Ne//rVozpw5/gDg4+PTMMEDAPLy8uz8/Pz+9BxzxYmeiBRj7jXFhw8fbvwXwJo1a9wCAgIqAeDuu+++8M0333hUVFSItLQ0u6ysLIebbrrJIi7bAJzoiUhB5l5T/NZbb3nu2LHD1cbGRrq5uRk+/fTTkwDQr1+/ywkJCUXh4eFRWq0Wb7311ikbG8uJT9YUE6kMa4rVjzXFRETUDIOeiEjlGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRFanrTXF+fn52oEDB4Y7OTn1mTp1qn/nrr7tGPREZHXaWlPs5OQkX3rppbwFCxbkdO7K24dBT0SKsdSaYldX19oxY8aUOTg41F75epbAcr7DS0RGs/n9d/zOZZ8yak1xd7+A8jGPP63KmmJLx4meiBTDmuLOwYmeyAq1NHmbiqXWFFs6TvREpBhLrSm2dJzoiUgxllpTDAA+Pj4xZWVl2urqarF58+auGzduTI+Pj7+sxHo7ijXFRCrDmmL1Y00xERE1w6AnIlI5Bj0Rkcox6ImIVI5BT0Skcgx6IiKVY9ATkdVpa03xt99+6xoVFRUZHh6uj4qKivzuu+9cOvcI2oZBT0RWp601xZ6entU//PBDZnp6esqnn3568tFHHw3q3CNoGwY9ESnGUmuKBw8eXBEYGFgNAPHx8ZcrKys1FRUVwsSny2hYgUBkhYrWpvtV518yak2xbc8u5R4Tw1VfU/zZZ5+5R0VFlTs6OlpMrQAneiJSjKXXFCclJTnMnz/f5+OPPz7VwVOhKE70RFaopcnbVCy5pvj48eO2EydODF22bNnJqKgoi2q15ERPRIqx1Jric+fOaW+77bawF198MWf06NGKNGwaEyd6IlKMpdYUv/76656nT5+2f/XVV71fffVVbwD49ddf0318fAxKrLejWFNMpDKsKVY/1hQTEVEzDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIlI5Bj0RWZ221hRv3brVSafT6XU6nT4iIkK/YsWKrp16AG3EoCciq9PWmuJ+/fpdPnz4cEpaWlrKTz/9lPH0008HVFdXd+5BtAGDnogUY6k1xS4uLrUNbZcVFRWi4X5LwQoEIiu0bt06v7Nnzxq1ptjT07M8ISFBtTXFW7Zs6TJt2rTAvLw8uw8++OBkw/MtASd6IlKMJdcUjxw58lJmZubRnTt3pv773//uVV5ebjFjPSd6IivU0uRtKpZcU9ygb9++l7t06VKTlJTkOGzYsHJjnh9T4URPRIqx1JritLQ0u4Yfvqanp9udOHHCISws7E9vDuaKEz0RKcZSa4p//fVX59tvv72XjY2N1Gg08s033zzdq1cvi6goBlhTTKQ6rClWP9YUExFRMwx6IiKVY9ATEakcg56ISOUY9EREKsegJyJSOQY9EVmdttYUN8jIyLBzcnLqM3/+fK/OWXn7MOiJyOq0taa4wZNPPuk7fPjwi52z6vZj0BORYiy1phgAVq5c2TUgIKAqMjLysklPkgmwAoHICqWkzvW7VJZu1JriLs7h5frIRaqsKb548aLmzTff7Ll9+/b0F198sacRT5siONETkWIstaZ4zpw53jNmzChwc3Orben55ogTPZEVamnyNhVLrSlOTk7u8sMPP7i/8MILviUlJdr6/dQ+99xzhaY6V8bEoCcixWRlZdl6enoapk+fXuTu7l6zbNmy7kDzmuLx48cXG3u/O3fuzGh6u6GmeNq0acVX1hTHxMRUAs1ripOTkxt/01RiYqK3s7NzjaWEPMCgJyIFWWpNsaVjTTGRyrCmWP1YU0xERM0w6ImIVI5BT0Skcgx6IiKVY9ATEakcg56ISOUY9ERkddpaU3zs2DE7BweHvjqdTq/T6fRTpkzx79wjaBsGPRFZnfbUFPv5+VWmpaWlpKWlpaxatep0562+7Rj0RKQYS64ptmSsQCCyQk+nnvZLu3TZqDXFui4O5e9E+quyphgAcnJy7CIjI/XOzs41L7/8cu7YsWPLjHTqTI4TPREpxlJriv39/atPnjx5KDU1NeWtt97Kfuihh4KLioosJj850RNZoZYmb1Ox1JpiR0dH6ejoWAMAQ4cOLff39688cuSIw7Bhw8pNc6aMy2LekYjI8mVlZdm6uLjUTp8+vSgxMTH/wIEDTkDzmmJT7Hfnzp0ZaWlpKWvWrDml0Wgaa4oB4Mqa4obnNK0pzsvLszEYDACAlJQUu6ysLPuIiIhKU6zVFDjRE5FiLLWm+KeffnJ+5ZVXfGxsbKRGo5HvvPPOKS8vrxol1moMrCkmUhnWFKsfa4qJiKgZBj0Rkcox6ImIVI5BT0Skcgx6IiKVY9ATEakcg56IrE5ba4oBYM+ePY69e/fWhYaGRoWHh+vLy8stpvGMQU9EVqetNcXV1dV44IEHgt5///1TmZmZR3/77bdjTasXzB2DnogUY6k1xd98841bZGRkxaBBgyoAoGfPnjU2NpZTLGA5KyUio5mz9qBfen6pUWuKw3u6lP97Ypwqa4qPHTtmL4TAkCFDwoqKimwmTJhQ9MorrxQY8fSZFCd6IlKMpdYUGwwGsW/fPuevvvrq5J49e459//337uvXr3cx0mkxOU70RFaopcnbVCy1ptjX17dq4MCBpb169TIAwC233HIxKSnJ6c477yw1zZkyLk70RKQYS60pvuuuu0rS0tIcS0tLNdXV1di1a5dLVFTUZVOs1RQ40RORYiy1prhHjx41M2bMKOjTp0+kEAKjRo26OGnSpItKrNUYWFNMpDKsKVY/1hQTEVEzDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIlI5Bj0RWZ221hS///77HjqdTt/wR6PRxO/evdvx+nsxHwx6IrI6ba0pfvzxx4vS0tJS0tLSUlasWHHSx8en8sYbb6zo3KNoPQY9ESnGUmuKm1qxYoVHQkJCsUlOkImwAoHIGq17wg9nU4xaUwxPfTkS3lNlTXFT69evd//mm28yO3i2FMWJnogUY6k1xQ22bNnSxdHRsbZ///4WU2gGcKInsk4tTN6mYqk1xQ0+//xzjwkTJhQZ+7yYGid6IlKMpdYUA0BNTQ02bNjgPnXqVIsLek70RKQYS60pBoBNmza59OrVq0qv1/9p+jd3rCkmUhnWFKsfa4qJiKgZBj0Rkcox6ImIVI5BT0Skcgx6IiKVY9ATEakcg56IzF5WVpbt2LFjgzv6Ok8++aRPz549Y52cnPoYY12WgkFPRGYvMDCw+scffzzR0ddJSEi4sGfPnlRjrMmSMOiJSDFXqyn28fGJeeKJJ3x0Op0+Ojo6cufOnU5DhgwJ8/Pzi3799dd7AHV1xGFhYVEAYDAYMG3aNN+wsLCo8PBw/cKFCz2v3M+pU6ds+/XrF6HT6fRhYWFRP/74ozMAjBo16lJAQEC1skfd+ViBQGSFnt/1vF9mcaZRa4pD3UPLXx78cptrihcsWAB/f/+qtLS0lP/7v//ze+SRRwL37NmTVlFRoYmJiYl65plnCpu+xptvvtnj9OnTdikpKUdtbW1RUFCgvXI/y5cv9xg1atTFRYsW5RsMBpSWllr1UGvVB09EyrpaTTEA3HvvvRcAICYmprxv376X3N3da729vQ12dna1586daxbkW7Zscf3b3/52rqFP3svLq+bK/dxwww2XVq9e3T0xMdF77969ju7u7rVXbmNNONETWaGWJm9TuVpNMdC8jvjK2uHq6uo//5qnK2zZsqXL9OnTAwDg+eefz73//vsv/vbbb8e+/vprt0ceeSRoxowZBTNmzDhvquMydwx6IlJMVlaWraenp2H69OlF7u7uNcuWLeve1tcYNWpUyYcfftj99ttvL2m4dDNy5MhLaWlpKQ3bpKen2wUHB1fNmjXrXGVlpdi/f78TAKsNel66ISLFJCcnO/bu3TtSp9PpFy5c6D1//vwzbX2NmTNnFvr6+lbpdLqoiIgI/bJlyzyu3Gbz5s0ukZGRUZGRkfqvv/7a45lnnikAgMcee8zXy8sr9vLlyxovL6/YxMREb2Mcl7ljTTGRyrCmWP1YU0xERM0w6ImIVI5BT0Skcgx6IiKVY9ATEakcg56ISOUY9ERk9kxdU7xp0yZnvV4faWNjE//JJ5+4d3Q/5oZBT0Rmz9Q1xcHBwVWffPJJ1vjx41X57VkGPREpxlxriiMiIqoGDhxYodGoMxLZdUNkhfKe+4dfZUaGUWuK7cPCyr3/tZA1xWbIqg+eiJTFmuLOwYmeyAq1NHmbCmuKOweDnogUw5rizsFLN0SkGHOtKd6+fbuTl5dX7MaNG91nzpwZEBoaGtXxozUfrCkmUhnWFKsfa4qJiKgZBj0Rkcox6ImIVI5BT0Skcgx6IiKVY9ATEakcg56IzJ6pa4oXLFjgFRISEhUeHq4fNGhQeHp6ul1H92VOGPREZPZMXVMcHx9ffuDAgdT09PSUhISE4pkzZ/p2dF/mhEFPRIox15ri8ePHl7q4uNQCwJAhQ8rOnDmjqomeXTdEVujXFal+RbllRq0p9vBxLh81NdLia4o//PDDHjfffPPF1m5vCTjRE5FizL2meOnSpR4HDx50evHFF/M7eqzmhBM9kRVqafI2FXOuKV63bp3LG2+80WvHjh3HHB0dVVUCxqAnIsWYa03xrl27HJ988smAjRs3Zvj4+BjaeXhmi5duiEgx5lpTPGfOHL/y8nLtPffcE6LT6fQjR44M7fjRmg/WFBOpDGuK1Y81xURE1AyDnohI5Rj0REQqx6AnIlI5Bj0Rkcox6ImIVI5BT0Rmz9Q1xa+//nqP8PBwvU6n08fHx0ckJyc7dHRf5oRBT0Rmz9Q1xY8++uj59PT0lLS0tJTExMT8p59+2q+j+zInDHoiUoy51hR7eHg0lp6VlZVphWixXseisOuGyAptfv8dv3PZp4xaU9zdL6B8zONPW2xN8auvvtpj6dKlXtXV1Zqff/75WFuP35xxoicixZhzTfG8efMKs7OzjyxYsCDnhRde6GWM4zUXnOiJrFBLk7epmHNNcYO//vWvRXPmzPFv3xGaJwY9ESnGXGuKDx8+bB8TE1MJAGvWrHELCAiobNcBmikGPREpJjk52XHevHm+Go0GNjY2cunSpacmT54c0pbXmDlzZmF6erq9TqeLsrGxkQ8++GDhc8891+w6/ubNm10WL17c08bGRjo5OdV8/vnnJ4G6muJvv/3Wo6Gm+P777z/31ltv5b311lueO3bscLWxsZFubm6GTz/99KQxj7uzsaaYSGVYU6x+rCkmIqJmGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRKRyDHoiMnumrilu8Omnn3YVQsT/9ttvRu0B6mwMeiIye6auKQaA4uJizX/+8x+v2NjYSx3dj7lh0BORYsy1phgAZs2a5TN79ux8e3t71X2LlBUIRFaoaG26X3X+JaNenrDt2aXcY2K4RdYU79y50yk3N9du0qRJF996662e7Tl+c8aJnogUY441xTU1NUhMTPRbvHhxpzR6KoETPZEVamnyNhVzrCm+cOGCNiMjw2HkyJERAHDu3DnbiRMnhq5duzZz2LBh5cY47s7GoCcixZhjTXG3bt1qiouLDzbcHjBgQMQbb7yRrZaQB3jphogUlJyc7Ni7d+9InU6nX7hwoff8+fPPtPU1Zs6cWejr61ul0+miIiIi9MuWLfO4cpvNmze7REZGRkVGRuq//vprj2eeeaYAqKsp9vLyim2oKU5MTPQ2xnGZO9YUE6kMa4rVjzXFRETUDIOeiEjlGPRERCrHoCciUjkGPRGRyjHoiYhUjkFPRGbP1DXFixcv7ubu7h6n0+n0Op1O/9Zbb7X5i1zmjN+MJSKzZ8ya4tmzZ5+NjIyMvvKx8ePHF69YseJ0R/dhjjjRE5FizLmmWM040RNZoXXr1vmdPXvWqDXFnp6e5QkJCRZZUwwAmzZt6hoeHu4cHBx8+T//+U92aGioat4QONETkWLMsaa4Yf+nT58+nJ6enjJq1KiSv/zlL0HGOmZzwImeyAq1NHmbijnWFANAz549G98sZs6cee6ll17y7chxmhsGPREpxhxrioG6a/oN1+5XrVrVNTg4+HK7DtBMMeiJSDHJycmO8+bN89VoNLCxsZFLly49NXny5JC2vMbMmTML09PT7XU6XZSNjY188MEHC5977rlm1/E3b97ssnjx4p42NjbSycmp5vPPPz8J1NUUf/vttx4NNcX333//ubfeeivv9ddf99y8eXNXrVYru3btavj000+zjHjYnY41xUQqw5pi9WNNMRERNcOgJyJSOQY9EZHKMeiJiFSOQU9EpHIMeiIilWPQE5HZM3VNcUZGht3AgQPDIyMj9eHh4fo1a9a4dXRf5oRBT0Rmz5g1xXv27Em98v758+f3mjBhQnFqamrK6tWrTyQmJvp3dF/mhEFPRIox15piIQRKSkq0AFBcXKz19PRUTXMlwAoEIquUkjrX71JZulFrirs4h5frIxdZZE3xq6++mnfLLbeE/fe///WsqKjQ/PDDD+ntOQfmihM9ESnGXGuKP/nkE4/JkyefLygoOPTNN99kPPTQQ0E1NX96WYvFiZ7ICrU0eZuKudYU/+9//+v+448/pgPAzTfffKmyslKTn59v4+PjY+joMZsDBj0RKcZca4q9vb2rNm7c6Pr3v//9/P79+x2qqqpEr169VBHyAC/dEJGCkpOTHXv37h2p0+n0Cxcu9J4/f/6Ztr7GzJkzC319fat0Ol1URESEftmyZR5XbrN582aXyMjIqMjISP3XX3/t8cwzzxQAdTXFXl5esQ01xYmJid4A8Pbbb2d/+umnPSIiIvRTpkwJ/uCDD7I0GvXEI2uKiVSGNcXqx5piIiJqhkFPRKRyDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIrNn6pri9PR0u0GDBoWHh4frBwwYEHH8+HHbju7LnDDoicjsmbqm+KmnnvKdMmXK+fT09JR//vOfebNmzfLt6L7MCYOeiBRjrjXFGRkZjrfeemsJANx+++2lv/zyS1eTngiFseuGyAo9nXraL+3SZaPWFOu6OJS/E+lvkTXFkZGR5atXr3Z//vnnz65cubLrpUuXNPn5+dqePXuqosKSEz0RKcZca4qXLFmSs2PHDpfIyEj9tm3bXDw9PattbNQzB6vnSIio1VqavE3FXGuKAwMDq3/66afjAHDx4kXNxo0b3bt3766KaR5g0BORgsy1pvjMmTM2np6eBq1Wi3/+85+9Jk+erKpSOF66ISLFmGtN8Y8//ugSHBwcHRgYGH327FmbV199tc3rMmesKSZSGdYUqx9riomIqBkGPRGRyjHoiYhUjkFPRKRyDHoiIpVj0BMRqRyDnojMnjFqiktLSzU33XRTaFBQUFRoaGjU9OnTfYy1PnPHoCcis2esmuJZs2YVnDx58uiRI0dS9uzZ4/zll1+6GmN95o5BT0SK6cyaYhcXl9rx48eXAnXdOrGxseXZ2dl2yp6BzsGuGyIrNGftQb/0/FKj1hSH93Qp//fEOIuoKT537pz2559/7jpnzpwCYxy7ueNET0SKMYea4urqakyYMCF42rRpBXq9vsqUx2suONETWaGWJm9TMYea4ilTpgQGBwdfnj9//lnTHKX5YdATkWI6u6b473//u3dJSYn2iy++yDLmcZk7XrohIsV0Zk3x8ePHbZcsWdIrIyPDISoqSq/T6fRvvfVWm99oLBFriolUhjXF6seaYiIiaoZBT0Skcgx6IiKVY9ATEakcg56ISOUY9EREKsegJyKzZ+qa4oqKCjFu3Lhgf3//6NjYWN2xY8dUVXbGoCcis2fqmuJ33323u5ubm+H06dNHZsyYUZCYmOjb8VWbDwY9ESnGXGuKv//++66PPPLIeQB4+OGHi3fv3u1SW1t75ctaLHbdEFmjdU/44WyKUWuK4akvR8J7FllTXFBQYBcUFFQFALa2tnB2dq4pKCiw6dWrl6Gjp8UccKInIsWwprhzcKInskYtTN6mYq41xV5eXlUnT560CwkJqa6urkZZWZnWy8tLFdM8wKAnIgWZa03xuHHjLixfvrzbzTfffOmTTz5xHzRoUKlGo54LHgx6IlJMcnKy47x583w1Gg1sbGzk0qVLT02ePDmkLa8xc+bMwvT0dHudThdlY2MjH3zwwcLnnnuu2XX8zZs3uyxevLinjY2NdHJyqvn8889PNtQUBwUFXY6KitIDwLRp084mJiaee+qpp87dfffdQf7+/tFubm41a9asOW7M4+5srCkmUhnWFKsfa4qJiKgZBj0Rkcox6ImIVI5BT0Skcgx6IiKVY9ATEakcg56IzJ4xaooBYOjQoWERERH60NDQqClTpvgbDKr58ut1MeiJyOwZq6Z4/fr1x48dO5aSnp5+9Pz587bLly93N8b6zB2DnogU05k1xQDg4eFRCwDV1dWiurpaCNFijY4qsAKByAo9v+t5v8ziTKPWFIe6h5a/PPhls68pHjJkSNihQ4e6DB8+/OLDDz9cbKzjN2ec6IlIMeZQU7xz586M/Pz8g1VVVZoNGza4mvJ4zQUneiIr1NLkbSrmUFMMAE5OTnL8+PEXvv3226533XVXifGP1Lww6IlIMZ1ZU3zx4sXiCxcuaAMCAqqrq6uxadMmt8GDB5ca9wjNE4OeiBTTmTXFJSUlmnHjxoVWVVUJKaW48cYbS+bMmVN4rf2oCWuKiVSGNcXqx5piIiJqhkFPRKRyDHoiIpVj0BMRqRyDnohI5Rj0REQqx6AnIrNnrJriBiNHjgxtKEmzBgx6IjJ7xqopBoDPPvusa5cuXf7Uj6NmDHoiUkxn1xRfvHhRs3jxYq8FCxacUfbIOxcrEIisUN5z//CrzMgwak2xfVhYufe/Fpp1TXFiYqLPU089VeDs7Fx75XPUjBM9ESmmM2uKd+/e7Xjy5En7qVOnXjD5gZoZTvREVqilydtUOrOm+NKlS5ojR444+fj4xBgMBlFUVGQzYMCAiL179x4z1fGaCwY9ESmmM2uKly9fnj137txCoO6a/+233x5mDSEPMOiJSEGdWVNs3COxLKwpJlIZ1hSrH2uKiYioGQY9EZHKMeiJiFSOQU9EpHIMeiIilWPQExGpHIOeiMyesWqKBwwYEBEYGBit0+n0Op1On5ubaxXfJbKKgyQiy2bMmuIVK1acGDZsWLkxXstScKInIsV0dk2xteJET2SFfl2R6leUW2bUmmIPH+fyUVMjzbqmGAAeffTRQI1Gg/HjxxcvWrTojEaj/nlX/UdIRGajM2uKAWDNmjUn0tPTU37//fe03bt3Oy9durSbqY/ZHHCiJ7JCLU3eptKZNcUzZsw4HxQUVA0A7u7utffdd1/R3r17uwA4b5KDNSMMeiJSTGfWFFdXV58/d+6cTa9evQyVlZVi48aNbiNHjiw17hGaJwY9ESmmM2uKKyoqNDfffHNYdXW1qK2tFUOHDi1JTEwsvNZ+1IQ1xUQqw5pi9WNNMRERNcOgJyJSOQY9EZHKMeiJiFSOQU9EpHIMeiIilWPQE5HZM1ZN8eXLl8XkyZMDAgMDo4OCgqI+/fTTrkZYntnjF6aIyOwZq6Z43rx5vXr06FGdlZV1pKamBmfPnrWKDORET0SK6eya4tWrV3d/5ZVX8gFAq9WiV69eBiWPv7NYxbsZETW3+f13/M5lnzJqTXF3v4DyMY8/bbY1xQ0tmImJid67d+92CQgIqPzoo49O+/n5qT7sOdETkWI6s6a4urpaFBQU2A4ePPhSSkpK6sCBAy89+eSTfqY/6s7HiZ7ICrU0eZtKZ9YUT58+/byDg0Pt1KlTiwHgL3/5S9H//ve/NrdnWiIGPREppjNrijUazflRo0Zd/OGHH1zuuOOO0o0bN7qGhYVVGPcIzRODnogU05k1xQDw1ltv5UyZMiVo9uzZ2m7duhlWrFiRZcTDM1usKSZSGdYUqx9riomIqBkGPRGRyjHoiYhUjkFPRKRyDHoiIpVj0BMRqRyDnojMnrFqij/88EOP8PBwfXh4uH7o0KFhZ86csYrvEjHoicjsGaOmuLq6GvPmzfPbvn17enp6ekpUVFTFv//97z81X6oRg56IFNOZNcW1tbVCSonS0lJNbW0tSkpKNN7e3lVKn4POYBX/bCGi5orWpvtV518yak2xbc8u5R4Tw822ptje3l6+9dZbp/v27Rvl6OhYExAQULlixYrTxjwH5ooTPREppjNriisrK8VHH33UY8+ePSkFBQWH9Hp9xXPPPdfL9Efd+TjRE1mhliZvU+nMmuKYmJgKAIiKiqoEgMmTJxe99tprPU1xnOaGQU9EiunMmuLbb7+9JDMz0yEvL8/G29vb8OOPP7qGh4dfNu4RmicGPREppjNrigMDA6vnzJlzZsiQIRE2NjbS19e3atWqVSeNe4TmiTXFRCrDmmL1Y00xERE1w6AnIlI5Bj0Rkcox6ImIVI5BT0Skcgx6IiKVY9ATkdkzVk3xxx9/7B4eHq4PDQ2Nevzxx32MsTZLwKAnIrNnjJri/Px87fz58323bduWnpmZebSgoMB2/fr1LsZaozlj0BORYjqzpvjYsWP2gYGBld7e3gagrkrhq6++clf2DHQOViAQWaF169b5nT171qg1xZ6enuUJCQlmW1NsMBjEiRMnHI4dO2YXHBxc9d1337m3pjBNDTjRE5FiOrOmuEePHjVvv/32qXvuuSe4f//+On9//0qNRmMVHTCc6ImsUEuTt6l0Zk3xjBkzzk+ZMuXilClTLgLAG2+80V2r/dM/BlSJQU9EiunMmmIA53Nzc218fHwMhYWF2v/+97+eX3755XGjHqCZYtATkWI6s6YYAB577DG/lJQUJwCYO3duXmxsbKXxjs58saaYSGVYU6x+rCkmIqJmGPRERCrHoCciUjkGPZH61NbW1lrFF4GsUf3/trVteQ6Dnkh9jhQWFrox7NWntrZWFBYWugE40pbn8eOVRCpjMBgezc/P/29+fn40OMypTS2AIwaD4dG2PIkfryQiUjm+2xMRqRyDnohI5Rj0REQqx6AnIlI5Bj0Rkcr9/9/YFs43VjwCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#head Iter : 1, FT Iter : 5\n",
    "%run 3classBaseExperiment --headEpoch 1 --ftEpoch 1 --expName base1epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(noOfHeadIter=1, noOfFtIter=5, noOfHeadEpoch=1, noOfFtEpoch=1, outp='output', expName='base1epoch')\n",
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " rescaling_1 (Rescaling)        (None, 224, 224, 3)  0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " normalization_1 (Normalization  (None, 224, 224, 3)  7          ['rescaling_1[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " stem_conv_pad (ZeroPadding2D)  (None, 225, 225, 3)  0           ['normalization_1[0][0]']        \n",
      "                                                                                                  \n",
      " stem_conv (Conv2D)             (None, 112, 112, 32  864         ['stem_conv_pad[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 112, 112, 32  128         ['stem_conv[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " stem_activation (Activation)   (None, 112, 112, 32  0           ['stem_bn[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_dwconv (DepthwiseConv2  (None, 112, 112, 32  288        ['stem_activation[0][0]']        \n",
      " D)                             )                                                                 \n",
      "                                                                                                  \n",
      " block1a_bn (BatchNormalization  (None, 112, 112, 32  128        ['block1a_dwconv[0][0]']         \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_activation (Activation  (None, 112, 112, 32  0          ['block1a_bn[0][0]']             \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block1a_se_squeeze (GlobalAver  (None, 32)          0           ['block1a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block1a_se_reshape (Reshape)   (None, 1, 1, 32)     0           ['block1a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_reduce (Conv2D)     (None, 1, 1, 8)      264         ['block1a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block1a_se_expand (Conv2D)     (None, 1, 1, 32)     288         ['block1a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_se_excite (Multiply)   (None, 112, 112, 32  0           ['block1a_activation[0][0]',     \n",
      "                                )                                 'block1a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block1a_project_conv (Conv2D)  (None, 112, 112, 16  512         ['block1a_se_excite[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block1a_project_bn (BatchNorma  (None, 112, 112, 16  64         ['block1a_project_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_conv (Conv2D)   (None, 112, 112, 96  1536        ['block1a_project_bn[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_bn (BatchNormal  (None, 112, 112, 96  384        ['block2a_expand_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_expand_activation (Act  (None, 112, 112, 96  0          ['block2a_expand_bn[0][0]']      \n",
      " ivation)                       )                                                                 \n",
      "                                                                                                  \n",
      " block2a_dwconv_pad (ZeroPaddin  (None, 113, 113, 96  0          ['block2a_expand_activation[0][0]\n",
      " g2D)                           )                                ']                               \n",
      "                                                                                                  \n",
      " block2a_dwconv (DepthwiseConv2  (None, 56, 56, 96)  864         ['block2a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block2a_bn (BatchNormalization  (None, 56, 56, 96)  384         ['block2a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_activation (Activation  (None, 56, 56, 96)  0           ['block2a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2a_se_squeeze (GlobalAver  (None, 96)          0           ['block2a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2a_se_reshape (Reshape)   (None, 1, 1, 96)     0           ['block2a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_reduce (Conv2D)     (None, 1, 1, 4)      388         ['block2a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2a_se_expand (Conv2D)     (None, 1, 1, 96)     480         ['block2a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_se_excite (Multiply)   (None, 56, 56, 96)   0           ['block2a_activation[0][0]',     \n",
      "                                                                  'block2a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_conv (Conv2D)  (None, 56, 56, 24)   2304        ['block2a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2a_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block2b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_expand_activation (Act  (None, 56, 56, 144)  0          ['block2b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block2b_dwconv (DepthwiseConv2  (None, 56, 56, 144)  1296       ['block2b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block2b_bn (BatchNormalization  (None, 56, 56, 144)  576        ['block2b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_activation (Activation  (None, 56, 56, 144)  0          ['block2b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block2b_se_squeeze (GlobalAver  (None, 144)         0           ['block2b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block2b_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block2b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block2b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block2b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_se_excite (Multiply)   (None, 56, 56, 144)  0           ['block2b_activation[0][0]',     \n",
      "                                                                  'block2b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_conv (Conv2D)  (None, 56, 56, 24)   3456        ['block2b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block2b_project_bn (BatchNorma  (None, 56, 56, 24)  96          ['block2b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block2b_drop (Dropout)         (None, 56, 56, 24)   0           ['block2b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block2b_add (Add)              (None, 56, 56, 24)   0           ['block2b_drop[0][0]',           \n",
      "                                                                  'block2a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_expand_conv (Conv2D)   (None, 56, 56, 144)  3456        ['block2b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block3a_expand_bn (BatchNormal  (None, 56, 56, 144)  576        ['block3a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_expand_activation (Act  (None, 56, 56, 144)  0          ['block3a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3a_dwconv_pad (ZeroPaddin  (None, 59, 59, 144)  0          ['block3a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block3a_dwconv (DepthwiseConv2  (None, 28, 28, 144)  3600       ['block3a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block3a_bn (BatchNormalization  (None, 28, 28, 144)  576        ['block3a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_activation (Activation  (None, 28, 28, 144)  0          ['block3a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3a_se_squeeze (GlobalAver  (None, 144)         0           ['block3a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3a_se_reshape (Reshape)   (None, 1, 1, 144)    0           ['block3a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_reduce (Conv2D)     (None, 1, 1, 6)      870         ['block3a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3a_se_expand (Conv2D)     (None, 1, 1, 144)    1008        ['block3a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_se_excite (Multiply)   (None, 28, 28, 144)  0           ['block3a_activation[0][0]',     \n",
      "                                                                  'block3a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_conv (Conv2D)  (None, 28, 28, 40)   5760        ['block3a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3a_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block3b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_expand_activation (Act  (None, 28, 28, 240)  0          ['block3b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block3b_dwconv (DepthwiseConv2  (None, 28, 28, 240)  6000       ['block3b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block3b_bn (BatchNormalization  (None, 28, 28, 240)  960        ['block3b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_activation (Activation  (None, 28, 28, 240)  0          ['block3b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block3b_se_squeeze (GlobalAver  (None, 240)         0           ['block3b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block3b_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block3b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block3b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block3b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_se_excite (Multiply)   (None, 28, 28, 240)  0           ['block3b_activation[0][0]',     \n",
      "                                                                  'block3b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_conv (Conv2D)  (None, 28, 28, 40)   9600        ['block3b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block3b_project_bn (BatchNorma  (None, 28, 28, 40)  160         ['block3b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block3b_drop (Dropout)         (None, 28, 28, 40)   0           ['block3b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block3b_add (Add)              (None, 28, 28, 40)   0           ['block3b_drop[0][0]',           \n",
      "                                                                  'block3a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_expand_conv (Conv2D)   (None, 28, 28, 240)  9600        ['block3b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4a_expand_bn (BatchNormal  (None, 28, 28, 240)  960        ['block4a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_expand_activation (Act  (None, 28, 28, 240)  0          ['block4a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4a_dwconv_pad (ZeroPaddin  (None, 29, 29, 240)  0          ['block4a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block4a_dwconv (DepthwiseConv2  (None, 14, 14, 240)  2160       ['block4a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block4a_bn (BatchNormalization  (None, 14, 14, 240)  960        ['block4a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_activation (Activation  (None, 14, 14, 240)  0          ['block4a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4a_se_squeeze (GlobalAver  (None, 240)         0           ['block4a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4a_se_reshape (Reshape)   (None, 1, 1, 240)    0           ['block4a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_reduce (Conv2D)     (None, 1, 1, 10)     2410        ['block4a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4a_se_expand (Conv2D)     (None, 1, 1, 240)    2640        ['block4a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_se_excite (Multiply)   (None, 14, 14, 240)  0           ['block4a_activation[0][0]',     \n",
      "                                                                  'block4a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_conv (Conv2D)  (None, 14, 14, 80)   19200       ['block4a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4a_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_expand_activation (Act  (None, 14, 14, 480)  0          ['block4b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4b_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4b_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_activation (Activation  (None, 14, 14, 480)  0          ['block4b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4b_se_squeeze (GlobalAver  (None, 480)         0           ['block4b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4b_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4b_activation[0][0]',     \n",
      "                                                                  'block4b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4b_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4b_drop (Dropout)         (None, 14, 14, 80)   0           ['block4b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4b_add (Add)              (None, 14, 14, 80)   0           ['block4b_drop[0][0]',           \n",
      "                                                                  'block4a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block4c_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block4c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_expand_activation (Act  (None, 14, 14, 480)  0          ['block4c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block4c_dwconv (DepthwiseConv2  (None, 14, 14, 480)  4320       ['block4c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block4c_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block4c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_activation (Activation  (None, 14, 14, 480)  0          ['block4c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block4c_se_squeeze (GlobalAver  (None, 480)         0           ['block4c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block4c_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block4c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block4c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block4c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block4c_activation[0][0]',     \n",
      "                                                                  'block4c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_conv (Conv2D)  (None, 14, 14, 80)   38400       ['block4c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block4c_project_bn (BatchNorma  (None, 14, 14, 80)  320         ['block4c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block4c_drop (Dropout)         (None, 14, 14, 80)   0           ['block4c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block4c_add (Add)              (None, 14, 14, 80)   0           ['block4c_drop[0][0]',           \n",
      "                                                                  'block4b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_conv (Conv2D)   (None, 14, 14, 480)  38400       ['block4c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5a_expand_bn (BatchNormal  (None, 14, 14, 480)  1920       ['block5a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_expand_activation (Act  (None, 14, 14, 480)  0          ['block5a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5a_dwconv (DepthwiseConv2  (None, 14, 14, 480)  12000      ['block5a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5a_bn (BatchNormalization  (None, 14, 14, 480)  1920       ['block5a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_activation (Activation  (None, 14, 14, 480)  0          ['block5a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5a_se_squeeze (GlobalAver  (None, 480)         0           ['block5a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5a_se_reshape (Reshape)   (None, 1, 1, 480)    0           ['block5a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_reduce (Conv2D)     (None, 1, 1, 20)     9620        ['block5a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5a_se_expand (Conv2D)     (None, 1, 1, 480)    10080       ['block5a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_se_excite (Multiply)   (None, 14, 14, 480)  0           ['block5a_activation[0][0]',     \n",
      "                                                                  'block5a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_conv (Conv2D)  (None, 14, 14, 112)  53760       ['block5a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5a_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_expand_activation (Act  (None, 14, 14, 672)  0          ['block5b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5b_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5b_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_activation (Activation  (None, 14, 14, 672)  0          ['block5b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5b_activation[0][0]',     \n",
      "                                                                  'block5b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5b_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5b_drop (Dropout)         (None, 14, 14, 112)  0           ['block5b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5b_add (Add)              (None, 14, 14, 112)  0           ['block5b_drop[0][0]',           \n",
      "                                                                  'block5a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block5c_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block5c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_expand_activation (Act  (None, 14, 14, 672)  0          ['block5c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block5c_dwconv (DepthwiseConv2  (None, 14, 14, 672)  16800      ['block5c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block5c_bn (BatchNormalization  (None, 14, 14, 672)  2688       ['block5c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_activation (Activation  (None, 14, 14, 672)  0          ['block5c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_se_excite (Multiply)   (None, 14, 14, 672)  0           ['block5c_activation[0][0]',     \n",
      "                                                                  'block5c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_conv (Conv2D)  (None, 14, 14, 112)  75264       ['block5c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block5c_project_bn (BatchNorma  (None, 14, 14, 112)  448        ['block5c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block5c_drop (Dropout)         (None, 14, 14, 112)  0           ['block5c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block5c_add (Add)              (None, 14, 14, 112)  0           ['block5c_drop[0][0]',           \n",
      "                                                                  'block5b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_conv (Conv2D)   (None, 14, 14, 672)  75264       ['block5c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6a_expand_bn (BatchNormal  (None, 14, 14, 672)  2688       ['block6a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_expand_activation (Act  (None, 14, 14, 672)  0          ['block6a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6a_dwconv_pad (ZeroPaddin  (None, 17, 17, 672)  0          ['block6a_expand_activation[0][0]\n",
      " g2D)                                                            ']                               \n",
      "                                                                                                  \n",
      " block6a_dwconv (DepthwiseConv2  (None, 7, 7, 672)   16800       ['block6a_dwconv_pad[0][0]']     \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block6a_bn (BatchNormalization  (None, 7, 7, 672)   2688        ['block6a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_activation (Activation  (None, 7, 7, 672)   0           ['block6a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_se_excite (Multiply)   (None, 7, 7, 672)    0           ['block6a_activation[0][0]',     \n",
      "                                                                  'block6a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_conv (Conv2D)  (None, 7, 7, 192)    129024      ['block6a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6a_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6b_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6b_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6b_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6b_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6b_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6b_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_activation (Activation  (None, 7, 7, 1152)  0           ['block6b_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6b_activation[0][0]',     \n",
      "                                                                  'block6b_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6b_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6b_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6b_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6b_drop (Dropout)         (None, 7, 7, 192)    0           ['block6b_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6b_add (Add)              (None, 7, 7, 192)    0           ['block6b_drop[0][0]',           \n",
      "                                                                  'block6a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6c_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6c_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6c_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6c_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6c_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6c_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6c_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_activation (Activation  (None, 7, 7, 1152)  0           ['block6c_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6c_activation[0][0]',     \n",
      "                                                                  'block6c_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6c_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6c_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6c_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6c_drop (Dropout)         (None, 7, 7, 192)    0           ['block6c_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6c_add (Add)              (None, 7, 7, 192)    0           ['block6c_drop[0][0]',           \n",
      "                                                                  'block6b_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block6d_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block6d_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_expand_activation (Act  (None, 7, 7, 1152)  0           ['block6d_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block6d_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  28800       ['block6d_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block6d_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block6d_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_activation (Activation  (None, 7, 7, 1152)  0           ['block6d_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block6d_activation[0][0]',     \n",
      "                                                                  'block6d_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_conv (Conv2D)  (None, 7, 7, 192)    221184      ['block6d_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block6d_project_bn (BatchNorma  (None, 7, 7, 192)   768         ['block6d_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block6d_drop (Dropout)         (None, 7, 7, 192)    0           ['block6d_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " block6d_add (Add)              (None, 7, 7, 192)    0           ['block6d_drop[0][0]',           \n",
      "                                                                  'block6c_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_conv (Conv2D)   (None, 7, 7, 1152)   221184      ['block6d_add[0][0]']            \n",
      "                                                                                                  \n",
      " block7a_expand_bn (BatchNormal  (None, 7, 7, 1152)  4608        ['block7a_expand_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_expand_activation (Act  (None, 7, 7, 1152)  0           ['block7a_expand_bn[0][0]']      \n",
      " ivation)                                                                                         \n",
      "                                                                                                  \n",
      " block7a_dwconv (DepthwiseConv2  (None, 7, 7, 1152)  10368       ['block7a_expand_activation[0][0]\n",
      " D)                                                              ']                               \n",
      "                                                                                                  \n",
      " block7a_bn (BatchNormalization  (None, 7, 7, 1152)  4608        ['block7a_dwconv[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_activation (Activation  (None, 7, 7, 1152)  0           ['block7a_bn[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block7a_se_squeeze (GlobalAver  (None, 1152)        0           ['block7a_activation[0][0]']     \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " block7a_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block7a_se_squeeze[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block7a_se_reshape[0][0]']     \n",
      "                                                                                                  \n",
      " block7a_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block7a_se_reduce[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_se_excite (Multiply)   (None, 7, 7, 1152)   0           ['block7a_activation[0][0]',     \n",
      "                                                                  'block7a_se_expand[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_conv (Conv2D)  (None, 7, 7, 320)    368640      ['block7a_se_excite[0][0]']      \n",
      "                                                                                                  \n",
      " block7a_project_bn (BatchNorma  (None, 7, 7, 320)   1280        ['block7a_project_conv[0][0]']   \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " top_conv (Conv2D)              (None, 7, 7, 1280)   409600      ['block7a_project_bn[0][0]']     \n",
      "                                                                                                  \n",
      " top_bn (BatchNormalization)    (None, 7, 7, 1280)   5120        ['top_conv[0][0]']               \n",
      "                                                                                                  \n",
      " top_activation (Activation)    (None, 7, 7, 1280)   0           ['top_bn[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3667 - accuracy: 0.4618 \n",
      "Epoch 00001: val_loss improved from inf to 2.21699, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-head-base1epoch-0901202203.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.3667 - accuracy: 0.4618 - val_loss: 2.2170 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2139 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.17353, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.2139 - accuracy: 0.5666 - val_loss: 2.1735 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1853 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.16332, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 2.1853 - accuracy: 0.5694 - val_loss: 2.1633 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1773 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.15714, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1773 - accuracy: 0.5524 - val_loss: 2.1571 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1619 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.13840, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 142s 12s/step - loss: 2.1619 - accuracy: 0.5666 - val_loss: 2.1384 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1091 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.12610, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-018-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1091 - accuracy: 0.5807 - val_loss: 2.1261 - val_accuracy: 0.5698\n",
      "Evaluating samm-018\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3444 - accuracy: 0.5071 \n",
      "Epoch 00001: val_loss improved from inf to 2.20928, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 132s 11s/step - loss: 2.3444 - accuracy: 0.5071 - val_loss: 2.2093 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2224 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.15392, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.2224 - accuracy: 0.5269 - val_loss: 2.1539 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1757 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.14505, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1757 - accuracy: 0.5382 - val_loss: 2.1451 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1716 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.13699, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1716 - accuracy: 0.5326 - val_loss: 2.1370 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1668 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.13379, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1668 - accuracy: 0.5467 - val_loss: 2.1338 - val_accuracy: 0.5529\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1274 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.11902, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-010-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1274 - accuracy: 0.5722 - val_loss: 2.1190 - val_accuracy: 0.5647\n",
      "Evaluating samm-010\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3869 - accuracy: 0.5042 \n",
      "Epoch 00001: val_loss improved from inf to 2.30192, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 119s 10s/step - loss: 2.3869 - accuracy: 0.5042 - val_loss: 2.3019 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1767 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.24003, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1767 - accuracy: 0.5524 - val_loss: 2.2400 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1382 - accuracy: 0.5921 \n",
      "Epoch 00001: val_loss improved from inf to 2.21734, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1382 - accuracy: 0.5921 - val_loss: 2.2173 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1382 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.21125, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1382 - accuracy: 0.5609 - val_loss: 2.2112 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1141 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.19930, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1141 - accuracy: 0.5637 - val_loss: 2.1993 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1138 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.19329, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub21-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1138 - accuracy: 0.5552 - val_loss: 2.1933 - val_accuracy: 0.5227\n",
      "Evaluating casme2-sub21\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4111 - accuracy: 0.4674 \n",
      "Epoch 00001: val_loss improved from inf to 2.15414, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 120s 10s/step - loss: 2.4111 - accuracy: 0.4674 - val_loss: 2.1541 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2159 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.12230, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.2159 - accuracy: 0.5552 - val_loss: 2.1223 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1741 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.12220, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1741 - accuracy: 0.5439 - val_loss: 2.1222 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1722 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.10454, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1722 - accuracy: 0.5581 - val_loss: 2.1045 - val_accuracy: 0.5814\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1176 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.10580, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1176 - accuracy: 0.5496 - val_loss: 2.1058 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1432 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.09358, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub16-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1432 - accuracy: 0.5552 - val_loss: 2.0936 - val_accuracy: 0.5814\n",
      "Evaluating casme2-sub16\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3900 - accuracy: 0.4759 \n",
      "Epoch 00001: val_loss improved from inf to 2.20713, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 120s 10s/step - loss: 2.3900 - accuracy: 0.4759 - val_loss: 2.2071 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2181 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.15338, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.2181 - accuracy: 0.5637 - val_loss: 2.1534 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2005 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.14615, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.2005 - accuracy: 0.5467 - val_loss: 2.1461 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1588 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.13858, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1588 - accuracy: 0.5609 - val_loss: 2.1386 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1341 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.12528, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 120s 10s/step - loss: 2.1341 - accuracy: 0.5694 - val_loss: 2.1253 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1344 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.12181, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub04-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1344 - accuracy: 0.5609 - val_loss: 2.1218 - val_accuracy: 0.5517\n",
      "Evaluating casme2-sub04\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3541 - accuracy: 0.4986 \n",
      "Epoch 00001: val_loss improved from inf to 2.24955, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 121s 10s/step - loss: 2.3541 - accuracy: 0.4986 - val_loss: 2.2496 - val_accuracy: 0.5119\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1979 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.20801, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1979 - accuracy: 0.5524 - val_loss: 2.2080 - val_accuracy: 0.5238\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1567 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.21837, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1567 - accuracy: 0.5666 - val_loss: 2.2184 - val_accuracy: 0.5238\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1489 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.20059, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1489 - accuracy: 0.5552 - val_loss: 2.2006 - val_accuracy: 0.5357\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1123 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.20152, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1123 - accuracy: 0.5807 - val_loss: 2.2015 - val_accuracy: 0.5357\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0822 - accuracy: 0.6091 \n",
      "Epoch 00001: val_loss improved from inf to 2.20158, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub25-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.0822 - accuracy: 0.6091 - val_loss: 2.2016 - val_accuracy: 0.5357\n",
      "Evaluating casme2-sub25\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3944 - accuracy: 0.4873 \n",
      "Epoch 00001: val_loss improved from inf to 2.22195, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 123s 10s/step - loss: 2.3944 - accuracy: 0.4873 - val_loss: 2.2219 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1845 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.18626, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1845 - accuracy: 0.5496 - val_loss: 2.1863 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1602 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.17802, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1602 - accuracy: 0.5354 - val_loss: 2.1780 - val_accuracy: 0.5455\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1555 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.17444, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1555 - accuracy: 0.5637 - val_loss: 2.1744 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1448 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.16189, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1448 - accuracy: 0.5411 - val_loss: 2.1619 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1135 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.16053, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-019-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 121s 10s/step - loss: 2.1135 - accuracy: 0.5722 - val_loss: 2.1605 - val_accuracy: 0.5682\n",
      "Evaluating samm-019\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3344 - accuracy: 0.4844 \n",
      "Epoch 00001: val_loss improved from inf to 2.37849, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 117s 10s/step - loss: 2.3344 - accuracy: 0.4844 - val_loss: 2.3785 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2124 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.29901, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.2124 - accuracy: 0.5354 - val_loss: 2.2990 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1855 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.27072, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 119s 10s/step - loss: 2.1855 - accuracy: 0.5354 - val_loss: 2.2707 - val_accuracy: 0.5200\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1204 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.26537, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1204 - accuracy: 0.5581 - val_loss: 2.2654 - val_accuracy: 0.5200\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1453 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.25400, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 2.1453 - accuracy: 0.5354 - val_loss: 2.2540 - val_accuracy: 0.5200\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1319 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.24698, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s3-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 2.1319 - accuracy: 0.5496 - val_loss: 2.2470 - val_accuracy: 0.5200\n",
      "Evaluating smic-s3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4107 - accuracy: 0.4844 \n",
      "Epoch 00001: val_loss improved from inf to 2.19512, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 125s 10s/step - loss: 2.4107 - accuracy: 0.4844 - val_loss: 2.1951 - val_accuracy: 0.5679\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2202 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.17261, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.2202 - accuracy: 0.5637 - val_loss: 2.1726 - val_accuracy: 0.5679\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1566 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.16632, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1566 - accuracy: 0.5609 - val_loss: 2.1663 - val_accuracy: 0.5679\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1193 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 2.16627, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1193 - accuracy: 0.5892 - val_loss: 2.1663 - val_accuracy: 0.5679\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1215 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.15499, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1215 - accuracy: 0.5836 - val_loss: 2.1550 - val_accuracy: 0.5679\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1033 - accuracy: 0.6204 \n",
      "Epoch 00001: val_loss improved from inf to 2.16383, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-007-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1033 - accuracy: 0.6204 - val_loss: 2.1638 - val_accuracy: 0.5679\n",
      "Evaluating samm-007\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4143 - accuracy: 0.4476 \n",
      "Epoch 00001: val_loss improved from inf to 2.14048, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 122s 10s/step - loss: 2.4143 - accuracy: 0.4476 - val_loss: 2.1405 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2156 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.12360, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.2156 - accuracy: 0.5496 - val_loss: 2.1236 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1869 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.12137, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1869 - accuracy: 0.5467 - val_loss: 2.1214 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1921 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.12238, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 2.1921 - accuracy: 0.5269 - val_loss: 2.1224 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1827 - accuracy: 0.5127 \n",
      "Epoch 00001: val_loss improved from inf to 2.10982, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1827 - accuracy: 0.5127 - val_loss: 2.1098 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1056 - accuracy: 0.5864 \n",
      "Epoch 00001: val_loss improved from inf to 2.10307, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-028-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1056 - accuracy: 0.5864 - val_loss: 2.1031 - val_accuracy: 0.6047\n",
      "Evaluating samm-028\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3739 - accuracy: 0.4929 \n",
      "Epoch 00001: val_loss improved from inf to 2.28674, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 121s 10s/step - loss: 2.3739 - accuracy: 0.4929 - val_loss: 2.2867 - val_accuracy: 0.5294\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2086 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.21733, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.2086 - accuracy: 0.5609 - val_loss: 2.2173 - val_accuracy: 0.5412\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1674 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.20484, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1674 - accuracy: 0.5524 - val_loss: 2.2048 - val_accuracy: 0.5294\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1293 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.20328, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 2.1293 - accuracy: 0.5722 - val_loss: 2.2033 - val_accuracy: 0.5412\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1358 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.19790, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1358 - accuracy: 0.5496 - val_loss: 2.1979 - val_accuracy: 0.5412\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1263 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.18946, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s6-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 122s 10s/step - loss: 2.1263 - accuracy: 0.5779 - val_loss: 2.1895 - val_accuracy: 0.5647\n",
      "Evaluating smic-s6\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3390 - accuracy: 0.4986 \n",
      "Epoch 00001: val_loss improved from inf to 2.16509, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 121s 10s/step - loss: 2.3390 - accuracy: 0.4986 - val_loss: 2.1651 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1987 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.14431, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1987 - accuracy: 0.5411 - val_loss: 2.1443 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1816 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.13126, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1816 - accuracy: 0.5411 - val_loss: 2.1313 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1540 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.12269, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1540 - accuracy: 0.5637 - val_loss: 2.1227 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1463 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.11569, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1463 - accuracy: 0.5609 - val_loss: 2.1157 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1325 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.10952, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 123s 10s/step - loss: 2.1325 - accuracy: 0.5581 - val_loss: 2.1095 - val_accuracy: 0.5977\n",
      "Evaluating smic-s19\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3624 - accuracy: 0.4958 \n",
      "Epoch 00001: val_loss improved from inf to 2.29214, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 145s 12s/step - loss: 2.3624 - accuracy: 0.4958 - val_loss: 2.2921 - val_accuracy: 0.5057\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1990 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.20391, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 147s 12s/step - loss: 2.1990 - accuracy: 0.5552 - val_loss: 2.2039 - val_accuracy: 0.5057\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1583 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.20089, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 157s 13s/step - loss: 2.1583 - accuracy: 0.5666 - val_loss: 2.2009 - val_accuracy: 0.5057\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1349 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.20280, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1349 - accuracy: 0.5666 - val_loss: 2.2028 - val_accuracy: 0.5057\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1017 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.17626, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 12s/step - loss: 2.1017 - accuracy: 0.5807 - val_loss: 2.1763 - val_accuracy: 0.5057\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1114 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.17345, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-021-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1114 - accuracy: 0.5722 - val_loss: 2.1735 - val_accuracy: 0.5057\n",
      "Evaluating samm-021\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3976 - accuracy: 0.4646 \n",
      "Epoch 00001: val_loss improved from inf to 2.12049, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 125s 10s/step - loss: 2.3976 - accuracy: 0.4646 - val_loss: 2.1205 - val_accuracy: 0.5976\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1902 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.09443, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1902 - accuracy: 0.5581 - val_loss: 2.0944 - val_accuracy: 0.6220\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1965 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.09347, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 147s 12s/step - loss: 2.1965 - accuracy: 0.5581 - val_loss: 2.0935 - val_accuracy: 0.6098\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1697 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.08558, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 141s 12s/step - loss: 2.1697 - accuracy: 0.5326 - val_loss: 2.0856 - val_accuracy: 0.6098\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1608 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.08452, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1608 - accuracy: 0.5467 - val_loss: 2.0845 - val_accuracy: 0.6098\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0970 - accuracy: 0.5921 \n",
      "Epoch 00001: val_loss improved from inf to 2.07867, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.0970 - accuracy: 0.5921 - val_loss: 2.0787 - val_accuracy: 0.6098\n",
      "Evaluating smic-s11\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3153 - accuracy: 0.4901 \n",
      "Epoch 00001: val_loss improved from inf to 2.14164, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 134s 11s/step - loss: 2.3153 - accuracy: 0.4901 - val_loss: 2.1416 - val_accuracy: 0.5833\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1924 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.11692, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1924 - accuracy: 0.5326 - val_loss: 2.1169 - val_accuracy: 0.5833\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1478 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.10575, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1478 - accuracy: 0.5694 - val_loss: 2.1058 - val_accuracy: 0.5833\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1266 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.09884, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1266 - accuracy: 0.5581 - val_loss: 2.0988 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1227 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.08818, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 124s 10s/step - loss: 2.1227 - accuracy: 0.5552 - val_loss: 2.0882 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0923 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.08149, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub07-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.0923 - accuracy: 0.5807 - val_loss: 2.0815 - val_accuracy: 0.6071\n",
      "Evaluating casme2-sub07\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3884 - accuracy: 0.4561 \n",
      "Epoch 00001: val_loss improved from inf to 2.07574, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 168s 14s/step - loss: 2.3884 - accuracy: 0.4561 - val_loss: 2.0757 - val_accuracy: 0.6512\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2229 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.06914, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 166s 13s/step - loss: 2.2229 - accuracy: 0.5326 - val_loss: 2.0691 - val_accuracy: 0.6512\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2418 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.05860, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.2418 - accuracy: 0.5326 - val_loss: 2.0586 - val_accuracy: 0.6628\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1991 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.04457, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 144s 12s/step - loss: 2.1991 - accuracy: 0.5241 - val_loss: 2.0446 - val_accuracy: 0.6628\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1577 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.03392, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 142s 12s/step - loss: 2.1577 - accuracy: 0.5722 - val_loss: 2.0339 - val_accuracy: 0.6628\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1681 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.02631, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-015-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 144s 12s/step - loss: 2.1681 - accuracy: 0.5439 - val_loss: 2.0263 - val_accuracy: 0.6628\n",
      "Evaluating samm-015\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4144 - accuracy: 0.4448 \n",
      "Epoch 00001: val_loss improved from inf to 2.26950, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 154s 13s/step - loss: 2.4144 - accuracy: 0.4448 - val_loss: 2.2695 - val_accuracy: 0.5513\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1975 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.22195, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1975 - accuracy: 0.5467 - val_loss: 2.2220 - val_accuracy: 0.5641\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1583 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.20820, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1583 - accuracy: 0.5609 - val_loss: 2.2082 - val_accuracy: 0.5641\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1682 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.19724, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1682 - accuracy: 0.5467 - val_loss: 2.1972 - val_accuracy: 0.5641\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1277 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 2.20012, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 139s 12s/step - loss: 2.1277 - accuracy: 0.5892 - val_loss: 2.2001 - val_accuracy: 0.5641\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1486 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.19889, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1486 - accuracy: 0.5751 - val_loss: 2.1989 - val_accuracy: 0.5641\n",
      "Evaluating casme2-sub12\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3519 - accuracy: 0.4674 \n",
      "Epoch 00001: val_loss improved from inf to 2.16842, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 154s 13s/step - loss: 2.3519 - accuracy: 0.4674 - val_loss: 2.1684 - val_accuracy: 0.6125\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2111 - accuracy: 0.5099 \n",
      "Epoch 00001: val_loss improved from inf to 2.15093, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.2111 - accuracy: 0.5099 - val_loss: 2.1509 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2052 - accuracy: 0.4901 \n",
      "Epoch 00001: val_loss improved from inf to 2.13121, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 2.2052 - accuracy: 0.4901 - val_loss: 2.1312 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1561 - accuracy: 0.5156 \n",
      "Epoch 00001: val_loss improved from inf to 2.11550, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 145s 12s/step - loss: 2.1561 - accuracy: 0.5156 - val_loss: 2.1155 - val_accuracy: 0.6250\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1216 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.12647, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 149s 12s/step - loss: 2.1216 - accuracy: 0.5581 - val_loss: 2.1265 - val_accuracy: 0.6375\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1384 - accuracy: 0.5099 \n",
      "Epoch 00001: val_loss improved from inf to 2.11854, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub02-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 172s 14s/step - loss: 2.1384 - accuracy: 0.5099 - val_loss: 2.1185 - val_accuracy: 0.6250\n",
      "Evaluating casme2-sub02\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3977 - accuracy: 0.4476 \n",
      "Epoch 00001: val_loss improved from inf to 1.91787, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 157s 12s/step - loss: 2.3977 - accuracy: 0.4476 - val_loss: 1.9179 - val_accuracy: 0.7381\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2008 - accuracy: 0.5297 \n",
      "Epoch 00001: val_loss improved from inf to 1.97458, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 2.2008 - accuracy: 0.5297 - val_loss: 1.9746 - val_accuracy: 0.7857\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2112 - accuracy: 0.5042 \n",
      "Epoch 00001: val_loss improved from inf to 1.94658, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 12s/step - loss: 2.2112 - accuracy: 0.5042 - val_loss: 1.9466 - val_accuracy: 0.7381\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1634 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 1.95359, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 141s 12s/step - loss: 2.1634 - accuracy: 0.5411 - val_loss: 1.9536 - val_accuracy: 0.7500\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1630 - accuracy: 0.5014 \n",
      "Epoch 00001: val_loss improved from inf to 1.94508, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 151s 12s/step - loss: 2.1630 - accuracy: 0.5014 - val_loss: 1.9451 - val_accuracy: 0.7738\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1647 - accuracy: 0.5212 \n",
      "Epoch 00001: val_loss improved from inf to 1.93762, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-016-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 142s 12s/step - loss: 2.1647 - accuracy: 0.5212 - val_loss: 1.9376 - val_accuracy: 0.7738\n",
      "Evaluating samm-016\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3863 - accuracy: 0.4646 \n",
      "Epoch 00001: val_loss improved from inf to 2.11154, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 135s 11s/step - loss: 2.3863 - accuracy: 0.4646 - val_loss: 2.1115 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2154 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.09839, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 2.2154 - accuracy: 0.5326 - val_loss: 2.0984 - val_accuracy: 0.5952\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1920 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.08921, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1920 - accuracy: 0.5609 - val_loss: 2.0892 - val_accuracy: 0.6071\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1621 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.07982, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1621 - accuracy: 0.5552 - val_loss: 2.0798 - val_accuracy: 0.6071\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1682 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.08052, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 11s/step - loss: 2.1682 - accuracy: 0.5269 - val_loss: 2.0805 - val_accuracy: 0.6071\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1169 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.06411, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub03-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 2.1169 - accuracy: 0.5609 - val_loss: 2.0641 - val_accuracy: 0.6190\n",
      "Evaluating casme2-sub03\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3772 - accuracy: 0.4873 \n",
      "Epoch 00001: val_loss improved from inf to 2.27394, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 126s 11s/step - loss: 2.3772 - accuracy: 0.4873 - val_loss: 2.2739 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1998 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.22559, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1998 - accuracy: 0.5552 - val_loss: 2.2256 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1765 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.22064, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1765 - accuracy: 0.5524 - val_loss: 2.2206 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1420 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.21235, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1420 - accuracy: 0.5722 - val_loss: 2.2124 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1358 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.20781, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1358 - accuracy: 0.5609 - val_loss: 2.2078 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1216 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.20146, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1216 - accuracy: 0.5751 - val_loss: 2.2015 - val_accuracy: 0.5632\n",
      "Evaluating casme2-sub13\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3774 - accuracy: 0.4759 \n",
      "Epoch 00001: val_loss improved from inf to 2.26865, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 123s 10s/step - loss: 2.3774 - accuracy: 0.4759 - val_loss: 2.2686 - val_accuracy: 0.5410\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1856 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.20014, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1856 - accuracy: 0.5524 - val_loss: 2.2001 - val_accuracy: 0.5574\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1427 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.20234, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1427 - accuracy: 0.5581 - val_loss: 2.2023 - val_accuracy: 0.5574\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1214 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.19290, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1214 - accuracy: 0.5552 - val_loss: 2.1929 - val_accuracy: 0.5574\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1141 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.18980, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1141 - accuracy: 0.5751 - val_loss: 2.1898 - val_accuracy: 0.5738\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1221 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.18654, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s2-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1221 - accuracy: 0.5779 - val_loss: 2.1865 - val_accuracy: 0.5574\n",
      "Evaluating smic-s2\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4393 - accuracy: 0.4533 \n",
      "Epoch 00001: val_loss improved from inf to 2.14727, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 125s 10s/step - loss: 2.4393 - accuracy: 0.4533 - val_loss: 2.1473 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2295 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.12929, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.2295 - accuracy: 0.5326 - val_loss: 2.1293 - val_accuracy: 0.5977\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1846 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.11408, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1846 - accuracy: 0.5411 - val_loss: 2.1141 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1940 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.10157, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1940 - accuracy: 0.5354 - val_loss: 2.1016 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1365 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.09105, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 12s/step - loss: 2.1365 - accuracy: 0.5382 - val_loss: 2.0911 - val_accuracy: 0.6207\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1674 - accuracy: 0.5297 \n",
      "Epoch 00001: val_loss improved from inf to 2.07820, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s5-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 149s 13s/step - loss: 2.1674 - accuracy: 0.5297 - val_loss: 2.0782 - val_accuracy: 0.6322\n",
      "Evaluating smic-s5\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3772 - accuracy: 0.4788 \n",
      "Epoch 00001: val_loss improved from inf to 2.26025, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 127s 10s/step - loss: 2.3772 - accuracy: 0.4788 - val_loss: 2.2603 - val_accuracy: 0.5287\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1900 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.20083, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 10s/step - loss: 2.1900 - accuracy: 0.5637 - val_loss: 2.2008 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1934 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.18869, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1934 - accuracy: 0.5467 - val_loss: 2.1887 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1485 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.18133, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1485 - accuracy: 0.5666 - val_loss: 2.1813 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1478 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.16717, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 130s 11s/step - loss: 2.1478 - accuracy: 0.5666 - val_loss: 2.1672 - val_accuracy: 0.5747\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1014 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.16050, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1014 - accuracy: 0.5694 - val_loss: 2.1605 - val_accuracy: 0.5632\n",
      "Evaluating casme2-sub20\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3829 - accuracy: 0.4703 \n",
      "Epoch 00001: val_loss improved from inf to 2.27932, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 126s 10s/step - loss: 2.3829 - accuracy: 0.4703 - val_loss: 2.2793 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1601 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.20665, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1601 - accuracy: 0.5807 - val_loss: 2.2066 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1619 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.21363, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1619 - accuracy: 0.5722 - val_loss: 2.2136 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1420 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 2.20256, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1420 - accuracy: 0.5892 - val_loss: 2.2026 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1176 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.19089, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 10s/step - loss: 2.1176 - accuracy: 0.5779 - val_loss: 2.1909 - val_accuracy: 0.5190\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1340 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.19150, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s13-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1340 - accuracy: 0.5637 - val_loss: 2.1915 - val_accuracy: 0.5190\n",
      "Evaluating smic-s13\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3746 - accuracy: 0.4844 \n",
      "Epoch 00001: val_loss improved from inf to 2.22204, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 127s 11s/step - loss: 2.3746 - accuracy: 0.4844 - val_loss: 2.2220 - val_accuracy: 0.5349\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1748 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.14906, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1748 - accuracy: 0.5637 - val_loss: 2.1491 - val_accuracy: 0.5349\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1937 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.14701, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1937 - accuracy: 0.5581 - val_loss: 2.1470 - val_accuracy: 0.5349\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1608 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.13320, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1608 - accuracy: 0.5411 - val_loss: 2.1332 - val_accuracy: 0.5349\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1390 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.11783, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1390 - accuracy: 0.5836 - val_loss: 2.1178 - val_accuracy: 0.5349\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1391 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.11087, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub01-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1391 - accuracy: 0.5836 - val_loss: 2.1109 - val_accuracy: 0.5349\n",
      "Evaluating casme2-sub01\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3666 - accuracy: 0.4561 \n",
      "Epoch 00001: val_loss improved from inf to 2.17630, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 127s 11s/step - loss: 2.3666 - accuracy: 0.4561 - val_loss: 2.1763 - val_accuracy: 0.5814\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1835 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.15502, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 126s 10s/step - loss: 2.1835 - accuracy: 0.5354 - val_loss: 2.1550 - val_accuracy: 0.5814\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1763 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.13870, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1763 - accuracy: 0.5467 - val_loss: 2.1387 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1308 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.13042, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1308 - accuracy: 0.5637 - val_loss: 2.1304 - val_accuracy: 0.5814\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1296 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.12101, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1296 - accuracy: 0.5751 - val_loss: 2.1210 - val_accuracy: 0.5814\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0966 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.11597, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-034-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.0966 - accuracy: 0.5524 - val_loss: 2.1160 - val_accuracy: 0.5814\n",
      "Evaluating samm-034\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3840 - accuracy: 0.4674 \n",
      "Epoch 00001: val_loss improved from inf to 2.16422, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 126s 10s/step - loss: 2.3840 - accuracy: 0.4674 - val_loss: 2.1642 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1605 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.13267, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1605 - accuracy: 0.5836 - val_loss: 2.1327 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1679 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.11596, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1679 - accuracy: 0.5382 - val_loss: 2.1160 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1333 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.10021, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1333 - accuracy: 0.5694 - val_loss: 2.1002 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1271 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.09450, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1271 - accuracy: 0.5637 - val_loss: 2.0945 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1523 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.08062, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1523 - accuracy: 0.5326 - val_loss: 2.0806 - val_accuracy: 0.6118\n",
      "Evaluating smic-s15\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3417 - accuracy: 0.4873 \n",
      "Epoch 00001: val_loss improved from inf to 2.20427, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 127s 11s/step - loss: 2.3417 - accuracy: 0.4873 - val_loss: 2.2043 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1714 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.13588, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 2.1714 - accuracy: 0.5779 - val_loss: 2.1359 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1783 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.12843, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 2.1783 - accuracy: 0.5694 - val_loss: 2.1284 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1349 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.12196, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1349 - accuracy: 0.5581 - val_loss: 2.1220 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1517 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.11375, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 2.1517 - accuracy: 0.5722 - val_loss: 2.1137 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1147 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.10432, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s12-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1147 - accuracy: 0.5836 - val_loss: 2.1043 - val_accuracy: 0.5625\n",
      "Evaluating smic-s12\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3921 - accuracy: 0.4873 \n",
      "Epoch 00001: val_loss improved from inf to 2.10174, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 126s 10s/step - loss: 2.3921 - accuracy: 0.4873 - val_loss: 2.1017 - val_accuracy: 0.6076\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2087 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.08283, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.2087 - accuracy: 0.5581 - val_loss: 2.0828 - val_accuracy: 0.6076\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1823 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.07493, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1823 - accuracy: 0.5552 - val_loss: 2.0749 - val_accuracy: 0.6203\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1592 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.06489, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1592 - accuracy: 0.5581 - val_loss: 2.0649 - val_accuracy: 0.6203\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1486 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.05549, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1486 - accuracy: 0.5524 - val_loss: 2.0555 - val_accuracy: 0.6203\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1173 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 2.05150, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1173 - accuracy: 0.5892 - val_loss: 2.0515 - val_accuracy: 0.6203\n",
      "Evaluating smic-s14\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3876 - accuracy: 0.4618 \n",
      "Epoch 00001: val_loss improved from inf to 2.09094, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 126s 10s/step - loss: 2.3876 - accuracy: 0.4618 - val_loss: 2.0909 - val_accuracy: 0.6232\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2197 - accuracy: 0.5184 \n",
      "Epoch 00001: val_loss improved from inf to 2.08603, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.2197 - accuracy: 0.5184 - val_loss: 2.0860 - val_accuracy: 0.6087\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2414 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.08688, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.2414 - accuracy: 0.5269 - val_loss: 2.0869 - val_accuracy: 0.6087\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1849 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.07621, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 127s 11s/step - loss: 2.1849 - accuracy: 0.5382 - val_loss: 2.0762 - val_accuracy: 0.6087\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1923 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.06587, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1923 - accuracy: 0.5609 - val_loss: 2.0659 - val_accuracy: 0.6232\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1488 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.05688, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-011-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 129s 11s/step - loss: 2.1488 - accuracy: 0.5496 - val_loss: 2.0569 - val_accuracy: 0.6232\n",
      "Evaluating samm-011\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4028 - accuracy: 0.4759 \n",
      "Epoch 00001: val_loss improved from inf to 2.26106, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 128s 11s/step - loss: 2.4028 - accuracy: 0.4759 - val_loss: 2.2611 - val_accuracy: 0.5556\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2315 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.20157, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.2315 - accuracy: 0.5524 - val_loss: 2.2016 - val_accuracy: 0.5556\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1933 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.18422, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1933 - accuracy: 0.5354 - val_loss: 2.1842 - val_accuracy: 0.5556\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1902 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.17051, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1902 - accuracy: 0.5439 - val_loss: 2.1705 - val_accuracy: 0.5679\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1153 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.16420, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1153 - accuracy: 0.5694 - val_loss: 2.1642 - val_accuracy: 0.5679\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1354 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.15922, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-035-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 128s 11s/step - loss: 2.1354 - accuracy: 0.5836 - val_loss: 2.1592 - val_accuracy: 0.5679\n",
      "Evaluating samm-035\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3766 - accuracy: 0.4873 \n",
      "Epoch 00001: val_loss improved from inf to 2.10771, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 123s 10s/step - loss: 2.3766 - accuracy: 0.4873 - val_loss: 2.1077 - val_accuracy: 0.6471\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1894 - accuracy: 0.6062 \n",
      "Epoch 00001: val_loss improved from inf to 2.09311, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1894 - accuracy: 0.6062 - val_loss: 2.0931 - val_accuracy: 0.6471\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1585 - accuracy: 0.5864 \n",
      "Epoch 00001: val_loss improved from inf to 2.08573, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 125s 10s/step - loss: 2.1585 - accuracy: 0.5864 - val_loss: 2.0857 - val_accuracy: 0.6471\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1423 - accuracy: 0.6034 \n",
      "Epoch 00001: val_loss improved from inf to 2.08235, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1423 - accuracy: 0.6034 - val_loss: 2.0823 - val_accuracy: 0.6471\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1263 - accuracy: 0.6006 \n",
      "Epoch 00001: val_loss improved from inf to 2.07220, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1263 - accuracy: 0.6006 - val_loss: 2.0722 - val_accuracy: 0.6471\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1215 - accuracy: 0.5921 \n",
      "Epoch 00001: val_loss improved from inf to 2.07062, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s1-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1215 - accuracy: 0.5921 - val_loss: 2.0706 - val_accuracy: 0.6471\n",
      "Evaluating smic-s1\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3900 - accuracy: 0.5099 \n",
      "Epoch 00001: val_loss improved from inf to 2.26239, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 134s 11s/step - loss: 2.3900 - accuracy: 0.5099 - val_loss: 2.2624 - val_accuracy: 0.5176\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1967 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.18143, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1967 - accuracy: 0.5581 - val_loss: 2.1814 - val_accuracy: 0.5176\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1596 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.18831, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1596 - accuracy: 0.5637 - val_loss: 2.1883 - val_accuracy: 0.5176\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1503 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.18220, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1503 - accuracy: 0.5666 - val_loss: 2.1822 - val_accuracy: 0.5176\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1385 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.16990, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1385 - accuracy: 0.5779 - val_loss: 2.1699 - val_accuracy: 0.5176\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0958 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 2.17035, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s9-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.0958 - accuracy: 0.5892 - val_loss: 2.1703 - val_accuracy: 0.5294\n",
      "Evaluating smic-s9\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3872 - accuracy: 0.4618 \n",
      "Epoch 00001: val_loss improved from inf to 2.38432, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 134s 11s/step - loss: 2.3872 - accuracy: 0.4618 - val_loss: 2.3843 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1504 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.29341, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1504 - accuracy: 0.5722 - val_loss: 2.2934 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1519 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.27354, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1519 - accuracy: 0.5694 - val_loss: 2.2735 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1539 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.26193, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1539 - accuracy: 0.5779 - val_loss: 2.2619 - val_accuracy: 0.4824\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1203 - accuracy: 0.5864 \n",
      "Epoch 00001: val_loss improved from inf to 2.24678, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1203 - accuracy: 0.5864 - val_loss: 2.2468 - val_accuracy: 0.4941\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1032 - accuracy: 0.5977 \n",
      "Epoch 00001: val_loss improved from inf to 2.23639, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub06-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1032 - accuracy: 0.5977 - val_loss: 2.2364 - val_accuracy: 0.4941\n",
      "Evaluating casme2-sub06\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3709 - accuracy: 0.4674 \n",
      "Epoch 00001: val_loss improved from inf to 2.17760, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 133s 11s/step - loss: 2.3709 - accuracy: 0.4674 - val_loss: 2.1776 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1805 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.13268, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1805 - accuracy: 0.5439 - val_loss: 2.1327 - val_accuracy: 0.5581\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1540 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.11756, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 2.1540 - accuracy: 0.5382 - val_loss: 2.1176 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1447 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.11050, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 2.1447 - accuracy: 0.5411 - val_loss: 2.1105 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1082 - accuracy: 0.6034 \n",
      "Epoch 00001: val_loss improved from inf to 2.10189, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 2.1082 - accuracy: 0.6034 - val_loss: 2.1019 - val_accuracy: 0.5698\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1222 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.09581, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub15-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 134s 11s/step - loss: 2.1222 - accuracy: 0.5411 - val_loss: 2.0958 - val_accuracy: 0.5814\n",
      "Evaluating casme2-sub15\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3708 - accuracy: 0.4646 \n",
      "Epoch 00001: val_loss improved from inf to 2.03648, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 132s 11s/step - loss: 2.3708 - accuracy: 0.4646 - val_loss: 2.0365 - val_accuracy: 0.6512\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2261 - accuracy: 0.5071 \n",
      "Epoch 00001: val_loss improved from inf to 2.05034, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 139s 12s/step - loss: 2.2261 - accuracy: 0.5071 - val_loss: 2.0503 - val_accuracy: 0.6744\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1988 - accuracy: 0.5042 \n",
      "Epoch 00001: val_loss improved from inf to 2.05066, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1988 - accuracy: 0.5042 - val_loss: 2.0507 - val_accuracy: 0.6744\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1476 - accuracy: 0.5071 \n",
      "Epoch 00001: val_loss improved from inf to 2.04016, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.1476 - accuracy: 0.5071 - val_loss: 2.0402 - val_accuracy: 0.6744\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1264 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.03031, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1264 - accuracy: 0.5552 - val_loss: 2.0303 - val_accuracy: 0.6744\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1195 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.01627, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-012-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1195 - accuracy: 0.5524 - val_loss: 2.0163 - val_accuracy: 0.6744\n",
      "Evaluating samm-012\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4289 - accuracy: 0.4844 \n",
      "Epoch 00001: val_loss improved from inf to 2.10084, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 131s 11s/step - loss: 2.4289 - accuracy: 0.4844 - val_loss: 2.1008 - val_accuracy: 0.6118\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2355 - accuracy: 0.5297 \n",
      "Epoch 00001: val_loss improved from inf to 2.08972, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.2355 - accuracy: 0.5297 - val_loss: 2.0897 - val_accuracy: 0.6353\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2086 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.07681, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.2086 - accuracy: 0.5637 - val_loss: 2.0768 - val_accuracy: 0.6353\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1713 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.06700, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 139s 12s/step - loss: 2.1713 - accuracy: 0.5411 - val_loss: 2.0670 - val_accuracy: 0.6353\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1305 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.05870, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1305 - accuracy: 0.5722 - val_loss: 2.0587 - val_accuracy: 0.6471\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1510 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.05016, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub11-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1510 - accuracy: 0.5581 - val_loss: 2.0502 - val_accuracy: 0.6588\n",
      "Evaluating casme2-sub11\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3704 - accuracy: 0.5156 \n",
      "Epoch 00001: val_loss improved from inf to 2.15888, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 135s 11s/step - loss: 2.3704 - accuracy: 0.5156 - val_loss: 2.1589 - val_accuracy: 0.5823\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2131 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.12477, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.2131 - accuracy: 0.5496 - val_loss: 2.1248 - val_accuracy: 0.5823\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1767 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.10984, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1767 - accuracy: 0.5439 - val_loss: 2.1098 - val_accuracy: 0.5823\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1647 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.10117, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1647 - accuracy: 0.5581 - val_loss: 2.1012 - val_accuracy: 0.5823\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1541 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.08729, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1541 - accuracy: 0.5581 - val_loss: 2.0873 - val_accuracy: 0.5823\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1346 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.08000, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub09-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1346 - accuracy: 0.5751 - val_loss: 2.0800 - val_accuracy: 0.6076\n",
      "Evaluating casme2-sub09\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4188 - accuracy: 0.4419 \n",
      "Epoch 00001: val_loss improved from inf to 2.10222, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 132s 11s/step - loss: 2.4188 - accuracy: 0.4419 - val_loss: 2.1022 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2329 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.08951, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.2329 - accuracy: 0.5382 - val_loss: 2.0895 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1903 - accuracy: 0.5297 \n",
      "Epoch 00001: val_loss improved from inf to 2.08262, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1903 - accuracy: 0.5297 - val_loss: 2.0826 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1628 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.07838, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1628 - accuracy: 0.5439 - val_loss: 2.0784 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1599 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.07295, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1599 - accuracy: 0.5581 - val_loss: 2.0729 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1512 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.05970, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub24-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1512 - accuracy: 0.5552 - val_loss: 2.0597 - val_accuracy: 0.6279\n",
      "Evaluating casme2-sub24\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4237 - accuracy: 0.4844 \n",
      "Epoch 00001: val_loss improved from inf to 2.24418, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 131s 11s/step - loss: 2.4237 - accuracy: 0.4844 - val_loss: 2.2442 - val_accuracy: 0.5647\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1894 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.21663, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1894 - accuracy: 0.5552 - val_loss: 2.2166 - val_accuracy: 0.5647\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1859 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.20087, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1859 - accuracy: 0.5581 - val_loss: 2.2009 - val_accuracy: 0.5647\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1772 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.19429, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1772 - accuracy: 0.5609 - val_loss: 2.1943 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1672 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.17971, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1672 - accuracy: 0.5581 - val_loss: 2.1797 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1483 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.16876, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-009-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 131s 11s/step - loss: 2.1483 - accuracy: 0.5779 - val_loss: 2.1688 - val_accuracy: 0.5765\n",
      "Evaluating samm-009\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3533 - accuracy: 0.4901 \n",
      "Epoch 00001: val_loss improved from inf to 2.13572, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 131s 11s/step - loss: 2.3533 - accuracy: 0.4901 - val_loss: 2.1357 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1846 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.11980, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1846 - accuracy: 0.5439 - val_loss: 2.1198 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2133 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.11756, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.2133 - accuracy: 0.5269 - val_loss: 2.1176 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1603 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.10942, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1603 - accuracy: 0.5467 - val_loss: 2.1094 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1490 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.09583, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1490 - accuracy: 0.5382 - val_loss: 2.0958 - val_accuracy: 0.6000\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0972 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.09243, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-017-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.0972 - accuracy: 0.5807 - val_loss: 2.0924 - val_accuracy: 0.6000\n",
      "Evaluating samm-017\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3273 - accuracy: 0.4929 \n",
      "Epoch 00001: val_loss improved from inf to 2.27364, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 132s 11s/step - loss: 2.3273 - accuracy: 0.4929 - val_loss: 2.2736 - val_accuracy: 0.5119\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1666 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.17820, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1666 - accuracy: 0.5581 - val_loss: 2.1782 - val_accuracy: 0.5119\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1476 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.16417, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1476 - accuracy: 0.5496 - val_loss: 2.1642 - val_accuracy: 0.5357\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1403 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.15464, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.1403 - accuracy: 0.5694 - val_loss: 2.1546 - val_accuracy: 0.5357\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0782 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.15302, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.0782 - accuracy: 0.5836 - val_loss: 2.1530 - val_accuracy: 0.5357\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0987 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.15569, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-022-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 133s 11s/step - loss: 2.0987 - accuracy: 0.5694 - val_loss: 2.1557 - val_accuracy: 0.5357\n",
      "Evaluating samm-022\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3909 - accuracy: 0.4674 \n",
      "Epoch 00001: val_loss improved from inf to 2.19410, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 131s 11s/step - loss: 2.3909 - accuracy: 0.4674 - val_loss: 2.1941 - val_accuracy: 0.5522\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2279 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.13725, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.2279 - accuracy: 0.5269 - val_loss: 2.1373 - val_accuracy: 0.5672\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1722 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.12649, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1722 - accuracy: 0.5581 - val_loss: 2.1265 - val_accuracy: 0.5672\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1727 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.12372, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1727 - accuracy: 0.5467 - val_loss: 2.1237 - val_accuracy: 0.5522\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1597 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.11003, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1597 - accuracy: 0.5722 - val_loss: 2.1100 - val_accuracy: 0.5821\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1314 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.11084, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s20-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 132s 11s/step - loss: 2.1314 - accuracy: 0.5836 - val_loss: 2.1108 - val_accuracy: 0.5821\n",
      "Evaluating smic-s20\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3775 - accuracy: 0.4646 \n",
      "Epoch 00001: val_loss improved from inf to 2.21906, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 133s 11s/step - loss: 2.3775 - accuracy: 0.4646 - val_loss: 2.2191 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2326 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.18160, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.2326 - accuracy: 0.5467 - val_loss: 2.1816 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1797 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.16660, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1797 - accuracy: 0.5722 - val_loss: 2.1666 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1659 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.15932, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1659 - accuracy: 0.5439 - val_loss: 2.1593 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1513 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.15424, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1513 - accuracy: 0.5836 - val_loss: 2.1542 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1354 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.14600, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub05-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1354 - accuracy: 0.5694 - val_loss: 2.1460 - val_accuracy: 0.5783\n",
      "Evaluating casme2-sub05\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4324 - accuracy: 0.4448 \n",
      "Epoch 00001: val_loss improved from inf to 2.10613, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 136s 11s/step - loss: 2.4324 - accuracy: 0.4448 - val_loss: 2.1061 - val_accuracy: 0.6279\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2265 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.09914, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.2265 - accuracy: 0.5354 - val_loss: 2.0991 - val_accuracy: 0.6512\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2157 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.08264, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.2157 - accuracy: 0.5411 - val_loss: 2.0826 - val_accuracy: 0.6512\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1665 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.07438, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1665 - accuracy: 0.5666 - val_loss: 2.0744 - val_accuracy: 0.6512\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1675 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.05379, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1675 - accuracy: 0.5581 - val_loss: 2.0538 - val_accuracy: 0.6512\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1644 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.04670, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub14-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1644 - accuracy: 0.5382 - val_loss: 2.0467 - val_accuracy: 0.6512\n",
      "Evaluating casme2-sub14\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4100 - accuracy: 0.5014 \n",
      "Epoch 00001: val_loss improved from inf to 2.36248, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 135s 11s/step - loss: 2.4100 - accuracy: 0.5014 - val_loss: 2.3625 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2009 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.28135, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.2009 - accuracy: 0.5779 - val_loss: 2.2814 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2001 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.25365, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.2001 - accuracy: 0.5779 - val_loss: 2.2536 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1668 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.23758, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1668 - accuracy: 0.5637 - val_loss: 2.2376 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1730 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.23131, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1730 - accuracy: 0.5609 - val_loss: 2.2313 - val_accuracy: 0.5000\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1602 - accuracy: 0.5949 \n",
      "Epoch 00001: val_loss improved from inf to 2.21436, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-006-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1602 - accuracy: 0.5949 - val_loss: 2.2144 - val_accuracy: 0.5000\n",
      "Evaluating samm-006\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3747 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.23170, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 140s 12s/step - loss: 2.3747 - accuracy: 0.5241 - val_loss: 2.2317 - val_accuracy: 0.5341\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2040 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.17021, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 141s 12s/step - loss: 2.2040 - accuracy: 0.5496 - val_loss: 2.1702 - val_accuracy: 0.5341\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1667 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.15840, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.1667 - accuracy: 0.5581 - val_loss: 2.1584 - val_accuracy: 0.5455\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1261 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.15214, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.1261 - accuracy: 0.5694 - val_loss: 2.1521 - val_accuracy: 0.5568\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1170 - accuracy: 0.5921 \n",
      "Epoch 00001: val_loss improved from inf to 2.14254, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.1170 - accuracy: 0.5921 - val_loss: 2.1425 - val_accuracy: 0.5455\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1141 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.13383, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub08-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.1141 - accuracy: 0.5694 - val_loss: 2.1338 - val_accuracy: 0.5455\n",
      "Evaluating casme2-sub08\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4101 - accuracy: 0.4788 \n",
      "Epoch 00001: val_loss improved from inf to 2.33181, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 140s 12s/step - loss: 2.4101 - accuracy: 0.4788 - val_loss: 2.3318 - val_accuracy: 0.4659\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1776 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.24394, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.1776 - accuracy: 0.5722 - val_loss: 2.2439 - val_accuracy: 0.4659\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1531 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.23795, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 140s 12s/step - loss: 2.1531 - accuracy: 0.5779 - val_loss: 2.2380 - val_accuracy: 0.4659\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1312 - accuracy: 0.6091 \n",
      "Epoch 00001: val_loss improved from inf to 2.22313, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1312 - accuracy: 0.6091 - val_loss: 2.2231 - val_accuracy: 0.4659\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1424 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.20140, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1424 - accuracy: 0.5694 - val_loss: 2.2014 - val_accuracy: 0.4773\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.0847 - accuracy: 0.6006 \n",
      "Epoch 00001: val_loss improved from inf to 2.20028, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-024-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.0847 - accuracy: 0.6006 - val_loss: 2.2003 - val_accuracy: 0.4773\n",
      "Evaluating samm-024\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3863 - accuracy: 0.4504 \n",
      "Epoch 00001: val_loss improved from inf to 2.07159, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 136s 11s/step - loss: 2.3863 - accuracy: 0.4504 - val_loss: 2.0716 - val_accuracy: 0.6591\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1865 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.08487, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1865 - accuracy: 0.5411 - val_loss: 2.0849 - val_accuracy: 0.6364\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1980 - accuracy: 0.5071 \n",
      "Epoch 00001: val_loss improved from inf to 2.05532, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 139s 12s/step - loss: 2.1980 - accuracy: 0.5071 - val_loss: 2.0553 - val_accuracy: 0.6591\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1639 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.05789, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1639 - accuracy: 0.5241 - val_loss: 2.0579 - val_accuracy: 0.6705\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1157 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.04815, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1157 - accuracy: 0.5524 - val_loss: 2.0481 - val_accuracy: 0.6705\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1258 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.03158, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-037-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1258 - accuracy: 0.5326 - val_loss: 2.0316 - val_accuracy: 0.6705\n",
      "Evaluating samm-037\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4074 - accuracy: 0.4448 \n",
      "Epoch 00001: val_loss improved from inf to 2.25159, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 136s 11s/step - loss: 2.4074 - accuracy: 0.4448 - val_loss: 2.2516 - val_accuracy: 0.5732\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1987 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.21167, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1987 - accuracy: 0.5694 - val_loss: 2.2117 - val_accuracy: 0.5610\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1599 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.19264, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 141s 12s/step - loss: 2.1599 - accuracy: 0.5581 - val_loss: 2.1926 - val_accuracy: 0.5610\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1199 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.19065, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1199 - accuracy: 0.5807 - val_loss: 2.1907 - val_accuracy: 0.5854\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1166 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.18807, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1166 - accuracy: 0.5722 - val_loss: 2.1881 - val_accuracy: 0.5732\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1356 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.18174, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s18-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1356 - accuracy: 0.5637 - val_loss: 2.1817 - val_accuracy: 0.5732\n",
      "Evaluating smic-s18\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3265 - accuracy: 0.5014 \n",
      "Epoch 00001: val_loss improved from inf to 2.20716, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 134s 11s/step - loss: 2.3265 - accuracy: 0.5014 - val_loss: 2.2072 - val_accuracy: 0.5696\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2059 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.15365, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.2059 - accuracy: 0.5581 - val_loss: 2.1536 - val_accuracy: 0.5696\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1570 - accuracy: 0.5297 \n",
      "Epoch 00001: val_loss improved from inf to 2.14151, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1570 - accuracy: 0.5297 - val_loss: 2.1415 - val_accuracy: 0.5696\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1472 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.12840, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1472 - accuracy: 0.5467 - val_loss: 2.1284 - val_accuracy: 0.5696\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1253 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.11732, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1253 - accuracy: 0.5666 - val_loss: 2.1173 - val_accuracy: 0.5696\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1195 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.10525, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-014-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.1195 - accuracy: 0.5751 - val_loss: 2.1052 - val_accuracy: 0.5823\n",
      "Evaluating samm-014\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3750 - accuracy: 0.4703 \n",
      "Epoch 00001: val_loss improved from inf to 2.27644, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 134s 11s/step - loss: 2.3750 - accuracy: 0.4703 - val_loss: 2.2764 - val_accuracy: 0.5185\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2313 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.17195, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 135s 11s/step - loss: 2.2313 - accuracy: 0.5637 - val_loss: 2.1720 - val_accuracy: 0.5185\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2097 - accuracy: 0.5326 \n",
      "Epoch 00001: val_loss improved from inf to 2.15432, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.2097 - accuracy: 0.5326 - val_loss: 2.1543 - val_accuracy: 0.5185\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1574 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.14702, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1574 - accuracy: 0.5666 - val_loss: 2.1470 - val_accuracy: 0.5185\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1624 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.13444, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1624 - accuracy: 0.5496 - val_loss: 2.1344 - val_accuracy: 0.5309\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1448 - accuracy: 0.5297 \n",
      "Epoch 00001: val_loss improved from inf to 2.12718, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub23-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1448 - accuracy: 0.5297 - val_loss: 2.1272 - val_accuracy: 0.5309\n",
      "Evaluating casme2-sub23\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4407 - accuracy: 0.4504 \n",
      "Epoch 00001: val_loss improved from inf to 2.16919, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 136s 11s/step - loss: 2.4407 - accuracy: 0.4504 - val_loss: 2.1692 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2181 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.14684, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.2181 - accuracy: 0.5552 - val_loss: 2.1468 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1870 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.13118, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1870 - accuracy: 0.5552 - val_loss: 2.1312 - val_accuracy: 0.5647\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1684 - accuracy: 0.5836 \n",
      "Epoch 00001: val_loss improved from inf to 2.12315, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1684 - accuracy: 0.5836 - val_loss: 2.1231 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1771 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.11312, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1771 - accuracy: 0.5666 - val_loss: 2.1131 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1204 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.10418, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-032-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1204 - accuracy: 0.5666 - val_loss: 2.1042 - val_accuracy: 0.5765\n",
      "Evaluating samm-032\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3495 - accuracy: 0.4419 \n",
      "Epoch 00001: val_loss improved from inf to 2.16231, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 137s 11s/step - loss: 2.3495 - accuracy: 0.4419 - val_loss: 2.1623 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1722 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.15388, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1722 - accuracy: 0.5241 - val_loss: 2.1539 - val_accuracy: 0.5930\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1947 - accuracy: 0.5099 \n",
      "Epoch 00001: val_loss improved from inf to 2.12811, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1947 - accuracy: 0.5099 - val_loss: 2.1281 - val_accuracy: 0.6047\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1379 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.12376, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1379 - accuracy: 0.5552 - val_loss: 2.1238 - val_accuracy: 0.6163\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1198 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.12150, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1198 - accuracy: 0.5496 - val_loss: 2.1215 - val_accuracy: 0.6163\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1055 - accuracy: 0.5864 \n",
      "Epoch 00001: val_loss improved from inf to 2.11122, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-030-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1055 - accuracy: 0.5864 - val_loss: 2.1112 - val_accuracy: 0.6047\n",
      "Evaluating samm-030\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3624 - accuracy: 0.4759 \n",
      "Epoch 00001: val_loss improved from inf to 2.14240, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 137s 11s/step - loss: 2.3624 - accuracy: 0.4759 - val_loss: 2.1424 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2219 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.11918, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.2219 - accuracy: 0.5439 - val_loss: 2.1192 - val_accuracy: 0.5682\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1851 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.10685, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1851 - accuracy: 0.5269 - val_loss: 2.1068 - val_accuracy: 0.5909\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1539 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.09801, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1539 - accuracy: 0.5581 - val_loss: 2.0980 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1355 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.09197, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1355 - accuracy: 0.5467 - val_loss: 2.0920 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1456 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.07203, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-023-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1456 - accuracy: 0.5467 - val_loss: 2.0720 - val_accuracy: 0.6250\n",
      "Evaluating samm-023\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3673 - accuracy: 0.4504 \n",
      "Epoch 00001: val_loss improved from inf to 2.13515, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 139s 12s/step - loss: 2.3673 - accuracy: 0.4504 - val_loss: 2.1351 - val_accuracy: 0.6024\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2246 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.12280, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.2246 - accuracy: 0.5439 - val_loss: 2.1228 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2202 - accuracy: 0.5184 \n",
      "Epoch 00001: val_loss improved from inf to 2.11798, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.2202 - accuracy: 0.5184 - val_loss: 2.1180 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1707 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.10351, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1707 - accuracy: 0.5241 - val_loss: 2.1035 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1227 - accuracy: 0.5666 \n",
      "Epoch 00001: val_loss improved from inf to 2.10638, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1227 - accuracy: 0.5666 - val_loss: 2.1064 - val_accuracy: 0.5783\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1454 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.09172, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-013-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 138s 11s/step - loss: 2.1454 - accuracy: 0.5552 - val_loss: 2.0917 - val_accuracy: 0.5783\n",
      "Evaluating samm-013\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4042 - accuracy: 0.4674 \n",
      "Epoch 00001: val_loss improved from inf to 2.19435, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 136s 11s/step - loss: 2.4042 - accuracy: 0.4674 - val_loss: 2.1944 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1952 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.16684, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1952 - accuracy: 0.5694 - val_loss: 2.1668 - val_accuracy: 0.5882\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1905 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.15285, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 136s 11s/step - loss: 2.1905 - accuracy: 0.5524 - val_loss: 2.1529 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1557 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 2.14283, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1557 - accuracy: 0.5892 - val_loss: 2.1428 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1504 - accuracy: 0.5552 \n",
      "Epoch 00001: val_loss improved from inf to 2.12184, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 137s 11s/step - loss: 2.1504 - accuracy: 0.5552 - val_loss: 2.1218 - val_accuracy: 0.5765\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1314 - accuracy: 0.5609 \n",
      "Epoch 00001: val_loss improved from inf to 2.11469, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-020-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 166s 13s/step - loss: 2.1314 - accuracy: 0.5609 - val_loss: 2.1147 - val_accuracy: 0.5765\n",
      "Evaluating samm-020\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3503 - accuracy: 0.4901 \n",
      "Epoch 00001: val_loss improved from inf to 2.19643, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 159s 13s/step - loss: 2.3503 - accuracy: 0.4901 - val_loss: 2.1964 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1687 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.14264, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 161s 13s/step - loss: 2.1687 - accuracy: 0.5439 - val_loss: 2.1426 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1800 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.13311, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 160s 13s/step - loss: 2.1800 - accuracy: 0.5411 - val_loss: 2.1331 - val_accuracy: 0.5690\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1550 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.12367, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 160s 13s/step - loss: 2.1550 - accuracy: 0.5241 - val_loss: 2.1237 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1319 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.11490, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 160s 13s/step - loss: 2.1319 - accuracy: 0.5467 - val_loss: 2.1149 - val_accuracy: 0.5862\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1394 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.10779, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub17-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 160s 13s/step - loss: 2.1394 - accuracy: 0.5241 - val_loss: 2.1078 - val_accuracy: 0.5862\n",
      "Evaluating casme2-sub17\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4154 - accuracy: 0.4618 \n",
      "Epoch 00001: val_loss improved from inf to 2.17742, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 162s 13s/step - loss: 2.4154 - accuracy: 0.4618 - val_loss: 2.1774 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2107 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.14079, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.2107 - accuracy: 0.5269 - val_loss: 2.1408 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1975 - accuracy: 0.5184 \n",
      "Epoch 00001: val_loss improved from inf to 2.13070, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 162s 13s/step - loss: 2.1975 - accuracy: 0.5184 - val_loss: 2.1307 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1211 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.11823, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 13s/step - loss: 2.1211 - accuracy: 0.5439 - val_loss: 2.1182 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1463 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.11182, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 13s/step - loss: 2.1463 - accuracy: 0.5382 - val_loss: 2.1118 - val_accuracy: 0.5625\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1671 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.10494, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-026-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 14s/step - loss: 2.1671 - accuracy: 0.5439 - val_loss: 2.1049 - val_accuracy: 0.5750\n",
      "Evaluating samm-026\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4059 - accuracy: 0.4391 \n",
      "Epoch 00001: val_loss improved from inf to 1.98453, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 163s 14s/step - loss: 2.4059 - accuracy: 0.4391 - val_loss: 1.9845 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2296 - accuracy: 0.5354 \n",
      "Epoch 00001: val_loss improved from inf to 2.00888, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 165s 14s/step - loss: 2.2296 - accuracy: 0.5354 - val_loss: 2.0089 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2188 - accuracy: 0.4929 \n",
      "Epoch 00001: val_loss improved from inf to 2.01965, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.2188 - accuracy: 0.4929 - val_loss: 2.0197 - val_accuracy: 0.7024\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1707 - accuracy: 0.5042 \n",
      "Epoch 00001: val_loss improved from inf to 2.00899, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 165s 14s/step - loss: 2.1707 - accuracy: 0.5042 - val_loss: 2.0090 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1368 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 1.99133, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1368 - accuracy: 0.5439 - val_loss: 1.9913 - val_accuracy: 0.6905\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1247 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 1.98994, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-033-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 165s 14s/step - loss: 2.1247 - accuracy: 0.5496 - val_loss: 1.9899 - val_accuracy: 0.6905\n",
      "Evaluating samm-033\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4100 - accuracy: 0.4221 \n",
      "Epoch 00001: val_loss improved from inf to 1.92636, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 161s 13s/step - loss: 2.4100 - accuracy: 0.4221 - val_loss: 1.9264 - val_accuracy: 0.7286\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2122 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 1.95100, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 14s/step - loss: 2.2122 - accuracy: 0.5241 - val_loss: 1.9510 - val_accuracy: 0.7143\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2023 - accuracy: 0.5071 \n",
      "Epoch 00001: val_loss improved from inf to 1.95045, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.2023 - accuracy: 0.5071 - val_loss: 1.9505 - val_accuracy: 0.7429\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1791 - accuracy: 0.5014 \n",
      "Epoch 00001: val_loss improved from inf to 1.95182, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 14s/step - loss: 2.1791 - accuracy: 0.5014 - val_loss: 1.9518 - val_accuracy: 0.7429\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1534 - accuracy: 0.5297 \n",
      "Epoch 00001: val_loss improved from inf to 1.92466, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1534 - accuracy: 0.5297 - val_loss: 1.9247 - val_accuracy: 0.7429\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1651 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 1.91615, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s4-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 14s/step - loss: 2.1651 - accuracy: 0.5467 - val_loss: 1.9162 - val_accuracy: 0.7429\n",
      "Evaluating smic-s4\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.4124 - accuracy: 0.4901 \n",
      "Epoch 00001: val_loss improved from inf to 2.17528, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 163s 14s/step - loss: 2.4124 - accuracy: 0.4901 - val_loss: 2.1753 - val_accuracy: 0.5795\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2258 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.15339, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.2258 - accuracy: 0.5467 - val_loss: 2.1534 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1731 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.13971, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1731 - accuracy: 0.5439 - val_loss: 2.1397 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1475 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.12150, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 14s/step - loss: 2.1475 - accuracy: 0.5581 - val_loss: 2.1215 - val_accuracy: 0.6136\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1594 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.10716, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1594 - accuracy: 0.5411 - val_loss: 2.1072 - val_accuracy: 0.6023\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1497 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.10707, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-031-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1497 - accuracy: 0.5637 - val_loss: 2.1071 - val_accuracy: 0.6136\n",
      "Evaluating samm-031\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3889 - accuracy: 0.4873 \n",
      "Epoch 00001: val_loss improved from inf to 2.20222, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 162s 13s/step - loss: 2.3889 - accuracy: 0.4873 - val_loss: 2.2022 - val_accuracy: 0.5526\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1915 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.13808, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1915 - accuracy: 0.5439 - val_loss: 2.1381 - val_accuracy: 0.5526\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1562 - accuracy: 0.5694 \n",
      "Epoch 00001: val_loss improved from inf to 2.12572, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 163s 14s/step - loss: 2.1562 - accuracy: 0.5694 - val_loss: 2.1257 - val_accuracy: 0.5658\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1470 - accuracy: 0.5382 \n",
      "Epoch 00001: val_loss improved from inf to 2.11734, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1470 - accuracy: 0.5382 - val_loss: 2.1173 - val_accuracy: 0.5658\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1146 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.11259, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1146 - accuracy: 0.5496 - val_loss: 2.1126 - val_accuracy: 0.5526\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1041 - accuracy: 0.5637 \n",
      "Epoch 00001: val_loss improved from inf to 2.09382, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\smic-s8-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 2.1041 - accuracy: 0.5637 - val_loss: 2.0938 - val_accuracy: 0.5658\n",
      "Evaluating smic-s8\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3797 - accuracy: 0.4788 \n",
      "Epoch 00001: val_loss improved from inf to 2.27606, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 165s 14s/step - loss: 2.3797 - accuracy: 0.4788 - val_loss: 2.2761 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2212 - accuracy: 0.5496 \n",
      "Epoch 00001: val_loss improved from inf to 2.20945, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 166s 14s/step - loss: 2.2212 - accuracy: 0.5496 - val_loss: 2.2095 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1881 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.19808, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1881 - accuracy: 0.5581 - val_loss: 2.1981 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1280 - accuracy: 0.6034 \n",
      "Epoch 00001: val_loss improved from inf to 2.19164, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 166s 14s/step - loss: 2.1280 - accuracy: 0.6034 - val_loss: 2.1916 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1240 - accuracy: 0.5411 \n",
      "Epoch 00001: val_loss improved from inf to 2.17697, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1240 - accuracy: 0.5411 - val_loss: 2.1770 - val_accuracy: 0.5114\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1071 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.17059, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\samm-036-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 166s 14s/step - loss: 2.1071 - accuracy: 0.5807 - val_loss: 2.1706 - val_accuracy: 0.5114\n",
      "Evaluating samm-036\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3733 - accuracy: 0.4788 \n",
      "Epoch 00001: val_loss improved from inf to 2.25420, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 165s 14s/step - loss: 2.3733 - accuracy: 0.4788 - val_loss: 2.2542 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2252 - accuracy: 0.5439 \n",
      "Epoch 00001: val_loss improved from inf to 2.21033, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.2252 - accuracy: 0.5439 - val_loss: 2.2103 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1608 - accuracy: 0.5524 \n",
      "Epoch 00001: val_loss improved from inf to 2.19210, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1608 - accuracy: 0.5524 - val_loss: 2.1921 - val_accuracy: 0.5385\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1663 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.18335, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1663 - accuracy: 0.5581 - val_loss: 2.1833 - val_accuracy: 0.5385\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1518 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.16823, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1518 - accuracy: 0.5722 - val_loss: 2.1682 - val_accuracy: 0.5513\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1026 - accuracy: 0.5807 \n",
      "Epoch 00001: val_loss improved from inf to 2.16544, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub26-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1026 - accuracy: 0.5807 - val_loss: 2.1654 - val_accuracy: 0.5513\n",
      "Evaluating casme2-sub26\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3599 - accuracy: 0.4759 \n",
      "Epoch 00001: val_loss improved from inf to 2.30641, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 165s 14s/step - loss: 2.3599 - accuracy: 0.4759 - val_loss: 2.3064 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1608 - accuracy: 0.5751 \n",
      "Epoch 00001: val_loss improved from inf to 2.23269, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 168s 14s/step - loss: 2.1608 - accuracy: 0.5751 - val_loss: 2.2327 - val_accuracy: 0.5128\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1538 - accuracy: 0.5949 \n",
      "Epoch 00001: val_loss improved from inf to 2.21190, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 169s 14s/step - loss: 2.1538 - accuracy: 0.5949 - val_loss: 2.2119 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1359 - accuracy: 0.5892 \n",
      "Epoch 00001: val_loss improved from inf to 2.20066, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 167s 14s/step - loss: 2.1359 - accuracy: 0.5892 - val_loss: 2.2007 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1078 - accuracy: 0.5921 \n",
      "Epoch 00001: val_loss improved from inf to 2.19255, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 168s 14s/step - loss: 2.1078 - accuracy: 0.5921 - val_loss: 2.1926 - val_accuracy: 0.5256\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1262 - accuracy: 0.5779 \n",
      "Epoch 00001: val_loss improved from inf to 2.18202, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub19-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 168s 14s/step - loss: 2.1262 - accuracy: 0.5779 - val_loss: 2.1820 - val_accuracy: 0.5256\n",
      "Evaluating casme2-sub19\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "-------Iteration Head: 0\n",
      "Learn the head\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.3326 - accuracy: 0.5241 \n",
      "Epoch 00001: val_loss improved from inf to 2.21474, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-head-base1epoch-0901202203.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet\\effNetVenv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 166s 14s/step - loss: 2.3326 - accuracy: 0.5241 - val_loss: 2.2147 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 0\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.2028 - accuracy: 0.5581 \n",
      "Epoch 00001: val_loss improved from inf to 2.17031, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 159s 13s/step - loss: 2.2028 - accuracy: 0.5581 - val_loss: 2.1703 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 1\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1647 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.16764, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 143s 12s/step - loss: 2.1647 - accuracy: 0.5722 - val_loss: 2.1676 - val_accuracy: 0.5517\n",
      "-------Iteration FT: 2\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1595 - accuracy: 0.5467 \n",
      "Epoch 00001: val_loss improved from inf to 2.15986, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 143s 12s/step - loss: 2.1595 - accuracy: 0.5467 - val_loss: 2.1599 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 3\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1899 - accuracy: 0.5269 \n",
      "Epoch 00001: val_loss improved from inf to 2.15000, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 143s 12s/step - loss: 2.1899 - accuracy: 0.5269 - val_loss: 2.1500 - val_accuracy: 0.5632\n",
      "-------Iteration FT: 4\n",
      "Fine-tuning whole network\n",
      "12/12 [==============================] - ETA: 0s - loss: 2.1452 - accuracy: 0.5722 \n",
      "Epoch 00001: val_loss improved from inf to 2.13818, saving model to c:\\Users\\User\\Documents\\UKM\\Project\\Models\\EfficientNet/trainingWeights\\casme2-sub22-finetuning-base1epoch-0901202203_finetune.h5\n",
      "12/12 [==============================] - 142s 12s/step - loss: 2.1452 - accuracy: 0.5722 - val_loss: 2.1382 - val_accuracy: 0.5747\n",
      "Evaluating casme2-sub22\n",
      "'Figure' object has no attribute 'saveFig'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#head Iter : 1, FT Iter : 5\n",
    "#renamed to base1epochAug\n",
    "%run 3classAugmentationExperiment --headEpoch 1 --ftEpoch 1 --expName base1epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 3classTrunc2Experiment --headEpoch 1 --ftEpoch 1  --expName trunc2epoch1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9966458c6ad74b8bc08c1c30471a4ad47794339d9867a51d09ce220173850fd9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
